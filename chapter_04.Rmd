---
title: "Chapter 4: Linear Models"
output: 
  html_document: 
    highlight: pygments
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
---



```{r}
library(rethinking)
library(tidyverse)
```

# The Normal Distribution

The normal distribution can come about as a result of addition. An illustration with a random walk.

```{r}
set.seed(736)
d <- tibble(x = replicate(1e3, sum(runif(16, -1, 1))))
d %>% 
    ggplot(aes(x)) + 
    geom_density(fill = "grey50") + 
    stat_function(fun = dnorm, 
                  args = list(mean = 0, 
                              sd = sd(d$x)), 
                  colour = "blue")
```

Can also get the normal from multiplication. Get 12 random numbers showing some small amount of growth, take their product (i.e. the combined growth).

```{r}
set.seed(1)
prod(1 + runif(12, 0, 0.1))
```

Now replicate that and examine the distribution.

```{r}
set.seed(1)
tibble(x = replicate(10000, prod(1 + runif(12, 0, 0.1)))) %>% 
    ggplot(aes(x)) + 
    geom_density(fill = "black")
```

This works even better as the multiplicative part gets smaller.

```{r}
set.seed(1)
bind_rows(tibble(distribution = "big", 
                 x = replicate(10000, prod(1 + runif(12, 0, 0.5)))), 
          tibble(distribution = "small", 
                 x = replicate(10000, prod(1 + runif(12, 0, 0.01))))) %>% 
    ggplot(aes(x)) + 
    geom_density(fill = "black", colour = "black") + 
    facet_wrap(~ distribution, scales = "free")
```

This works because the individual numbers are small so their product is approx. equal to their sum.

Normal also arises from log-multiplication.

```{r}
set.seed(12)
tibble(exponent = c(3L, 4L, 5L)) %>% 
    mutate(x = map(exponent, 
                   ~ replicate(n = (10^.x), 
                               log(prod(1 + runif(12, 0, 0.5)))))) %>% 
    transmute(plot_title = sprintf("%s replications", 
                                   scales::comma(10^exponent)), 
              x) %>% 
    unnest() %>% 
    ggplot(aes(x)) + 
    geom_density(color = "transparent", 
                 fill = "gray33") + 
    facet_wrap(~ plot_title)
```

Probability Density Function for a normal random variable $y$ with mean $\mu$ and variance $\sigma^2$is:

$$
f(y|\mu, \sigma^2) = 
\frac{1}
{\sqrt{2\pi\sigma^2}}
\text{exp}\bigg(-
\frac{(y - \mu)^2}
{2\sigma^2}\bigg)
$$


# Language of Modelling

Build up models using a certain structure of language:

$$
\text{outcome}_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \beta \times \textrm{predictor}_i
\\
\beta \sim \mathcal{N}(0, 10)
\\
\sigma \sim \textrm{HalfCauchy}(0, 1)
$$

Revisiting the globe-tossing example we get:

$$
w \sim \textrm{Binomial}(n, p)
\\
p \sim \textrm{Uniform}(0, 1)
$$

And the likelihood is defined as:

$$
\text{P}(w|n, p) = \binom{n}{p}p^w(1 - p)^{(n - w)}
$$

And we can illustrate this thus:

```{r}
d <- tibble(w = 6, 
            n = 9, 
            p = seq(0, 1, length.out = 100)) %>% 
    mutate(prior = dunif(p, 0, 1), 
           likelihood = dbinom(w, n, p)) %>% 
    mutate(posterior = (prior * likelihood)/sum(prior * likelihood))
sample_n(d, 6)

# Now plot the prior, likelihood, and posterior
d %>% 
    select(-w, -n) %>% 
    gather(key, value, -p) %>% 
    mutate(key = factor(key, levels = c("prior", 
                                        "likelihood", 
                                        "posterior"))) %>% 
    ggplot(aes(p, ymin = 0, ymax = value, fill = key)) + 
    geom_ribbon() + 
    scale_fill_manual(values = c("blue", "red", "purple")) + 
    scale_y_continuous(NULL, breaks = NULL) + 
    theme(legend.position = "none") + 
    facet_wrap(~ key, scales = "free")
```

Because the prior was uniform the posterior is just a rescaled version of the likelihood.

# Gaussian Model of Height

```{r}
library(rethinking)
data(Howell1)
d <- Howell1 %>% 
    as_tibble()
glimpse(d)
```

Filter to adults only and plot the height.

```{r}
d2 <- d %>% 
    filter(age >= 18)
d2 %>% 
    ggplot(aes(height)) + 
    geom_density(fill = "black", bw = 1.5) + 
    geom_rug(alpha = 0.2)
```

Now some housekeeping: detach `rethinking` ready to attach `brms` (as many functions in the packages share names).

```{r}
rm(Howell1)
detach("package:rethinking", unload = TRUE)
library(brms)
library(tidybayes)
```

First step in modelling is the likelihood:

$$ h_i \sim \mathcal{N}(\mu, \sigma)$$

Now we need priors for $\mu$ and $\sigma$.

$$
\mu \sim \mathcal{N}(178, 20)
\\
\sigma \sim Uniform(0, 50)
$$

Can plot the priors to see what the asumptions are.

```{r}
# first for mu
tibble(x = seq(100, 250, by = 0.1)) %>% 
    ggplot(aes(x, y = dnorm(x, 178, 20))) + 
    geom_line() + 
    ylab("density")

# then for sigma
tibble(x = seq(-10, 60, by = 0.1)) %>% 
    ggplot(aes(x, y = dunif(x, 0, 50))) + 
    geom_line() + 
    scale_y_continuous(NULL, breaks = NULL) + 
    scale_x_continuous(breaks = seq(-10, 60, by = 10)) + 
    theme(panel.grid = element_blank())
```

Can get a clearer picture by sampling from the Prior Predictive Distribution, which is defined by the priors and likelihood.

```{r}
ppd <- tibble(sample_mu = rnorm(1e4, mean = 178, sd = 20), 
              sample_sigma = runif(1e4, min = 0, max = 50)) %>% 
    mutate(prior_h = rnorm(1e4, mean = sample_mu, sd = sample_sigma))

ppd %>% 
    ggplot(aes(prior_h)) + 
    geom_density(fill = "black") + 
    scale_y_continuous(NULL, breaks = NULL) + 
    labs(subtitle = expression(paste("Prior predictive distribution for ", 
                                     italic(h[i]))), 
         x = NULL) +  
    theme_minimal()
```

### Grid Approximation

Can start off with a brute force approach via grid approximation.

```{r}
# helper function for computing log-likelihood
loglike_compute <- as_mapper(~ dnorm(d2$height, 
                                     .x,
                                     .y, 
                                     log = TRUE) %>% 
                                 sum())
grid_size <- 200L
grid_approx <- tibble(mu = seq(from = 140,
                               to = 160,
                               length.out = grid_size), 
                      sigma = seq(from = 4,
                                  to = 9,
                                  length.out = grid_size)) %>% 
    expand(mu, sigma) %>% 
    mutate(log_likelihood = map2_dbl(mu, sigma, loglike_compute)) %>% 
    mutate(prior_mu = dnorm(mu, 178, 20, log = TRUE), 
           prior_sigma = dunif(sigma, 0, 50, log = TRUE)) %>% 
    mutate(product = log_likelihood + prior_mu + prior_sigma) %>% 
    mutate(posterior = exp(product - max(product)))

head(grid_approx)
```

Can visualise these distributions in a couple of ways.

```{r}
# contour plot
grid_approx %>% 
    ggplot(aes(mu, sigma, z = posterior)) + 
    geom_contour() + 
    labs(x = expression(mu), 
         y = expression(sigma)) + 
    coord_cartesian(xlim = range(grid_approx$mu), 
                    ylim = range(grid_approx$sigma))
# heat map
grid_approx %>% 
    ggplot(aes(mu, sigma)) + 
    geom_raster(aes(fill = posterior)) + 
    scale_fill_viridis_c() + 
    labs(x = expression(mu), 
         y = expression(sigma)) + 
    theme_minimal() + 
    theme(panel.grid = element_blank())
```

Now sample from the posterior using `dplyr::sample_n()`.

```{r}
set.seed(1756)
grid_samples <- grid_approx %>% 
    sample_n(1e4, replace = TRUE, weight = posterior)

grid_samples %>% 
    ggplot(aes(mu, sigma)) + 
    geom_point(size = 0.9, alpha = 1/15) + 
    labs(x = expression(mu[samples]),
         y = expression(sigma[samples])) + 
    theme(panel.grid = element_blank())
```

Can look at the marginal distributions of the parameters also.

```{r}
grid_samples %>% 
    select(mu, sigma) %>% 
    gather() %>% 
    ggplot(aes(value)) + 
    geom_density(fill = "black") + 
    facet_wrap(~ key, scales = "free") + 
    scale_y_continuous(NULL, breaks = NULL) + 
    scale_x_continuous(NULL)
```

And check the HPDI.

```{r}
grid_samples %>% 
    select(mu, sigma) %>% 
    gather(key = "parameter") %>% 
    group_by(parameter) %>% 
    mode_hdi()
```

Distributions look roughly normal, but now repeat with a much smaller sample to show what can go wrong.

```{r}
# new helper function for computing log-likelihood, but from d3
loglike_compute <- as_mapper(~ dnorm(d3$height, 
                                     .x,
                                     .y, 
                                     log = TRUE) %>% 
                                 sum())
set.seed(4341)
d3 <- sample_n(d2, size = 20) # small sample of the adults
# rebuild the grid approximation and posterior sampling
small_grid_approx <- tibble(mu = seq(from = 150,
                                     to = 170,
                                     length.out = grid_size), 
                            sigma = seq(from = 4,
                                        to = 20,
                                        length.out = grid_size)) %>% 
    expand(mu, sigma) %>% 
    mutate(log_likelihood = map2_dbl(mu, sigma, loglike_compute)) %>% 
    mutate(prior_mu = dnorm(mu, 178, 20, log = TRUE), 
           prior_sigma = dunif(sigma, 0, 50, log = TRUE)) %>% 
    mutate(product = log_likelihood + prior_mu + prior_sigma) %>% 
    mutate(posterior = exp(product - max(product)))

small_grid_samples <- small_grid_approx %>% 
    sample_n(1e4, replace = TRUE, weight = posterior)

# plot the posterior sample heights
small_grid_samples %>% 
    ggplot(aes(mu, sigma)) + 
    geom_point(size = 0.9, alpha = 1/15) + 
    labs(x = expression(mu[samples]),
         y = expression(sigma[samples])) + 
    theme(panel.grid = element_blank())
```

There's a much longer tail to the right for sigma, because variance is only bounded below. Can see this clearly in plots of the marginal densities.

```{r}
small_grid_samples %>% 
    select(mu, sigma) %>% 
    gather() %>% 
    ggplot(aes(value)) + 
    geom_density(fill = "black") + 
    facet_wrap(~ key, scales = "free") + 
    scale_y_continuous(NULL, breaks = NULL) + 
    scale_x_continuous(NULL)
```

## Using MCMC via `brms`

Now build the models from the book but using MCMC via `brms` instead of quadratic approximation via `rethinking::quap()`.

Start with the basic model with no predictors.

```{r}
b4_1 <- brm(data = d2, 
            family = gaussian, 
            height ~ 1, 
            prior = c(prior(normal(178, 20), class = Intercept), 
                      prior(uniform(0, 50), class = sigma)), 
            iter = 31000, warmup = 30000, chains = 4, cores = 4, 
            file = "./Stan/ch_04/b4_1")
```

Can improve the performance of the model by changing the prior to $\sigma \sim \text{HalfCauchy}(0, 1)$, which is more realistic. Can visualise this easily and compare to the normal for reference.

```{r}
tibble(x = seq(0, 4, length.out = 1e3)) %>% 
    ggplot(aes(x, )) +  
    stat_function(fun = dcauchy) + 
    stat_function(fun = dnorm, colour = "blue") + 
    stat_function(fun = dunif, colour = "red", args = list(min = 0, max = 50)) + 
    theme_minimal() + 
    scale_y_continuous("density") + 
    ggtitle(expression(paste("Comparing Priors on ", sigma)),  
            subtitle = "Half-Cauchy black; Normal blue; Uniform red")
```

Now build the model.

```{r}
b4_1_half_cauchy <- brm(data = d2, 
                        family = gaussian, 
                        height ~ 1, 
                        prior = c(prior(normal(178, 20), class = Intercept), 
                                  prior(cauchy(0, 1), class = sigma)), 
                        iter = 2000, warmup = 1000, chains = 4, cores = 4, 
                        file = "Stan/ch_04/b4_1_half_cauchy")
```

Can use the default method to get useful diagnostic plots.

```{r}
# plot the model with the uniform prior
plot(b4_1)
# then the half-cauchy prior model
plot(b4_1_half_cauchy)
```

And get a summary with `print()` method.

```{r}
print(b4_1_half_cauchy)
```

Or by accessing the `fit` element to see the Stan-style summary.

```{r}
b4_1_half_cauchy$fit
```

Now build a deliberately 'bad' model with a prior on $\mu$ that is far too narrow.

```{r}
b4_2 <- brm(data = d2, 
            family = gaussian, 
            height ~ 1, 
            prior = c(prior(normal(178, .1), class = Intercept),
                      prior(uniform(0, 50), class = sigma)),
            iter = 3000, warmup = 2000, chains = 4, cores = 4, 
            file = "Stan/ch_04/b4_2")
```

```{r}
print(b4_2)
plot(b4_2)
```

## Sampling From `brms` Fit

```{r}
post <- posterior_samples(b4_1_half_cauchy)
head(post)
# get covariance matrix for the two parameters of interest
post %>% 
    select(b_Intercept, sigma) %>% 
    cov() 
# and the correlation matrix
post %>% 
    select(b_Intercept, sigma) %>% 
    cor() 
```

Need to work a bit harder to get something like the output from `precis()` on a `quap()` model.

```{r}
# brms has its own function that is ok
posterior_summary(b4_1_half_cauchy)
# or we can build something from the posterior_samples() output
post %>% 
    rename(mu = b_Intercept) %>% 
    select(-lp__) %>% 
    gather("parameter") %>% 
    group_by(parameter) %>% 
    summarise(mean = mean(value), 
              sd = sd(value), 
              q2.5 = quantile(value, 0.025), 
              q97.5 = quantile(value, 0.975))
```

## Getting $\sigma$ right 

This is less a problem with `brms` and Hamiltonian MCMC, which doesn't rely on the assumption of a multivariate normal distribution. Can see this in the posterior density for $\sigma$.

```{r}
post %>% 
    ggplot(aes(sigma)) + 
    geom_density(fill = "black") + 
    scale_y_continuous(NULL, breaks = NULL) + 
    xlab(expression(sigma)) + 
    theme(panel.grid = element_blank())
```

For the occasions when we need to model $\sigma$ can use an approach of modelling instead $log(\sigma)$, as outlined in the vignette [Estimating Distributional Models with brms](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html).

# Add a Predictor

Now look at how height covaries with weight ($x$).

```{r}
d2 %>% 
    ggplot(aes(height, weight)) + 
    geom_point(alpha = 0.3) + 
    theme_minimal()
```

Set up a model, with the likelihood:

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
$$
linear model;

$$
\mu_i = \alpha + {\beta}x_i
$$

and then the priors.
$$
\alpha \sim \mathcal{N}(178, 20)
\\
\beta \sim \mathcal{N}(0, 10)
\\
\sigma \sim \textrm{Uniform}(0, 50)
$$
Can translate this to the `brms` syntax.

- $h_i \sim \mathcal{N}(\mu_i, \sigma)$: `family = gaussian`
- $\mu_i = \alpha + {\beta}(x_i - \bar{x})$: `height ~ 1 + weight`
- $\alpha \sim \mathcal{N}(178, 20)$: `prior(normal(178, 20), class = Intercept)`
- $\beta \sim \mathcal{N}(0, 10)$: `prior(normal(0, 10), class = b)`
- $\sigma \sim \textrm{Uniform}(0, 50)$: `prior(uniform(0, 50), class = sigma)`

Now build the model with `brm()`.

```{r}
b4_3 <- brm(
    height ~ 1 + weight, 
    data = d2, 
    family = gaussian, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(normal(0, 10), class = b), 
              prior(uniform(0, 50), class = sigma)), 
    iter = 41000, 
    warmup = 40000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_3"
)
```

See the trace plots.

```{r}
plot(b4_3)
```

Need loads of iterations because of the very weak prior on $\sigma$. Can improve this a lot (as before) by using a prior like $\sigma \sim \text{HalfCauchy}(0, 1)$.

## Interpreting the Model

```{r}
posterior_summary(b4_3)
```

Can examine correlation easily enough by putting samples in a data frame.

```{r}
posterior_samples(b4_3) %>% 
    select(-lp__) %>% 
    cor() %>% 
    round(digits = 2)
```

Or see it with pairs plots.

```{r}
pairs(b4_3)
```

The strong negative correlation between $\alpha$ and $\beta$ can cause problems, so address this by centering:

```{r}
d4 <- d2 %>% 
    mutate(weight_c = weight - mean(weight))
head(d4)
```

Now fit a model using the `weight_c` predictor (and with the prior on $\sigma$ switched to the $\text{HalfCauchy}(0, 1)$ as it's better in every way).

```{r}
b4_4 <- brm(
    height ~ 1 + weight_c, 
    data = d4, 
    family = gaussian, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(normal(0, 10), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    iter = 2000, 
    warmup = 1000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_4"
)
```

```{r}
plot(b4_4)
```

```{r}
summary(b4_4)
```
Compare the number of effective samples to that from the previous model.

```{r}
summary(b4_3)
```

Far more effective samples with many fewer iterations.

Can see the correlations in the new model with pairs plots.

```{r}
pairs(b4_4)
```

Or build the same matrix as before, and get the summary.

```{r}
posterior_samples(b4_4) %>% 
    select(-lp__) %>% 
    cor() %>% 
    round(digits = 2)

posterior_summary(b4_4)

summary(b4_4)
```

Can add the maximum a posteriori line to a scatterlot.
```{r}
d4 %>% 
    ggplot(aes(weight_c, height)) + 
    geom_point(shape = 1, 
               size = 2, 
               colour = "steelblue") + 
    geom_abline(intercept = fixef(b4_4)["Intercept", "Estimate"], 
                slope = fixef(b4_4)["weight_c", "Estimate"]) + 
    xlab("weight (mean-centered)") + 
    theme_minimal()
```

This overstates the certainty of the parameter estimates though, so can add some uncertainty to the plot. Illustrate how this changes with different samples sizes.

```{r}
# create a list with the models of different sample sizes
sample_sizes <- c(10, 50, 150, nrow(d4))
b_sample_sizes <- map(
    sample_sizes, 
    ~ brm(
        height ~ 1 + weight_c, 
        data = d4 %>% 
            sample_n(size = .x), 
        family = gaussian, 
        prior = c(prior(normal(178, 20), class = Intercept), 
                  prior(normal(0, 10), class = b), 
                  prior(cauchy(0, 1), class = sigma)), 
        iter = 5000, 
        warmup = 4000, 
        chains = 4, 
        cores = 4, 
        file = sprintf("Stan/ch_04/b_sample_sizes_%s", 
                       paste(.x))
    )
)
```

Now create the data frames of posterior samples and then the plots.

```{r}
post_list <- map(b_sample_sizes, 
                 ~ posterior_samples(.x))

# create a list with all the plot objects
plot_list <- imap(b_sample_sizes, function(model, i) {
    g <- model[["data"]] %>% 
        as_tibble() %>% 
        mutate(sample_size = sample_sizes[i]) %>% 
        ggplot(aes(weight_c, height)) + 
        geom_point(shape = 1, size = 2, colour = "steelblue") + 
        geom_abline(intercept = sample_n(post_list[[i]], size = 20) %>% 
                        pull(b_Intercept), 
                    slope = sample_n(post_list[[i]], size = 20) %>% 
                        pull(b_weight_c), 
                    size = 1/3, 
                    alpha = 0.3) + 
        coord_cartesian(xlim = range(d4$weight_c), 
                        ylim = range(d4$height)) + 
        labs(subtitle = sprintf("N = %d", sample_sizes[i])) + 
        theme_minimal()
    return(g)
}
)
# use multiplot to put all four in a grid
source("R/multiplot.R")
multiplot(plotlist = plot_list, cols = 2)
```

Most often will plot an interval or contour around the MAP regression line, rather than these clouds of regression lines. Start with a particular weight value, e.g. 50kg, then use the values in `post` for $\alpha$ and $\beta$ to estimate $\mu$.

```{r}
# Use model 4_4 but remember to scale our weight of 50kg
post <- posterior_samples(b4_4)
rescaled_50 <- 50 - mean(d2$weight)
mu_at_50 <- post %>% 
    transmute(mu_at_50 = b_Intercept + (b_weight_c * rescaled_50))
head(mu_at_50)
```

View the density with the 89% HPDI overlaid.

```{r}
mu_at_50 %>%
    ggplot(aes(x = mu_at_50)) +
    geom_density(fill = "steelblue") +
    stat_pointintervalh(aes(y = 0), 
                        point_interval = mode_hdi, 
                        .width = 0.89) + 
    scale_y_continuous(NULL, breaks = NULL) +
    labs(x = expression(mu["height | weight = 50"])) +
    theme_classic()
```

Need to repeat this for all values of `weight`, via `fitted()`.

```{r}
mu <- fitted(b4_4, summary = FALSE)
str(mu)
```

This method can also take custom predictor values via the `newdata` argument.

```{r}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight_seq <- tibble(weight = seq(from = 25, to = 70, by = 1)) %>% 
    mutate(weight_c = weight - mean(d2$weight))
# use fitted to compute mu for each sample from posterior and for each weight in # weight_seq
mu <- fitted(b4_4, 
             summary = FALSE, 
             newdata = weight_seq) %>% 
    as_tibble() %>% 
    mutate(Iter = row_number()) %>% 
    select(Iter, everything()) %>% 
    set_names(c("Iter", sprintf("weight_%02d", weight_seq$weight))) %>% 
    gather(key = "weight", value = "height", -Iter) %>% 
    mutate(weight = as.integer(str_extract(weight, "\\d{2}")))
head(mu)
```

Now plot the data. 

```{r}
mu %>% 
    ggplot(aes(weight, height)) + 
    geom_point(alpha = 0.05, 
               colour = "steelblue") + 
    theme_classic()
```

Can also use the `fitted` method with `summary = TRUE` argument to get the data needed for regression lines.

```{r}
mu_summary <- bind_cols(weight_seq, 
                        fitted(b4_3, 
                               newdata = weight_seq, 
                               # change from default 95% interval to 
                               probs = c(0.055, 0.945)) %>% 
                            as_tibble())
head(mu_summary, 6)
d2 %>% 
    ggplot() + 
    geom_ribbon(data = mu_summary, 
                aes(x = weight, 
                    ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = mu_summary, 
              aes(x = weight, y = Estimate)) + 
    geom_point(aes(weight, height), 
               colour = "steelblue", 
               shape = 1, 
               alpha = .7) + 
    coord_cartesian(xlim = range(d2$weight)) + 
    theme_classic()
```

## Prediction Interval

Those plots are only showing intervals for $\mu$, not the range of plausible heights. To do that we need the `predict()` method.

```{r}
pred_height <- weight_seq %>% 
    bind_cols(predict(b4_4, 
                      newdata = weight_seq, 
                      probs = c(0.055, 0.945)) %>% 
                  as_tibble())
head(pred_height)
```

This is similar to `mu`, but simulations of heights directly, rather than the average height $\mu$. Can summarise these and then plot them directly.

```{r}
# create a tibble of the plausible heights
sim_heights <- predict(b4_4, 
                       summary = FALSE, 
                       newdata = weight_seq) %>% 
    as_tibble() %>% 
    mutate(Iter = row_number()) %>% 
    select(Iter, everything()) %>% 
    set_names(c("Iter", sprintf("weight_%02d", weight_seq$weight))) %>% 
    gather(key = "weight", value = "height", -Iter) %>% 
    mutate(weight = as.integer(str_extract(weight, "\\d{2}")))
sim_heights %>% 
    ggplot(aes(weight, height)) + 
    geom_point(alpha = 0.05, 
               colour = "steelblue") + 
    theme_classic()
```

Can then build a plot with the credible intervals for $\mu$ and the simulated heights.

```{r}
d2 %>% 
    ggplot(aes(weight)) + 
    geom_ribbon(data = pred_height, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey85") + 
    geom_ribbon(data = mu_summary, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = mu_summary, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d2$weight), 
                    ylim = range(d2$height)) + 
    theme_classic()
```

## Polynomial Regression

Can consider higher powers of a single predictor, in this case by returning to the full dataset with children included.

```{r}
d %>% 
    ggplot(aes(weight, height)) + 
    geom_jitter(colour = "steelblue", 
                alpha = 0.7) + 
    theme_classic()
```

The relationship is no longer just linear. There are better ways to capture this than by adding higher powers of the predictor, but using this approach here will show its weaknesses. The model would be:

$$
\mu_i = \alpha + \beta_1{x_i} + \beta_2{x_i^2}
$$

First step is to normalise data to have zero mean and unit variance.

```{r}
d5 <- d %>% 
    mutate(weight_scaled = scale(weight))
head(d5)
```

Set up a new model with fairly weak priors.

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_1}x_i + {\beta_2}x^2_i
\\
\alpha \sim \mathcal{N}(178, 20)
\\
\beta_1 \sim \text{Log-Normal}(0, 1)
\\
\beta_2 \sim \mathcal{N}(0, 1)
\\
\sigma \sim \textrm{HalfCauchy}(0, 1)
$$

No need to add the square term to the data frame, can define it in the model.

```{r}
b4_5 <- brm(
    height ~ 1 + weight_scaled + I(weight_scaled^2), 
    data = d5, 
    family = gaussian, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(lognormal(0, 1), class = b, coef = "weight_scaled"), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    iter = 2000, 
    warmup = 1000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_5"
)
```

Need to plot to make this clearer.

```{r}
plot(b4_5)
```

And use `fitted()` and `predict()` to get plots of the posteriors.

```{r}
weight_s <- tibble(weight_scaled = seq(-3, 3, length.out = 60))
fitted_quad <- weight_s %>% 
    bind_cols(fitted(b4_5, 
                     newdata = weight_s, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())
pred_quad <- weight_s %>% 
    bind_cols(predict(b4_5, 
                      newdata = weight_s, 
                      probs = c(0.055, 0.945)) %>% 
                  as_tibble())
```

Now plot with uncertainty for $\mu$ and plausible heights.

```{r}
d5 %>% 
    ggplot(aes(weight_scaled)) + 
    geom_ribbon(data = pred_quad, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey85") + 
    geom_ribbon(data = fitted_quad, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = fitted_quad, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d5$weight_scaled), 
                    ylim = range(d5$height)) + 
    xlab("Normalised Weight (Mean = 0, Variance = 1)") + 
    theme_classic()
```

Can also build the linear and cubic models to compare.

```{r}
# first the cubic
b4_6 <- brm(
    height ~ 1 + weight_scaled + I(weight_scaled^2) + I(weight_scaled^3), 
    data = d5, 
    family = gaussian, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(lognormal(0, 1), class = b, coef = "weight_scaled"), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    iter = 2000, 
    warmup = 1000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_6"
)
# then the simple linear
b4_7 <- brm(
    height ~ 1 + weight_scaled, 
    data = d5, 
    family = gaussian, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    iter = 2000, 
    warmup = 1000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_7"
)
```

Then build the plots.

```{r}
fitted_cubic <- weight_s %>% 
    bind_cols(fitted(b4_6, 
                     newdata = weight_s, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())
pred_cubic <- weight_s %>% 
    bind_cols(predict(b4_6, 
                      newdata = weight_s, 
                      probs = c(0.055, 0.945)) %>% 
                  as_tibble())
d5 %>% 
    ggplot(aes(weight_scaled)) + 
    geom_ribbon(data = pred_cubic, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey85") + 
    geom_ribbon(data = fitted_cubic, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = fitted_cubic, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d5$weight_scaled), 
                    ylim = range(d5$height)) + 
    xlab("Normalised Weight (Mean = 0, Variance = 1)") + 
    theme_classic()

fitted_linear <- weight_s %>% 
    bind_cols(fitted(b4_7, 
                     newdata = weight_s, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())
pred_linear <- weight_s %>% 
    bind_cols(predict(b4_7, 
                      newdata = weight_s, 
                      probs = c(0.055, 0.945)) %>% 
                  as_tibble())
d5 %>% 
    ggplot(aes(weight_scaled)) + 
    geom_ribbon(data = pred_linear, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey85") + 
    geom_ribbon(data = fitted_linear, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = fitted_linear, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d5$weight_scaled), 
                    ylim = range(d5$height)) + 
    xlab("Normalised Weight (Mean = 0, Variance = 1)") + 
    theme_classic()
```

# Practice

## Easy 

4E1. $y_i \sim \mathcal{N}(\mu, \sigma)$ is the likelihood.

4E2. Two parameters in the posterior: $\mu$, and $\sigma$.

4E3. 

$$
\text{P}(\mu, \sigma|y_i) = 
    \frac
        {\prod_i \mathcal{N}(y_i|\mu, \sigma)\mathcal{N}(\mu|0, 10)\textrm{Uniform}(\sigma|0, 10)}
        {\int\int\prod_i \mathcal{N}(y_i|\mu, \sigma)\mathcal{N}(\mu|0, 10)\textrm{Uniform}(\sigma|0, 10) d{\mu} d\sigma}
$$

4E4. The line $mu_i = \alpha + \beta{x}_i$ is the linear model.

4E5. There are four parameters for the model.

## Medium

4M1.

```{r}
num_trials <- 1e3
prior_h <- tibble(mu = rnorm(n = num_trials, mean = 0, sd = 10), 
                  sigma = runif(n = num_trials, min = 0, max = 10)) %>% 
    mutate(height = rnorm(n = num_trials, mean = mu, sd = sigma))

prior_h %>% 
    ggplot(aes(height)) + 
    geom_density(fill = "steelblue") + 
    theme_classic()
```

4M2.

NB. if modelling would create a `brms` model instead.

```{r}
flist <- alist(
    y ~ dnorm(mu, sigma), 
    mu ~ dnorm(0, 10), 
    sigma ~ dunif(0, 10)
)
```

4M3.

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta}x_i
\\
\alpha \sim \mathcal{N}(0, 50)
\\
\beta \sim \text{Uniform}(0, 10)
\\
\sigma \sim \text{Uniform}(0, 50)
$$

4M4.

Assumptions:

- height will not decrease between years
- variance remains constant throughout sample.

Model would be as follows:

$$
h_i \sim (\mu_i, \sigma)
\\
\mu_i = \alpha + \beta{t_i}
\\
\alpha \sim \mathcal{N}(100, 20)
\\
\beta \sim \textrm{Exponential}(1)
\\
\sigma \sim \textrm{Exponential}(1)
$$

4M5.

Assuming that "in the first year" means at the start of the first year, would change distribution so that $\alpha \sim \mathcal{N}(120, 20)$.

Requirement that every student got taller each year is ok with the Exponential prior I used. If $X \sim \textrm{Exponential}(\frac{1}{4})$ then $P(X = 0) = 0$, so all students will grow with probability 1.

4M6.

Can simply change the prior so that $\sigma \sim \textrm{Uniform}(0, 8)$ to limit the variance. Can test the variance of students after 1, 2, 3 years with this model to see how this behaves. Suppose that the sample is of 30 students.

```{r}
num_sims <- 1e4
n <- 30
samples <- tibble(id = seq_len(num_sims)) %>% 
    mutate(alpha = map(id, ~ rnorm(n, 100, 20)), 
           beta = map(id, ~ rexp(n)), 
           sigma = map(id, ~ rexp(n))) %>% 
    mutate(year_1 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 1), 
                                 sd = ..3)), 
           year_2 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 2), 
                                 sd = ..3)), 
           year_3 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 3), 
                                 sd = ..3))) %>% 
    mutate(year_1_var = map_dbl(year_1, var), 
           year_2_var = map_dbl(year_2, var), 
           year_3_var = map_dbl(year_3, var))
```

How often is the variance above 64?

```{r}
samples %>% 
    select(id, matches("year\\_\\d\\_var")) %>% 
    gather(key = year, value = variance, -id) %>% 
    mutate(year = as.integer(str_extract(year, "\\d"))) %>% 
    group_by(year) %>% 
    summarise(avg_var = mean(variance), 
              high_var = mean(variance > 64), 
              high_var_count = sum(variance > 64))
```

The variance from all the parameters estimated is compounded in the final sample, which means the variance is much higher than the 64 stated. This means that we need to place 'stricter' priors on the parameters, or simplify the model to have fewer degrees of freedom. So a new model could be:

$$
h_i \sim (\mu_i, \sigma)
\\
\mu_i = \alpha + \beta{t_i}
\\
\alpha \sim \mathcal{N}(120, 5)
\\
\beta \sim \textrm{Exponential}(2)
\\
\sigma \sim \textrm{Exponential}(2)
$$

And now generate new samples from this prior predictive distribution and check variances after three years (when variance will be at its highest).

```{r}
samples02 <- tibble(id = seq_len(num_sims)) %>% 
    mutate(alpha = map(id, ~ rnorm(n, 100, 5)), 
           beta = map(id, ~ rexp(n, rate = 2)), 
           sigma = map(id, ~ rexp(n, rate = 2))) %>% 
    mutate(year_1 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 1), 
                                 sd = ..3)), 
           year_2 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 2), 
                                 sd = ..3)), 
           year_3 = pmap(list(alpha, beta, sigma), 
                         ~ rnorm(n, 
                                 mean = ..1 + (..2 * 3), 
                                 sd = ..3))) %>% 
    mutate(year_1_var = map_dbl(year_1, var), 
           year_2_var = map_dbl(year_2, var), 
           year_3_var = map_dbl(year_3, var))
samples02 %>% 
    select(id, matches("year\\_\\d\\_var")) %>% 
    gather(key = year, value = variance, -id) %>% 
    mutate(year = as.integer(str_extract(year, "\\d"))) %>% 
    group_by(year) %>% 
    summarise(avg_var = mean(variance), 
              high_var = mean(variance > 64), 
              high_var_count = sum(variance > 64))
```

This model seems to work well to restrict the variance to be strictly less than 64, but these priors are much more opinionated than may be justified. A more conservative set of priors might allow for the possibility of variance greater than 64, but would give greater weight to the data in the posterior distribution.

## Hard

4H1. 

Can feed these new data points through the model (`b4_4`) set up before for the !Kung.

```{r}
new_weights <- tibble(weight = c(46.95, 43.72, 64.78, 32.59, 54.63)) %>% 
  mutate(weight_c = weight - mean(d$weight))
sim_heights <- predict(b4_4, newdata = new_weights, summary = FALSE) %>% 
  as_tibble() %>% 
  set_names(sprintf("person_%02d", seq_len(nrow(new_weights)))) %>% 
  gather(key = "Individual") %>% 
  group_by(Individual)
# now build a summary of the simulated heights
mode_hdi(sim_heights, .width = 0.89) %>% 
  mutate(weight = new_weights$weight) %>% 
  transmute(Individual = as.integer(str_extract(Individual, "\\d$")), 
            weight, 
            expected_height = value, 
            interval_low = .lower, 
            interval_high = .upper)
```

4H2.

a.

```{r}
d6 <- d %>% 
  filter(age < 18) %>% 
  as_tibble() %>% 
  mutate(weight_c = weight - mean(weight))
b4_9 <- brm(
    height ~ 1 + weight_c, 
    data = d6, 
    family = gaussian, 
    prior = c(prior(normal(120, 30), class = Intercept), 
              prior(normal(0, 10), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    iter = 2000, 
    warmup = 1000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_9"
)
summary(b4_9)
```

The model predicts that each child will get c. 27cm taller for each 10kgs heavier.

b.

```{r}
weight_s <- tibble(weight_c = seq(-15, 27, length.out = 200))

fitted_linear <- weight_s %>% 
    bind_cols(fitted(b4_9, 
                     newdata = weight_s, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())
pred_linear <- weight_s %>% 
    bind_cols(predict(b4_9, 
                      newdata = weight_s, 
                      probs = c(0.055, 0.945)) %>% 
                  as_tibble())
d6 %>% 
    ggplot(aes(weight_c)) + 
    geom_ribbon(data = pred_linear, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey85") + 
    geom_ribbon(data = fitted_linear, 
                aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "grey70") + 
    geom_line(data = fitted_linear, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d6$weight_c), 
                    ylim = range(d6$height)) + 
    xlab("Centered Weight") + 
    theme_classic()
```

c. 

The linear model shows bias in almost all areas of the population. There is a curve to the data, starting steep and then levelling off towards the highest weights. Fitting a straight line cannot capture that.

4H3.

a. 

```{r}
b4_10 <- brm(
    height ~ 1 + log(weight), 
    data = d, 
    family = gaussian, 
    prior = c(prior(normal(178, 100), class = Intercept), 
              prior(normal(0, 100), class = b), 
              prior(uniform(0, 50), class = sigma)), 
    iter = 50000, 
    warmup = 49000, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_04/b4_10", 
    control = list(adapt_delta = 0.98)
)
summary(b4_10)
```

b.

```{r}
weight_s <- tibble(weight = seq(1, 130, by = 0.5))

fitted_linear <- weight_s %>% 
    bind_cols(fitted(b4_10, 
                     newdata = weight_s, 
                     probs = c(0.015, 0.985)) %>% 
                  as_tibble())
pred_linear <- weight_s %>% 
    bind_cols(predict(b4_10, 
                      newdata = weight_s, 
                      probs = c(0.015, 0.985)) %>% 
                  as_tibble())
d %>% 
    ggplot(aes(weight)) + 
    geom_ribbon(data = pred_linear, 
                aes(ymin = Q1.5, 
                    ymax = Q98.5), 
                fill = "grey85") + 
    geom_ribbon(data = fitted_linear, 
                aes(ymin = Q1.5, 
                    ymax = Q98.5), 
                fill = "grey70") + 
    geom_line(data = fitted_linear, 
              aes(y = Estimate)) + 
    geom_point(aes(y = height), 
               shape = 1, 
               colour = "steelblue") + 
    coord_cartesian(xlim = range(d$weight), 
                    ylim = range(d$height)) + 
    xlab("Weight") + 
    theme_classic()
```

