---
title: "Chapter 4: Linear Models"
output: 
  html_notebook: 
    highlight: kate
    theme: journal
---

# Notes

The normal distribution comes about as a result of addition. An illustration with a random walk.

```{r}
suppressPackageStartupMessages({
    library(purrr)
    library(rethinking)
})
dens(replicate(n = 1e3, 
               expr = sum(runif(16, -1, 1))), 
     norm.comp = TRUE)
```

Even clearer with more examples.

```{r}
purrr::walk(4:6, function(i) {
    dens(replicate(n = 10 ^ i,
                   expr = sum(runif(16, -1, 1))), norm.comp = TRUE)
})
```

Can also get the normal from multiplication. Get 12 random numbers showing some small amount of growth, take their product (i.e. the combined growth).

```{r}
prod(1 + runif(12, 0, 0.1))
```

Now replicate that and examine the distribution.

```{r}
par(mfrow = c(2, 3))
walk(seq_len(6L), function(i) {
    dens(replicate(10^i, prod(1 + runif(12, 0, 0.1))), 
         norm.comp = TRUE)
})
```

This works because the individual numbers are small so their product is approx. equal to their sum.

Normal also arises from log-multiplication.

```{r}
par(mfrow = c(2, 3))
walk(seq_len(6L), function(i) {
     log.big <- replicate(10^i, 
                          log(prod(1 + runif(12, 0, 0.5))))
     dens(log.big, norm.comp = TRUE)
})
```

## Language of Modelling

Build up models using a certain structure of language, viz.:

$$
outcome_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \beta \times \textrm{predictor}_i
\\
\beta \sim \mathcal{N}(0, 10)
\\
\sigma \sim \textrm{HalfCauchy}(0, 1)
$$

Revisiting the globe-tossing example we get:

$$
w \sim Binom(n, p)
\\
p \sim Uniform(0, 1)
$$

```{r}
w <- 6
n <- 9
p_grid <- seq(from = 0,
              to = 1, 
              length.out = 100)
posterior <- dbinom(w, n, p_grid) * dunif(p_grid, 0, 1)
posterior <- posterior/sum(posterior)
dens(posterior)
```

## Gaussian Model of Height

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
str(d)
```

Filter to adults only and plot the height.

```{r}
d2 <- d[d$age >= 18, ]
dens(d2$height)
```

First step in modelling is the likelihood:

$$ h_i \sim \mathcal{N}(\mu, \sigma)$$

Now we need priors for $\mu$ and $\sigma$.

$$
\mu \sim \mathcal{N}(178, 20)
\\
\sigma \sim Uniform(0, 50)
$$

Can plot the priors to see what the asumptions are.

```{r}
par(mfrow = c(1, 2))
curve(dnorm(x, 178, 20), from = 100, to = 250, xlab = "prior for mu")
curve(dunif(x, 0, 50), from = -10, to = 60, xlab = "prior for sigma")
```

Can get a clearer picture by sampling from the Prior Predictive Distribution, which is defined by the priors and likelihood.

```{r}
sample_mu <- rnorm(1e4, mean = 178, sd = 20)
sample_sigma <- runif(1e4, min = 0, max = 50)
prior_h <- rnorm(1e4, mean = sample_mu, sd = sample_sigma)
dens(prior_h)
```

### Grid Approximation

Can start off with a brute force approach via grid approximation.

```{r}
mu.list <- seq(from = 140,
               to = 160,
               length.out = 200)
sigma.list <- seq(from = 4,
                  to = 9,
                  length.out = 200)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(seq_len(nrow(post)), function(i)
    sum(
        dnorm(
            d2$height,
            mean = post$mu[i],
            sd = post$sigma[i],
            log = TRUE
       )))
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
    dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```
 
 `rethinking` has functions for visualising these distributions.
 
```{r}
contour_xyz(post$mu, post$sigma, post$prob)
image_xyz(post$mu, post$sigma, post$prob)
```

Now sample from the posterior.

```{r}
sample.rows <- sample(seq_len(nrow(post)), 
                      size = 1e4, 
                      replace = TRUE, 
                      prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
plot(
    sample.mu,
    sample.sigma,
    cex = 0.5,
    pch = 16,
    col = col.alpha(rangi2, 0.1)
   )
```

Can look at the marginal distributions of the parameters also.

```{r}
dens(sample.mu)
dens(sample.sigma)
```

And check the HPDI.

```{r}
HPDI(sample.mu)
HPDI(sample.sigma)
```

Distributions look roughly normal, but now repeat with a much smaller sample to show what can go wrong.

```{r}
d3 <- sample(d2$height, size = 20)
mu.list <- seq(from = 150,
               to = 170,
               length.out = 200)
sigma.list <- seq(from = 4,
                  to = 20,
                  length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i)
    sum(dnorm(
        d3,
        mean = post2$mu[i],
        sd = post2$sigma[i],
        log = TRUE
   )))
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) +
    dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample2.rows <- sample(
    1:nrow(post2),
    size = 1e4,
    replace = TRUE,
    prob = post2$prob
)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
plot(
    sample2.mu,
    sample2.sigma,
    cex = 0.5,
    col = col.alpha(rangi2, 0.1),
    xlab = "mu",
    ylab = "sigma",
    pch = 16
)
```

There's a much longer tail to the right for sigma, because variance is only bounded below. Can see this clearly in a plot.

```{r}
dens(sample2.sigma, norm.comp = TRUE)
```

## Maximum A Posteriori

Now use a quadratic approximation, with the peak at the _Maximum A Posteriori_ (MAP), using `rethinking::map()`.

Define the model using formula syntax, and define start points as the sample mean and standard deviation.

```{r}
flist <- alist(
    height ~ dnorm(mu, sigma), 
    mu ~ dnorm(178, 20), 
    sigma ~ dunif(0, 50)
)

starts <- list(
    mu = mean(d2$height), 
    sigma = sd(d2$height)
)
```

```{r}
m4.1 <- map(flist, data = d2, start = starts)
precis(m4.1)
```

Can run this again with a much stronger prior on $\mu$.

```{r}
m4.2 <- map(
    flist = alist(
        height ~ dnorm(mu, sigma),
        mu ~ dnorm(178, 0.1),
        sigma ~ dunif(0, 50)),
    start = starts, 
    data = d2)
precis(m4.2)
```

Posterior for $\mu$ has barely moved from the prior, but $\sigma$ has changed a lot.

Try again with something more moderate.

```{r}
m4.2b <- map(
    flist = alist(
        height ~ dnorm(mu, sigma),
        mu ~ dnorm(178, 5),
        sigma ~ dunif(0, 25)),
    start = starts, 
    data = d2)
precis(m4.2b)
```

From the `map()` output can use `vcov()` to recover the variance-covariance matrix for the parameters.

```{r}
vcov(m4.2b)
```

This can then be decomposed further into the variances for each parameter:

```{r}
diag(vcov(m4.2b))
```

and the correlation matrix.

```{r}
cov2cor(vcov(m4.2b))
```

`rethinking::extract.samples()` is a helper function that will extract the relevant values and generate samples.

```{r}
post3 <- extract.samples(object = m4.2b, n = 1e4)
head(post3)
```

```{r}
precis(post3)
plot(post3)
```

This function is a convenient way to call `MASS::mvrnorm`.

```{r}
library(MASS)
post4 <- mvrnorm(n = 1e4,
                 mu = coef(m4.2b),
                 Sigma = vcov(m4.2b))
precis(as.data.frame(post4))
plot(post4)
```

Sometimes a quadratic assumption for $\sigma$ is a problem, can estimate $\textrm{ln}(\sigma)$ instead.

```{r}
m4.1_logsigma <- map(alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2, 10)), 
    data = d2)
post5 <- extract.samples(m4.1_logsigma)
# get back to natural scale for sigma
post5$sigma <- exp(post5$log_sigma)
precis(post5)
plot(post5$mu, post5$sigma)
```

##Â Add a Predictor

Now look at how height covaries with weight ($x$).

```{r}
plot(d2$height ~ d2$weight)
```

Set up a model, with the likelihood:

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
$$
linear model;

$$
\mu_i = \alpha + {\beta}x_i
$$

and then the priors.
$$
\alpha \sim \mathcal{N}(178, 100)
\\
\beta \sim \mathcal{N}(0, 10)
\\
\sigma \sim Uniform(0, 50)
$$

Now build the model with `map()`.

```{r}
m4.3 <- map(
    flist = alist(
        height ~ dnorm(mu, sigma), 
        mu <- a + b*weight, 
        a ~ dnorm(178, 100), 
        b ~ dnorm(0, 10), 
        sigma ~ dunif(0, 50)
   ), 
    data = d2, 
    start = list(
        a = mean(d2$height), 
        b = 0
   )
)
```

```{r}
precis(m4.3, corr = TRUE)
```

Negative correlation between $\alpha$ and $\beta$ can cause problems in some models. Can address this by scaling:

```{r}
d2$weight.c <- d2$weight - mean(d2$weight)
```

```{r}
plot(height ~ weight, data = d2)
abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])
```

The MAP line overstates the certainty of the model though. Can sample some pairs of the coefficients and add many lines to the plot.

```{r}
post6 <- extract.samples(m4.3)
N <- 10
dN <- d2[seq_len(N), ]
mN <- map(
    flist = alist(
        height ~ dnorm(mu, sigma), 
        mu <- a + b*weight, 
        a ~ dnorm(178, 100), 
        b ~ dnorm(0, 10), 
        sigma ~ dunif(0, 50)
   ), 
    data = dN, 
    start = list(
        a = mean(dN$height), 
        b = 0
   )
)
```

And see how this changes as sample size increases.

```{r}
par(mfrow = c(2, 2))
walk(c(10, 50, 150, 352), function(j) {
    # extract j samples from the posterior
    tmp_post <- extract.samples(mN, size = j)
    # display raw data and sample size
    plot(
        dN$weight,
        dN$height,
        xlim = range(d2$weight),
        ylim = range(d2$height),
        col = rangi2,
        xlab = "weight",
        ylab = "height"
   )
    mtext(concat("N = ", j))
    # plot the lines, with transparency
    for (i in seq_len(j)) {
        abline(a = tmp_post$a[i],
               b = tmp_post$b[i],
               col = col.alpha("black", 0.3))
    }
})
```

Most often will plot an interval or contour around the MAP regression line, rather than these clouds of regression lines. Start with a particular weight value, e.g. 50kg, then use the values in `post` for $\alpha$ and $\beta$ to estimate $\mu$.

```{r}
mu_at_50 <- post6$a + post6$b * 50
```

View the density.

```{r}
dens(mu_at_50, 
     col = rangi2, 
     lwd = 2, 
     xlab = "mu|weight = 50")
```

And the HPDI.

```{r}
HPDI(mu_at_50)
```

Need to repeat this for all values of `weight`, via `rethinking::link()`.

```{r}
mu <- link(m4.3)
str(mu)
```

```{r}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
```

```{r}
# use type="n" to hide raw data
plot(height ~ weight, d2, type = "n")
# loop over samples and plot each mu value
walk(seq_len(100), function(i) {
    points(weight.seq,
           mu[i, ],
           pch = 16,
           col = col.alpha(rangi2, 0.1))
})
```

Can summarise these data.

```{r}
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

And add to the plot.

```{r}
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))
# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)
# plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```

The recipe in summary:

1. Use `link()` to generate distributions of posterior values of $\mu$. Give values for the horizontal axis if needed.
2. Summarise that output with functions like `mean()`, `HPDI()`.
3. Generate plots to visualise the estimates.

Can do what `link()` does manually.

```{r}
post7 <- extract.samples(m4.3)
mu.link <- function(weight) {
    post7$a + post7$b * weight
}
weight.seq <- seq(from = 25, to = 70, by = 1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

## Prediction Interval

Now generate an interval for actual heights, not just $\mu$.

```{r}
sim.height <- sim(fit = m4.3, 
                  data = list(weight = weight.seq))
str(sim.height)
```

This matrix is similar to `mu`, but simulations of heights directly, rather than the average height $\mu$. Can summarise these.

```{r}
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

Now plot these with all the previous bits.

```{r}
# plot raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
# draw MAP line
lines(weight.seq, mu.mean)
# draw HPDI region for line
shade(mu.HPDI, weight.seq)
# draw PI region for simulated heights
shade(height.PI, weight.seq)
```

Can do what `sim()` does manually.

```{r}
post8 <- extract.samples(m4.3)
weight.seq2 <- 25:70
sim.height2 <- sapply(weight.seq2, 
                      function(weight) {
                          rnorm(n = nrow(post8), 
                                mean = post8$a + post8$b * weight, 
                                sd = post8$sigma)
                      })
height.PI2 <- apply(sim.height2, 2, PI, prob = 0.89)
```

## Polynomial Regression

Can consider higher powers of a single predictor, in this case by returning to the full dataset with children included.

```{r}
plot(d$weight, d$height)
```

The relationship is no longer just linear. There are better ways to capture this this by adding higher powers of the predictor, but this will show weaknesses. The model would be:

$$
\mu_i = \alpha + \beta_1{x_i} + \beta_2{x_i^2}
$$

First step is to normalise data to have zero mean and unit variance.

```{r}
d$weight.s <- (d$weight - mean(d$weight))/sd(d$weight)
```

Set up a new model with fairly weak priors.

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
$$
linear model;

$$
\mu_i = \alpha + {\beta_1}x_i + {\beta_2}x^2_i
$$

and then the priors.
$$
\alpha \sim \mathcal{N}(178, 100)
\\
\beta_1 \sim \mathcal{N}(0, 10)
\\
\beta_2 \sim \mathcal{N}(0, 10)
\\
\sigma \sim Uniform(0, 50)
$$

Could run the model and define the square term in there, but easier to add it to the `data.frame`.

```{r}
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
    flist = alist(
        height ~ dnorm(mu, sigma), 
        mu <- a + (b1 * weight.s) + (b2 * weight.s2), 
        a ~ dnorm(178, 100), 
        b1 ~ dnorm(0, 10), 
        b2 ~ dnorm(0, 10), 
        sigma ~ dunif(0, 50)
    ), 
    data = d
)
precis(m4.5)
```

Need to plot to make this clearer.

```{r}
weight.seq <- seq(from = -2.2,
                  to = 2,
                  length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq ^ 2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

```{r}
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

# Practice

## Easy 

4E1. $y_i$ is the likelihood.

4E2. Three parameters in the posterior.

4E3. $\frac{\prod_i \mathcal{N}(y_i|\mu, \sigma)\mathcal{N}(\mu|0, 10)Uniform(\sigma|0, 10)}{\int\int\prod_i \mathcal{N}(y_i|\mu, \sigma)\mathcal{N}(\mu|0, 10)Uniform(\sigma|0, 10) d\mu d\sigma}$

4E4. The line $mu_i = \alpha + \beta{x}_i$ is the linear model.

4E5. There are five parameters for the model.

## Medium

4M1.

```{r}
trials <- 1e3
sample_mu <- rnorm(n = trials, mean = 0, sd = 10)
sample_sigma <- runif(n = trials, min = 0, max = 10)
prior_h <- rnorm(n = trials, mean = sample_mu, sd = sample_sigma)
dens(prior_h)
```

4M2.

```{r}
flist <- alist(
    y ~ dnorm(mu, sigma), 
    mu ~ dnorm(0, 10), 
    sigma ~ dunif(0, 10)
)
```

4M3.

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta}x_i
\\
\alpha \sim \mathcal{N}(0, 50)
\\
\beta \sim Uniform(0, 10)
\\
\sigma \sim Uniform(0, 50)
$$

4M4.

Assumptions:

- height will not decrease between years
- variance remains constant throughout sample.

Model would be as follows:

$$
y_i \sim (\mu_i, \sigma)
\\
\mu_i = \alpha + \beta{t_i}
\\
\alpha \sim \mathcal{N}(100, 20)
\\
\beta \sim \textrm{Gamma}(20, 1)
\\
\sigma \sim \textrm{Uniform}(0, 30)
$$

4M5.

Assuming that "in the first year" means at the start of the first year, would change distribution so that $\alpha \sim \mathcal{N}(120, 20)$.

Requirement that every student got taller each year is ok with the Gamma prior I used. If $X \sim \textrm{Gamma}(\alpha, \beta)$ then $P(X = 0) = 0$, so all students will grow with probability 1.

4M6.

Can simply change the prior so that $\sigma \sim \textrm{Uniform}(0, 8)$ to limit the variance. Can test the variance of students after 1, 2, 3 years with this model to see how this behaves.

```{r}
n <- 1e4
seq_len(3) %>%
    map_df( ~ dplyr::data_frame(
        year = .x,
        variance = var(
            rnorm(
                n,
                mean = rnorm(n, 120, 20) + (rgamma(n, 20, 1) * year),
                sd = runif(n, 0, 8)))))
```

The variance from all the parameters estimated is compounded in the final sample, which means the variance is much higher than the 64 stated. This means that we need to place 'stricter' priors on the parameters, or simplify the model to have fewer degrees of freedom. So a new model could be:

$$
y_i \sim (\mu_i, \sigma)
\\
\mu_i = \alpha + \beta{t_i}
\\
\alpha \sim \mathcal{N}(120, 5)
\\
\beta \sim \textrm{Gamma}(5, 1)
\\
\sigma \sim \textrm{Uniform}(0, 3)
$$

And now generate new samples from this prior predictive distribution and check variances after three years (when variance will be at its highest.

```{r}
n <- 1e3
samples <- rerun(1e4,
                 rnorm(
                     n,
                     mean = rnorm(n, 120, 2) + (rgamma(n, 5, 1) * 3),
                     sd = runif(n, 0, 2)))
sample_vars <- unlist(lapply(samples, var))
hist(sample_vars)
sum(sample_vars > 64)
```

This model seems to work ok to restrict the variance to be strictly less than 64, but these priors are much more opinionated than may be justified. A more conservative set of priors might allow for the possibility of variance greater than 64, but would give greater weight to the data in the posterior distribution.

## Hard

4H1. 

Can feed these new data points through the model set up before for the !Kung.

```{r}
data("Howell1")
d <- Howell1
# normalise the weights
d$weight_norm <- scale(d$weight)

# set up the model
model <- map(
    flist = alist(
        height ~ dnorm(mean = mu, sd = sigma),
        mu <- alpha + beta * weight_norm,
        alpha ~ dnorm(mean = 0, sd = 100),
        beta ~ dnorm(mean = 0, sd = 10),
        sigma ~ dunif(min = 0, max = 64)
    ), 
    data = d
)

new_weights <- c(46.95, 43.72, 64.78, 32.59, 54.63)
new_norm_weights <- (new_weights - mean(d$weight))/sd(d$weight)
sim_heights <- sim(model, data = list(weight_norm = new_norm_weights))
```

And then get the HPDI and predicted heights.

```{r}
data.frame(Individual = seq_len(5), 
           weight = new_weights, 
           expected_height = round(colMeans(sim_heights), 2), 
           HPDI_89_pct_low = apply(sim_heights, 2, HPDI)[1, ], 
           HPDI_89_pct_high = apply(sim_heights, 2, HPDI)[2, ])
```

4H2.

a.

```{r}
d2 <- d[d$age < 18, ]
model <- map(
    flist = alist(
        height ~ dnorm(mean = mu, sd = sigma),
        mu <- alpha + beta * weight,
        alpha ~ dnorm(mean = 100, sd = 50),
        beta ~ dnorm(mean = 0, sd = 10),
        sigma ~ dunif(min = 0, max = 64)
    ), 
    data = d2, 
    start = list(
        alpha = mean(d$height), 
        beta = 0, 
        sigma = 10
    )
)

precis(model)
```

The model predicts that each child will get c. 27cm taller for each 10kgs heavier.

b.

```{r}
library(MASS)
ntrials <- 1e5
weight.seq2 <- seq_len(70)
posterior_samples <- as.data.frame(
    mvrnorm(n = ntrials, 
            mu = coef(model), 
            Sigma = vcov(model)))
sample_mu <- sapply(weight.seq2, function(weight) {
    posterior_samples$alpha + posterior_samples$beta * weight
})
sample_mu_mean <- apply(sample_mu, 2, mean)
sample_mu_hpdi <- apply(sample_mu, 2, HPDI)
sample_heights <- sapply(weight.seq2, function(weight) {
    rnorm(n = nrow(posterior_samples), 
          mean = posterior_samples$alpha + posterior_samples$beta * weight, 
          sd = posterior_samples$sigma)
})
sample_height_hpdi <- apply(sample_heights, 2, HPDI)

plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.3))
lines(x = weight.seq2, y = sample_mu_mean)
shade(object = sample_mu_hpdi, lim = weight.seq2)
shade(object = sample_height_hpdi, lim = weight.seq2)
```

c. 

The linear model shows bias in almost all areas of the population. There is a curve to the data, starting steep and then levelling off towards the highest weights. Fitting a straight line cannot capture that.

4H3.

```{r}
d <- Howell1
model <- map(
    flist = alist(
        height ~ dnorm(mean = mu, sd = sigma),
        mu <- alpha + beta * log(weight),
        alpha ~ dnorm(mean = 100, sd = 100),
        beta ~ dnorm(mean = 0, sd = 100),
        sigma ~ dunif(min = 0, max = 50)
    ), 
    data = d, 
    start = list(
        alpha = 0, 
        beta = 0, 
        sigma = 10
    )
)

precis(model)
```


