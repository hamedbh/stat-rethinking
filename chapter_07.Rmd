---
title: "Chapter 7: Ulysses' Compass"
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(dagitty)
library(ggdag)
library(ggrepel)
```

# Notes

Need to steer between two dangers:

1. **Overfitting**: learning too much from the data.
2. **Underfitting** learning too little from the data.

Two ways to control this:

1. **Regularising Priors**: somewhat more sceptical priors will tend to help with overfitting.
2. Using **Information Criteria** to judge the performance of models.


## The Problem with Parameters

Adding more parameters will _nearly always_ improve the model fit. Here is a demo using a common measure of model fit: $R^2$. This measures how well the model can retrodict the data. It is commonly described as _variance explained_, and its formula is:

$$
R^2 = \frac{\text{var(outcome) - var(residuals)}}{\text{var(outcome)}} = 
1 - \frac{\text{var(residuals)}}{\text{var(outcome)}}
$$

It will increase as we add parameters even when those parameters are just random noise.

```{r}
d <- tibble(
    species = c(
        "afarensis",
        "africanus",
        "habilis",
        "boisei",
        "rudolfensis",
        "ergaster",
        "sapiens"),
    brain = c(438, 452, 612, 521, 752, 871, 1350), 
    mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
) %>% 
    mutate(brain_std = brain/(max(brain)), 
           mass_std = as.numeric(scale(mass)))

d %>% 
    ggplot(aes(mass, 
               brain, 
               label = species)) + 
    geom_point(colour = "steelblue", 
               shape = 21) + 
    geom_label_repel(size = 3, 
                     seed = 438) + 
    coord_cartesian(xlim = c(30, 65)) + 
    labs(x = "body mass (kg)", 
         y = "brain volume (cc)") + 
    theme_bw() + 
    theme(panel.grid = element_blank())
```

We could just model the brain size as a function of body mass, or of the log of body mass. But we will fit an increasingly complex series of polynomial models to the data, using the normalised versions of the predictors created above. Start with the simple linear model:

\begin{eqnarray}
b_i & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta{m_i} \\
\alpha & \sim & \mathcal{N}(0.5, 1) \\
\beta & \sim & \mathcal{N}(0, 10) \\
\sigma & \sim & \text{Lognormal}(0, 1) \\
\end{eqnarray}

These will be done best using McElreath's functions from `{rethinking}`, as it will better illustrate what is happening than using `brms`.

```{r}
# create a function for computing R-squared
R2_is_bad <- function(quap_fit) {
    s <- rethinking::sim(quap_fit, refresh = 0)
    r <- apply(s, 2, mean) - d$brain_std
    1 - rethinking::var2(r)/rethinking::var2(d$brain_std)
}
m7_1 <- rethinking::quap(
    flist = alist(
        brain_std ~ dnorm(mu, exp(log_sigma)), 
        mu <- a + b * mass_std, 
        a ~ dnorm(0.5, 1), 
        b ~ dnorm(0, 10), 
        log_sigma ~ dnorm(0, 1)
    ), 
    data = d
)
R2_is_bad(m7_1)
```

Now we move on to adding the polynomial terms. The model specification changes slightly: the next shows the quadratic term.

\begin{eqnarray}
b_i & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i   & = & \alpha + \beta_1 \text{mass_std}_i + \beta_2 \text{mass_std}_i^2 \\
\alpha  & \sim & \text{Normal}(0.5, 1) \\
\beta_j & \sim & \text{Normal}(0, 10) ~~~~~~~~~~~ \text{ for } j = 1..2 \\
\sigma  & \sim & \text{Lognormal}(0, 1)
\end{eqnarray}

We can set up all of the models.

```{r}
poly_fits <- tibble(degree = 1:6) %>% 
    mutate(model_name = map_chr(degree, ~ str_c("m7_", .x))) %>% 
    mutate(
        formula = list(
            alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                  mu <- a + b * mass_std, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10), 
                  log_sigma ~ dnorm(0, 1)), 
            alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                  mu <- a + 
                      b[1] * mass_std + 
                      b[2] * mass_std^2, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10), 
                  log_sigma ~ dnorm(0, 1)), 
            alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                  mu <- a + 
                      b[1] * mass_std + 
                      b[2] * mass_std^2 + 
                      b[3] * mass_std^3, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10), 
                  log_sigma ~ dnorm(0, 1)), 
            alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                  mu <- a + 
                      b[1] * mass_std + 
                      b[2] * mass_std^2 + 
                      b[3] * mass_std^3 + 
                      b[4] * mass_std^4, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10), 
                  log_sigma ~ dnorm(0, 1)), 
            alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                  mu <- a + 
                      b[1] * mass_std + 
                      b[2] * mass_std^2 + 
                      b[3] * mass_std^3 + 
                      b[4] * mass_std^4 + 
                      b[5] * mass_std^5, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10), 
                  log_sigma ~ dnorm(0, 1)), 
            alist(brain_std ~ dnorm(mu, 0.001), 
                  mu <- a + 
                      b[1] * mass_std + 
                      b[2] * mass_std^2 + 
                      b[3] * mass_std^3 + 
                      b[4] * mass_std^4 + 
                      b[5] * mass_std^5 + 
                      b[6] * mass_std^6, 
                  a ~ dnorm(0.5, 1), 
                  b ~ dnorm(0, 10)))) %>% 
    mutate(start = map(degree, 
                       ~ list(b = rep(0, .x)))) %>% 
    mutate(quap_fit = map2(formula, 
                           start, 
                           ~ rethinking::quap(
                               flist = .x, 
                               data = d, 
                               start = .y)))
poly_fits
```

The last model added a sixth-order polynomial. In that case the standard deviation was fixed as $\sigma = 0.001$, for a reason that will become clear soon.

```{r}
poly_fits %>% 
    transmute(model_name, 
              R2 = map_dbl(quap_fit, ~ round(R2_is_bad(.x), digits = 2)))
```

The final model has the best score, but it is not a good model. Because it has as many parameters as there are degrees of freedom it is essentially just a recoding of the dataset. And because it's done through the medium of polynomial coefficients some weird things can happen. Focus just on the plot for the final model to see this.

```{r}
new_masses <- tibble(key = seq_len(100), 
                     mass_std = seq(from = min(d$mass_std), 
                                    to = max(d$mass_std), 
                                    length.out = 100))

predictions <- rethinking::link(poly_fits[["quap_fit"]][[6]], 
                                data = list(mass_std = new_masses$mass_std)) %>% 
    as_tibble(.name_repair = "universal") %>% 
    gather(value = "pred") %>% 
    mutate(key = str_extract(key, "\\d+") %>% 
               as.integer()) %>% 
    inner_join(new_masses, 
               by = "key")
predictions %>% 
    mutate(mass = (mass_std * sd(d$mass)) + mean(d$mass, na.rm = TRUE), 
           brain = pred * max(d$brain, na.rm = TRUE)) %>% 
    group_by(key, mass) %>% 
    summarise(mean_pred = mean(brain), 
              Q5.5 = quantile(brain, 0.055), 
              Q94.5 = quantile(brain, 0.945)) %>% 
    ggplot(aes(mass, mean_pred)) + 
    geom_line() + 
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5), 
                alpha = 0.5) + 
    geom_point(data = d, 
               aes(x = mass, y = brain), 
               colour = "steelblue", 
               size = 2, 
               alpha = 0.7) + 
    theme_bw() + 
    geom_hline(yintercept = 0, 
               size = 1, 
               linetype = 2) + 
    labs(y = "brain volume (cc)", 
         x = "body mass (kg)", 
         title = "m7_6: R^2 = 1")

```

In order to contort itself to fit the sixth-order polynomial to the data points the model predicts negative brain mass for primates with body mass of c. 59kg.

Going too far the other way can also give a bad model. An intercept-only model would be specified as:

\begin{align}
b_i & \sim \text{Normal}(\mu, \sigma) \\
\mu   & = \alpha
\end{align}

How does it perform?

```{r}
m7_7 <- tibble(
    degree = 0, 
    model_name = "m7_7", 
    formula = list(alist(brain_std ~ dnorm(mu, exp(log_sigma)), 
                         mu <- a, 
                         a ~ dnorm(0.5, 1), 
                         log_sigma ~ dnorm(0, 1))), 
    start = list(b = rep(0, 1))
) %>% 
    mutate(quap_fit = map(formula,
                          ~ rethinking::quap(
                              flist = .x, 
                              data = d)))

bind_rows(poly_fits, 
          m7_7) %>% 
    mutate(R2 = map_dbl(quap_fit, ~ round(R2_is_bad(.x), digits = 2)))
```

The negative $R^2$ is just an approximation error from zero. Can plot the results to see how it's worked.

```{r}
predictions_m7_7 <- 
    rethinking::link(m7_7[["quap_fit"]][[1]], 
                     data = list(mass_std = new_masses$mass_std)) %>% 
    as_tibble(.name_repair = "universal") %>% 
    gather(value = "pred") %>% 
    mutate(key = str_extract(key, "\\d+") %>% 
               as.integer()) %>% 
    inner_join(new_masses, 
               by = "key")
predictions_m7_7 %>% 
    mutate(mass = (mass_std * sd(d$mass)) + mean(d$mass, na.rm = TRUE), 
           brain = pred * max(d$brain, na.rm = TRUE)) %>% 
    group_by(key, mass) %>% 
    summarise(mean_pred = mean(brain), 
              Q5.5 = quantile(brain, 0.055), 
              Q94.5 = quantile(brain, 0.945)) %>% 
    ggplot(aes(mass, mean_pred)) + 
    geom_line() + 
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5), 
                alpha = 0.5) + 
    geom_point(data = d, 
               aes(x = mass, y = brain), 
               colour = "steelblue", 
               size = 2, 
               alpha = 0.7) + 
    theme_bw() + 
    labs(y = "brain volume (cc)", 
         x = "body mass (kg)", 
         title = "m7_7: R^2 = 0")
```

This is an example of **underfitting**.

Another way to conceive of overfitting and underfitting are their sensitivity to changes in the data: an overfit model is more sensitive, an undefit model less so.

```{r}
sensitivity <- poly_fits %>% 
    select(degree, quap_fit) %>% 
    filter(degree %in% c(1, 4)) %>% 
    crossing(dropped_id = 1:7) %>% 
    pmap_dfr(function(degree, quap_fit, dropped_id) {
        
        tmp <- d[-dropped_id, ]
        tmp_fit <- rethinking::quap(
            quap_fit@formula, 
            data = tmp, 
            start = list(b = rep(0, degree))
        )
        
        rethinking::link(tmp_fit, 
                         data = list(mass_std = new_masses$mass_std), 
                         refresh = 0) %>% 
            as_tibble() %>% 
            gather(value = "pred") %>% 
            mutate(key = str_extract(key, "\\d+") %>% 
                       as.integer()) %>% 
            inner_join(new_masses, 
                       by = "key") %>% 
            mutate(mass = (mass_std * sd(d$mass)) + mean(d$mass, 
                                                         na.rm = TRUE), 
                   brain = pred * max(d$brain, 
                                      na.rm = TRUE), 
                   dropped_id = dropped_id) %>% 
            mutate(model_name = str_c("m7_", degree)) %>% 
            group_by(model_name, dropped_id, mass) %>% 
            summarise(mean_pred = mean(brain)) %>% 
            ungroup()
    })
```

```{r}
sensitivity %>% 
    ggplot(aes(x = mass, 
               y = mean_pred, 
               group = dropped_id)) + 
    geom_line(colour = "grey50", 
              size = 0.3) +  
    facet_wrap(~ model_name) +
    geom_point(data = d,
               aes(x = mass, y = brain),
               colour = "steelblue", 
               alpha = 0.7, 
               inherit.aes = FALSE) +
    theme_bw() + 
    labs(y = "brain volume (cc)", 
         x = "body mass (kg)", 
         title = "Sensitivity of model to dropping one observation")
```

## Entropy and Accuracy

To walk the line between under- and overfitting we need some sort of criteria for success, and we can get to this via **information theory**, and then setting **out-of-sample deviance** as the measure of distance from the mythical (and impossible) perfect model.

There are two dimensions to consider, bearing in mind also that the right approach is completely context-dependent:

1. _Cost-benefit analysis_: what is the cost of being wrong? What is the payoff when right?
2. _Accuracy in context_: some prediction tasks are easier than others, so need to account for degree of difficulty.

The book uses the example of two weatherpersons, each of whom predicts the probability of rain. The first one, `WP1`, plays the game 'properly' (i.e. how you'd expect a real-life weatherperson to do it) and changes the prediction each day. `WP2` thinks he can do better just by guessing that there will be no rain at all.

```{r}
WP <- tibble(
    day = 1:10, 
    WP1 = c(rep(1, 3), rep(0.6, 7)), 
    WP2 = rep(0, 10), 
    observed = c(rep(1, 3), rep(0, 7))
)
WP
```

How do we measure which is better?

One approach is to use a simple hit rate, or the average chance of a correct prediction.

```{r}
WP %>% 
    mutate(WP1_correct = if_else(observed == 1, 
                                 WP1, 
                                 1 - WP1), 
           WP2_correct = if_else(observed == 1, 
                                 WP2, 
                                 1 - WP2)) %>% 
    summarise_at(vars(WP1_correct, WP2_correct), 
                 mean, 
                 na.rm = TRUE)
```

On this measure WP2 is better. But is this a good measure? This doesn't account for the asymetrical costs of being caught in the rain vs. carrying an umbrella. We can assign being caught in the rain a cost of -5, and carrying an umbrella a cost of -1. Suppose that the chance of carrying an umbrella is equal to the probability of rain predicted. We can now calculate a total cost for each weatherperson.

```{r}
WP %>% 
    mutate(WP1_cost = if_else(observed == 1,
                              (-1 * WP1) + (-5 * (1 - WP1)),
                              (-1 * WP1)),
           WP2_cost = if_else(observed == 1,
                              (-1 * WP2) + (-5 * (1 - WP2)),
                              (-1 * WP2))) %>% 
    summarise_at(vars(WP1_cost, WP2_cost), 
                 sum, 
                 na.rm = TRUE)
```

WP2's method of blind optimism doesn't look so good anymore. And it looks even worse if we consider the probability of each predicting the exact sequence of rain and sunshine.

```{r}
WP %>% 
    mutate(WP1_prob = if_else(observed == 1, 
                              WP1, 
                              1 - WP1), 
           WP2_prob = if_else(observed == 1, 
                              WP2, 
                              1 - WP2)) %>% 
    summarise_at(vars(WP1_prob, WP2_prob), 
                 prod, 
                 na.rm = TRUE)
```

By predicting with total certainty all the time WP2's probability collapses to zero, while WP1 still has a slim chance of getting the exact sequence correct. Compared with hit rate, which was the _average_ probability of a correct prediction, this is the _joint_ probability. Which is better, because it appears as the likelihood in Bayes' theorem.

Now return to the idea from earlier of the degree of difficulty in prediction. The book uses the exaample of archery, and the distance from the target (bullseye). If you were to add an extra dimension, of having to hit the target at the right time, the distance from the best to worst results would increase.

Can boil all of this down to one question:

> How much is our uncertainty reduced by learning an outcome?

So we need a way to quantify the uncertainty in a probability distribution. There are three desirable qualities:

1. _Continuous_, to prevent a situation where a small change in probabilities might cause a jump in uncertainty.
2. _Increase as number of outcomes increases_: returning to the weatherperson example, the measure of uncertainty would be greater if we added a third type of weather, such as snow, to the possibilities.
3. _Additive_: if we were to measure the uncertainty about rain/shine, and then uncertainty about hot/cold, the uncertainty over the four combinations should be the same as for the separate ones.

The function that satisfies these is the **Information Entropy**, which we call $H$. For $n$ possible events where each event $i$ has a probability $p_i$, call the vector of probabilities $\vec{p}$.

$$
H(\vec{p}) = - E \log (p_i) = - \sum_{i = 1}^{n} p_i \log(p_i)
$$

The information entropy is the average log-probability of an event.

NB. The negative sign at the front of the definition just ensures that entropy is on the interval $[0, \infty)$ instead of $(-\infty, 0]$, and the number increases as entropy does. Also note that $\log(0)$ could pose problems in the sum, so we rely on L'H&ocirc;pital's rule that says $\lim_{p_i \to 0} p_i \log(p_i) = 0$. So it's legit to say that $0 \log(0) = 0$ when computing $H(\vec{p})$, and so those terms just drop out. 

Can illustrate this with an example. Suppose the true probabilities of rain or shine are $p_1 = 0.3$ and $p_2 = 0.7$. Then:

$$
H(\vec{p}) = - \bigg( \big( 0.3 \times \log(0.3) \big) + \big( 0.7 \times \log(0.7) \big) \bigg)
$$


```{r}
p <- c(0.3, 0.7)
-sum(p * log(p))
```

But if we consider a place like Abu Dhabi where the probabilities are instead $p_1 = 0.01$ and $p_2 = 0.99$ then the entropy changes.

```{r}
p <- c(0.01, 0.99)
-sum(p * log(p))
```

Now there is much less uncertainty about any one day, compared with a place with a 30% chance of rain.

Becomes even more pronounced adding a third weather outcome, snow. 

```{r}
p <- c(0.7, 0.15, 0.15)
-sum(p * log(p))
```

An important application of entropy is **Maximum Entropy**, or finding probability distributions that are most consistent with what we know. Or: given what we know, which is the _least surprising distribution_?

This gives a measure of uncertainty in a particular structure, but we also need to measure the distance between a model and the underlying distribution. For this we use **Divergence**, in particular a measure called _Kullback-Leibler Divergence_.

Let the true probabilities be $p_1 = 0.3$ and $p_2 = 0.7$. We might approximate these with a model that assigns probabilities $q_1 = 0.25$ and $q_2 = 0.75$. Now we need to quantify the additional uncertainty created by using the approximations $\vec{q}$.

$$
D_{KL}(p, q) = \sum_i p_i \big( \log(p_i) - \log(q_i) \big) 
             = \sum_i p_i \log \bigg( \frac{p_i}{q_i} \bigg)
$$

Or: the average distance in log-probability between the target and the model.

As $q$ becomes more different from $p$, then $D_{KL}(p, q)$ will increase. It will reach its minimum when $p = q$. The plot below illustrates this for the example above where $p_1 = 0.3$ and $p_2 = 0.7$.

```{r}
tibble(p1 = 0.3, 
       q1 = seq(0.01, 0.99, length.out = 99)) %>% 
    transmute(p1, 
              p2 = 1 - p1, 
              q1, 
              q2 = 1 - q1) %>% 
    mutate(D_KL = (p1 * log(p1/q1)) +  (p2 * log(p2/q2))) %>% 
    ggplot(aes(x = q1, y = D_KL)) + 
    geom_line(colour = "steelblue") + 
    geom_vline(xintercept = 0.3, 
               linetype = 2) + 
    geom_label(aes(x = 0.35, y = 2.1, label = "q = p")) + 
    theme_bw() + 
    labs(title = "KL divergence of q from p", 
         y = NULL, 
         x = expression(q[1]))
```

When $q = p$ then the divergence is zero, but from there it grows. 

Another way to derive the divergence comes from calculating cross-entropy:

$$
H(p, q) = - \sum_i p_i \log(q_i)
$$

This averages across $p$ and $q$. $D_{KL}$ is the difference between $H(p, q)$ and $H(p)$. 

\begin{align}
D_{KL} & = H(p, q) - H(p) \\
       & = - \sum_i p_i \log(q_i) - \big( - \sum_i p_i \log(p_i) \big) \\
       & = - \sum_i p_i \big( \log(q_i) - \log(p_i) \big)
\end{align}

Note that this is asymmetrical: in general $H(p, q) \neq H(q, p)$. Why?

The example in the book is of navigating from the Earth to Mars, and predicting the probability of landing on water or land. A simple model would use the distribution of water and land on the Earth ($q$) to predict the distribution on Mars ($p$), and then we can calculate $D_{E \to M} = D_KL(p, q)$.

```{r}
q <- c(0.7, 0.3)
p <- c(0.01, 0.99)
KL_divergence <- function(p, q) {
    tmp <- sum(p * log(q/p))
    tmp[is.infinite(tmp) | is.na(tmp)] <- 0
    -1 * tmp
}
round(KL_divergence(p, q), 2)
```

But now we can calculate $D_{M \to E} = D_KL(q, p)$ for the return journey.

```{r}
round(KL_divergence(q, p), 2)
```

The divergence is higher for the return journey because Mars is almost all land, so it is 'more easily surprised' when it hits water on Earth (as it will 70% of the time). This is the notion of maximum entropy distributions mentioned earlier. So when modelling if we use distributions with high entropy to estimate the true probabilities the distance between the model and the truth will be minimised.

In practice though we do not know the target distribution, $p$. But it doesn't matter, because we are generally only interested in comparing the divergences of different candidate models, e.g $q$ and $r$. The difference would be $D_{KL}(p, q) - D_{KL}(p, r)$. But the terms for $p$ will cancel:

\begin{eqnarray}
D_{KL}(p, q) - D_{KL}(p, r) & = & 
    \bigg[- \sum_i p_i \log(q_i) - \big( - \sum_i p_i \log(p_i) \big) \bigg] - 
    \bigg[- \sum_i p_i \log(r_i) - \big( - \sum_i p_i \log(p_i) \big) \bigg]\\
    & = & - \sum_i p_i \log(q_i) - \big( - \sum_i p_i \log(r_i) \big) \\
    & = & \sum_i \log(q_i) - \big(\sum_i \log(r_i) \big) \\
\end{eqnarray}

So it's not necessary to know $p$ to assess models, as long as we are only interested in comparing them with one another. The value of $\sum_i \log(q_i)$ on its own isn't meaningful.

The convention is to call this the score for a model, summing over all the observations $i$.

$$
S(q) = \sum_i log(q_i)
$$

This is just the total score, rather than its expected value.

For Bayesian statistics we need to calculate this score for the whole posterior distribution. We do this by calculating the log of the average probability for each observation, where that average is taken over the whole posterior distribution. This is called the **log-pointwise-predictive density**. For some data $y$ and posterior distribution $\Theta$:

$$
\text{lppd}(y, \Theta) = \sum_i \log \frac{1}{S} \sum_s p (y_i | \Theta_s)
$$

This seems simple: compute the probability for each observation, $i$, from each sample, $s$, take the average, then its log, then sum over all observations. It needs some care to do the required arithmetic with precision. Two functions from McElreath's `{rethinking}` package can help:

```{r}
# to take the log of the sum of the exponentiated values for a vector in a 
# numerically stable way
body(rethinking::log_sum_exp)
# to calculate the lppd for a quap fit
body(rethinking::lppd)
```

```{r}
set.seed(1)
sims <- rethinking::sim(poly_fits[["quap_fit"]][[1]])
set.seed(1)
ll <- rethinking::sim(poly_fits[["quap_fit"]][[1]], 
                      ll = TRUE)
n <- ncol(ll)
ns <- nrow(ll)
f <- function(j) {
    rethinking::log_sum_exp(ll[, j]) - log(ns)
}
sum(sapply(seq_len(n), f))
sum(rethinking::lppd(poly_fits[["quap_fit"]][[1]]))
```

```{r}
log_lik(read_rds("Stan/ch_07/b7_1.rds"))
apply(log_lik(read_rds("Stan/ch_07/b7_1.rds")), 
      2, 
      function(x) log_sum_exp(x) - log(length(x)))
```

```{r}
poly_fits %>% 
    select(model_name, 
           quap_fit) %>% 
    mutate(lppd = map_dbl(quap_fit, 
                          ~ rethinking::lppd(.x, n = 1e4) %>% 
                              sum()
                          ))
```

Creating a `log_sum_exp()` function is straightforward enough, and to work with `{brms}` models we can build on the `brms::log_lik()` function, which does the same as `rethinking::sim()`.

```{r}
log_sum_exp <- function(x) {
    xmax <- max(x)
    xsum <- sum(exp(x - xmax))
    xmax + log(xsum)
    # max(x) + log(sum(exp(x - max(x))))
}

lppd <- function(fit, ...) {
    ll <- brms::log_lik(fit, ...)
    purrr::map_dbl(
        seq_len(ncol(ll)), 
        ~ rethinking::log_sum_exp(ll[, .x]) - log(nrow(ll))
    )
}

# lppd(b7_1)
brms::loo(b7_1)
# log_lik(b7_1, pointwise = TRUE)
```

```{r}
b7_1 <- read_rds("Stan/ch_07/b7_1.rds")
b7_2 <- read_rds("Stan/ch_07/b7_2.rds")
b7_3 <- read_rds("Stan/ch_07/b7_3.rds")
b7_4 <- read_rds("Stan/ch_07/b7_4.rds")
b7_5 <- read_rds("Stan/ch_07/b7_5.rds")
b7_6 <- read_rds("Stan/ch_07/b7_6.rds")
```

```{r}
tibble(fit = list(b7_1, 
       b7_2, 
       b7_3, 
       b7_4, 
       b7_5, 
       b7_6)) %>% 
    mutate(WAIC = map(fit, WAIC))
```

```{r}
WAIC(b7_1)
brms::loo(b7_1)
```

```{r}
ll <- b7_3 %>% 
    log_lik() %>% 
    as_tibble(.name_repair = ~ d$species)

glimpse(ll)
```

```{r}
ll %>% 
    mutate(sums = rowSums(.)) %>% 
    mutate(deviance = -2 * sums) %>% 
    ggplot(aes(x = deviance, y = 0)) + 
    geom_halfeyeh(fill = "grey50", 
                  colour = "grey30", 
                  point_interval = median_qi, 
                  .width = 0.89) + 
    scale_x_continuous(breaks = quantile(.data$deviance, c(0.055, 
                                                           0.5, 
                                                           0.945)), 
                       labels = quantile(.data$deviance, c(0.055, 
                                                           0.5, 
                                                           0.945)) %>% 
                           round(2))
```

b7_2
b7_3
b7_4
b7_5
b7_6

```{r}
map(seq_len(4), 
    ~ list(Intercept = mean(d$brain),
           mass_s    = 0,
              sigma     = sd(d$brain)))
```

