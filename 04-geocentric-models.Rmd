---
title: "Geocentric Models"
output: html_document
---

```{r setup}
library(rethinking)
library(tidyverse)
library(patchwork)
library(splines)
library(zeallot)
# set up the theme
theme_set(theme_light())
```

## Why normal distributions are normal

RM illustrates some of the reasons why the normal distribution is so common. 

### Normal by addition

Gives the example of flipping coins, then of a random walk. 

```{r}
set.seed(1116)
rand_walk <- tibble(
  person_id = rep(seq_len(1000), each = 17), 
  step_id = rep(seq(0, 16), 1000)
) %>% 
  mutate(rand = sign(step_id) * runif(17 * 1000, -1, 1)) %>% 
  group_by(person_id) %>% 
  mutate(x = cumsum(rand)) %>% 
  ungroup()
```

```{r}
rand_walk %>% 
  ggplot(aes(step_id, x)) + 
  geom_path(colour = "steelblue", alpha = 0.3) + 
  labs(x = "Step number", y = "Position")
```

```{r}
rand_walk %>% 
  filter(step_id %in% c(4L, 8L, 16L)) %>% 
  mutate(
    grp = factor(sprintf("Steps = %s", step_id)) %>% 
      fct_inorder()
  ) %>% 
  ggplot(aes(x)) + 
  geom_density(colour = "steelblue", bw = 0.2) + 
  facet_wrap(~ grp, scales = "free_y") + 
  labs(x = "Position", caption = "Gaussian kernel with sd = 0.2")
```

### Normal by multiplication

```{r}
set.seed(1458)
growth_deviations <- tibble(
  id = rep(seq_len(10000), each = 12), 
  big = runif(10000 * 12, 1, 1.5), 
  medium = runif(10000 * 12, 1, 1.1), 
  small = runif(10000 * 12, 1, 1.01)
)

growth_deviations %>% 
  pivot_longer(cols = -id, names_to = "grp") %>% 
  mutate(grp = factor(grp, levels = c("big", "medium", "small"))) %>%
  group_by(id, grp) %>% 
  summarise(x = prod(value), .groups = "drop") %>% 
  ggplot(aes(x)) + 
  geom_density(colour = "steelblue") + 
  facet_wrap(~ grp, scales = "free")
```

### Normal by log-multiplication

Can take the `big` deviations from the last example, which didn't give a very Gaussian shape, and plot them on a log scale. 

```{r}
log_growth <- growth_deviations %>% 
  select(id, big) %>% 
  group_by(id) %>% 
  summarise(x = log(prod(big)), .groups = "drop")

log_growth %>% 
  ggplot(aes(x)) + 
  geom_density(colour = "steelblue") + 
  stat_function(
    colour = "firebrick", 
    fun = dnorm, 
    args = list(mean = mean(log_growth$x), sd = sd(log_growth$x))
  )
```

### Using Gaussian distributions

In the Rethinking box RM points out that in some domains Gaussian distributions are not safe, e.g. in financial time series: in the short-term fluctuations look Gaussian, but over longer periods this will not hold. 

## A language for describing models

RM lays out the process for specifying models: 

1. Define a set of _variables_ (observed variables are _data_, unobserved variables are _parameters_); 
2. Define those variables in relation to one another and/or in terms of a probability distribution; 
3. The combination of variables and distributions gives a _joint generative model_. 

The layout will be something like: 

$$
\begin{align}
y_i   &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &=    \beta x_i \\
\beta &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \text{Exponential}(1) \\
x_i &\sim \mathcal{N}(0, 1) \\
\end{align}
$$

## Gaussian model of height

### The data

Start building up a model with the !Kung data, using only adults for the moment. 

```{r}
data("Howell1")
d <- Howell1 %>% 
  as_tibble()
adult_kung <- d %>% 
  filter(age >= 18)
adult_kung
```

```{r}
precis(adult_kung)
```

### The model

Plot the heights. 

```{r}
adult_kung %>% 
  ggplot(aes(height)) + 
  geom_density(colour = "steelblue", bw = 1)
```

We specify our model of height as: 

$$
\begin{align}
h_i    &\sim \mathcal{N}(\mu, \sigma) \\
\mu    &\sim \mathcal{N}(178, 20) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

The first line is the likelihood, the other two are priors. 

RM notes in the Rethinking box that this model assumes the $h_i$ are independent and identically distributed, which is almost certainly untrue (e.g. of family members). However in practice the assumption is not really a physical assumption, rather an _onotological_ one. In short: absent some knowledge about the structure of the correlations, the iid assumption ends up being OK. 

We can learn more about our priors with plots, starting with the prior on $\mu$.

```{r}
ggplot(data = tibble(x = c(100, 250)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 178, sd = 20)) + 
  labs(
    title = paste(expression(mu), "~", expression(dnorm(178, 20))), 
    x = expression(mu), 
    y = "Density"
  )
```

Now on $\sigma$. 

```{r}
ggplot(data = tibble(x = c(-10, 60)), aes(x)) + 
  stat_function(fun = dunif, args = list(min = 0, max = 50), n = 1001) + 
  labs(
    title = paste(expression(sigma), "~", expression(dunif(0, 50))), 
    x = expression(sigma), 
    y = NULL
  ) + 
  theme(
    panel.grid = element_blank(), 
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank()
  )
```

It's only really possible to understand the implications once we do some _Prior Predictive Simulation_. 

```{r}
set.seed(1817)
N <- 1e4
pp_heights <- tibble(
  mu = rnorm(N, 178, 20), 
  sigma = runif(N, 0, 50)
) %>% 
  mutate(prior_h = rnorm(N, mean = mu, sd = sigma))

pp_heights %>% 
  ggplot(aes(prior_h)) + 
  geom_density(colour = "steelblue") + 
  labs(
    title = paste(expression(h), "~", expression(dnorm(mu, sigma)))
  )
```

Can illustrate what happens with a bad prior also, where $\mu \sim \mathcal{N}(178, 100)$. 

```{r}
set.seed(1826)
N <- 1e4
pp_heights_wide <- tibble(
  mu = rnorm(N, 178, 100), 
  sigma = runif(N, 0, 50)
) %>% 
  mutate(prior_h = rnorm(N, mean = mu, sd = sigma))

pp_heights_wide %>% 
  ggplot(aes(prior_h)) + 
  geom_density(colour = "steelblue") + 
  geom_vline(xintercept = 0, linetype = 2) + 
  labs(title = paste(expression(h), "~", expression(dnorm(mu, sigma))))
```

Our model now expects to see many people with heights greater than any recorded. More worrying: c. `scales::percent(mean(pp_heights_wide$prior_h <= 0))` of the probability mass is below 0. 

### Grid approximation of the posterior distribution

This technique won't last much longer, but we use it here. 

```{r}
post_height <- expand_grid(
  mu = seq(150, 160, length.out = 100), 
  sigma = seq(7, 9, length.out = 100)
) %>% 
  mutate(
    LL = map2_dbl(
      mu, 
      sigma, 
      ~ sum(dnorm(adult_kung$height, .x, .y, log = TRUE)
      )
    )
  ) %>% 
  mutate(
    prod = LL + dnorm(mu, 178, 20, log = TRUE) + dunif(sigma, 0, 50, TRUE)
  ) %>% 
  mutate(prob = exp(prod - max(prod)))
```

Can get a contour plot:

```{r}
post_height %>% 
  ggplot(aes(x = mu, y = sigma, z = prob)) + 
  geom_contour() + 
  coord_cartesian(xlim = c(150, 160), ylim = c(7, 9)) + 
  labs(x = expression(mu), y = expression(sigma))
```

Or a heatmap. 

```{r}
post_height %>% 
  ggplot(aes(x = mu, y = sigma, fill = prob)) + 
  geom_raster(interpolate = TRUE) + 
  scale_fill_viridis_c() + 
  coord_cartesian(xlim = c(150, 160), ylim = c(7, 9)) + 
  labs(x = expression(mu), y = expression(sigma))
```

### Sampling from the posterior

Similar approach to before, made easier by using tidyverse. 

```{r}
set.seed(1853)
post_samples <- post_height %>% 
  sample_n(size = 1e4, replace = TRUE, weight = prob)

post_samples %>% 
  ggplot(aes(mu, sigma)) + 
  geom_point(colour = "steelblue", alpha = 0.3)
```

Can also plot the densities. 

```{r}
post_samples %>% 
  select(mu, sigma) %>% 
  pivot_longer(everything(), names_to = "parameter") %>% 
  ggplot(aes(value)) + 
  geom_density(colour = "steelblue") + 
  facet_wrap(~ parameter, scales = "free")
```

More skew on $\sigma$, which makes sense: it is bounded below by 0, so there is more freedom for it to vary up than down. 

### Finding the posterior distribution with `quap`

Goodbye grid approximation. We use the `adult_kung` data again. 

```{r}
flist_4_1 <- alist(
  height ~ dnorm(mu, sigma), 
  mu ~ dnorm(178, 20), 
  sigma ~ dunif(0, 50)
)

m4_1 <- quap(flist = flist_4_1, data = adult_kung)
```

```{r}
precis(m4_1)
```

Can set start values, which also highlights the difference between `alist()` and `list()`: only in the latter are the arguments evaluated. 

```{r}
m4_1 <- quap(
  flist = flist_4_1, 
  data = adult_kung, 
  start = list(
    mu = mean(adult_kung$height), 
    sigma = sd(adult_kung$height)
  )
)

precis(m4_1)
```

Now we try again with a more informative prior: nothing changes but the standard deviation on $\mu$.

```{r}
flist_4_2 <- alist(
  height ~ dnorm(mu, sigma), 
  mu ~ dnorm(178, 0.1), 
  sigma ~ dunif(0, 50)
)

m4_2 <- quap(flist = flist_4_2, data = adult_kung)

precis(m4_2)
```

The posterior estimate for $\mu$ is near-identical to the prior. Because of this the estimate for $\sigma$ has increased dramatically. 

### Sampling from a `quap`

The key step is understanding that the result from `quap()` is a set of quadratic approximations to the posteriors of each of the parameters, which is just a multivariate normal distribution. So we can sample from them jointly to get samples. We can extract the covariance matrix for a `quap()` result easily. 

```{r}
vcov(m4_1)
```

RM shows that the components of this matrix (viz. the variances on the diagonal and the correlation matrix) may be easier to work with directly. 

```{r}
diag(vcov(m4_1))
cov2cor(vcov(m4_1))
```

Extracting samples from the posterior estimate is easy with RM's package. 

```{r}
set.seed(1141)
quap_post <- extract.samples(m4_1, n = 1e4) %>% 
  as_tibble()
precis(quap_post)
```

## Linear prediction

Now we add a predictor variable to the mix. Weight seems a good choice. 

```{r}
adult_kung %>% 
  ggplot(aes(weight, height)) + 
  geom_point(alpha = 0.4, colour = "steelblue")
```

### The linear model strategy

We modify the previous model thus: 

$$
\begin{align}
h_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &=    \alpha + \beta(x_i - \bar{x}) \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta  &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

Here $x_i$ is the weight of the i'th person, and $\bar{x}$ is the mean weight across all people. Subtracting the mean (i.e. centering) is important here because it means that $\alpha$ now has the interpretation as the expected height of a person when they have the mean weight. 

#### Priors

We want to sample from our priors: do they give reasonable results?

```{r}
set.seed(2971)
tibble(id = seq_len(100), a = rnorm(100, 178, 20), b = rnorm(100, 0, 10)) %>% 
  expand(nesting(id, a, b), weight = range(adult_kung$weight)) %>% 
  mutate(height = a + (b * (weight - mean(adult_kung$weight)))) %>% 
  ggplot(aes(x = weight, y = height, group = id)) + 
  geom_line(alpha = 0.2) + 
  geom_hline(yintercept = 0, linetype = 2) + 
  geom_hline(yintercept = 272)
```

The dashed line is at zero, and the solid line is the height of the tallest person in history. The prior model makes many predictions that are impossible. We can do better simply by constraining the $\beta$ parameter to be non-negative: it doesn't make sense that weight would have a negative association with height. 

Updated model is: 

$$
\begin{align}
h_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &= \alpha + \beta(x_i - \bar{x}) \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta &\sim \text{Lognormal}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

Now redo the prior predictive simulation. 

```{r}
set.seed(2971)
tibble(id = seq_len(100), a = rnorm(100, 178, 20), b = rlnorm(100, 0, 1)) %>% 
  expand(nesting(id, a, b), weight = range(adult_kung$weight)) %>% 
  mutate(height = a + (b * (weight - mean(adult_kung$weight)))) %>% 
  ggplot(aes(x = weight, y = height, group = id)) + 
  geom_line(alpha = 0.2) + 
  geom_hline(yintercept = 0, linetype = 2) + 
  geom_hline(yintercept = 272)
```

There are a few very tall people predicted, but no-one with negative height. 

### Finding the posterior distribution

Fit the model again with `quap()`, using the new prior. 

```{r}
xbar <- mean(adult_kung$weight)
flist_4_3 <- alist(
  height ~ dnorm(mu, sigma), 
  mu <- a + (b * (weight - xbar)), 
  a ~ dnorm(178, 20), 
  b ~ dlnorm(0, 1), 
  sigma ~ dunif(0, 50)
)

m4_3 <- quap(flist = flist_4_3, data = adult_kung)
```

### Interpreting the posterior distribution

Most often we will need to plot simulations to understand our posterior distribution. But we start off looking at tables of numbers. 

#### Tables of marginal distributions

```{r}
precis(m4_3)
```

How to interpret these parameters? `b` is the expected change in height for a unit change in weight. The 89% estimate for `b` is all close to 1, from which we can infer that there is likely to be a positive association between height and weight. RM points out that this **does not** say that height and weight have a linear relationship, only that these are the most plausible linear relationships consistent with the data. 

To understand the full model we need the covariance matrix. 

```{r}
round(vcov(m4_3), 3)
```

There is v little covariance between the parameters. Can get this in plots with: 

```{r}
pairs(m4_3)
```

#### Plotting posterior inference against the data

Start off just plotting the mean posterior estimate with the raw data. 

```{r}
set.seed(1413)
post_4_3 <- extract.samples(m4_3) %>% 
  as_tibble()

adult_kung %>% 
  ggplot(aes(weight, height)) + 
  geom_point(alpha = 0.3, colour = "steelblue") + 
  geom_line(
    data = tibble(
      x = seq(min(adult_kung$weight), max(adult_kung$weight), length.out = 100)
    ) %>% 
      mutate(height = mean(post_4_3$a) + mean(post_4_3$b) * (x - xbar)), 
    aes(x = x, y = height)
  )
```

#### Adding uncertainty around the mean

That overstates how sure we are about the association, so we can add uncertainty. RM goes through the process of building this up for different numbers of observations. Here I just cut to the end, plotting 20 of the lines with all 352 observations. 

```{r}
set.seed(1457)

adult_kung %>% 
  ggplot(aes(weight, height)) + 
  geom_point(alpha = 0.3, colour = "steelblue") + 
  geom_line(
    data = post_4_3 %>% 
      sample_n(size = 20) %>% 
      rowid_to_column() %>% 
      select(rowid, a, b) %>% 
      expand(
        nesting(rowid, a, b), 
        x = seq(min(adult_kung$weight), max(adult_kung$weight), length.out = 100)
      ) %>% 
      transmute(
        rowid, 
        x, 
        height = a + b * (x - xbar)
      ), 
    aes(x, height, group = rowid), 
    alpha = 0.2
  )
```

#### Plotting regression intervals and contours

First we can generate samples for $\mu$ at each possible value of weight, then plot these to illustrate the uncertainty in our posterior. 

```{r}
set.seed(722)
mu <- link(m4_3, data = tibble(weight = seq(25, 70))) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = (str_sub(weight, 4L) %>% as.integer()) + 24L)
```

```{r}
mu %>% 
  ggplot(aes(weight, height)) + 
  geom_point(alpha = 0.3, colour = "steelblue")
```

Can also plot the raw data with the 89% PI overlaid. First create a helper function to get PI summaries 

```{r}
get_89_PI <- function(x) {
  tibble(x = PI(x), bound = c("lower", "upper"))
}
```

```{r}
mu_summary <- mu %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(height = mean(post_4_3$a) + mean(post_4_3$b) * (weight - xbar))

adult_kung %>% 
  ggplot(aes(weight, height)) + 
  geom_point(alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(adult_kung$weight))
```

#### Prediction intervals

The next step beyond is to propagate the uncertainty about both $\mu$ and $\sigma$ through and make predictions about our quantity of interest, height. RM has a function to do this also, called `sim()`.

```{r}
set.seed(758)
sim_heights <- sim(m4_3, data = tibble(weight = seq(25, 70))) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = (str_sub(weight, 4L) %>% as.integer()) + 24L)

sim_heights
```

Can summarise the PI for the heights at each weight. 

```{r}
height_PI <- sim_heights %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)
height_PI
```

Now put everything into a plot. 

```{r}
adult_kung %>% 
  ggplot(aes(x = weight)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(adult_kung$weight))
```

## Curves from lines

We now consider fitting curves: first with polynomial regression and then with splines. 

### Polynomial regression

Try fitting the model with a square term in height. The model is below. To make the notation simpler, $x_i$ now refers to the standardised weight (i.e. with the mean subtracted and divided by the standard deviation). 

$$
\begin{align}
h_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &= \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha &\sim \mathcal{N}(178, 20) \\
\beta_1 &\sim \text{Lognormal}(0, 1) \\
\beta_2 &\sim \mathcal{N}(0, 1) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

Note that the coefficient on the square term is not constrained to be non-negative: there isn't any physical interpretation for the square of a person's weight, so there's no good reason to set such a constraint in the prior. 

Now we can fit the model. 

```{r}
d <- Howell1 %>% 
  as_tibble() %>% 
  mutate(weight_s = scale(weight) %>% as.double()) %>% 
  mutate(weight_s2 = weight_s^2)

m4_5 <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b1 * weight_s) + (b2 * weight_s2), 
    a ~ dnorm(178, 20), 
    b1 ~ dlnorm(0, 1), 
    b2 ~ dnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = d
)
precis(m4_5)
```

Can redo the late plot from above, using this new fit. 

```{r}
set.seed(929)
weight_seq <- seq(-2.2, 2.2, length.out = 30)
mu2_summary <- link(
  m4_5, 
  data = tibble(weight_s = weight_seq, weight_s2 = weight_seq^2)
) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(
    weight_s = map_dbl(
      weight, 
      ~ weight_seq[(str_sub(.x, 4L) %>% as.integer())]
    ), 
    weight_s2 = weight_s^2
  ) %>%
  group_by(weight_s, weight_s2) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(
    height = coef(m4_5)[["a"]] + (coef(m4_5)[["b1"]] * weight_s) + 
      (coef(m4_5)[["b2"]] * weight_s2)
  )

sim_heights2 <- sim(
  m4_5, 
  data = tibble(weight_s = weight_seq, weight_s2 = weight_seq^2)
) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  transmute(
    .id, 
    weight_s = map_dbl(
      weight, 
      ~ weight_seq[(str_sub(.x, 4L) %>% as.integer())]
    ), 
    weight_s2 = weight_s^2, 
    height
  )

height_PI2 <- sim_heights2 %>% 
  group_by(weight_s, weight_s2) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)

d %>% 
  ggplot(aes(x = weight_s)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI2, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu2_summary, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(d$weight_s))
```

RM goes further and does a cubic model. 

```{r}
set.seed(1019)
d_cubic <- d %>% 
  mutate(weight_s3 = weight_s^3)

m4_6 <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b1 * weight_s) + (b2 * weight_s2) + (b3 * weight_s3), 
    a ~ dnorm(178, 20), 
    b1 ~ dlnorm(0, 1), 
    b2 ~ dnorm(0, 1), 
    b3 ~ dnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = d_cubic
)

mu_summary_cubic <- link(
  m4_6, 
  data = tibble(
    weight_s = weight_seq, 
    weight_s2 = weight_seq^2, 
    weight_s3 = weight_seq^3
  )
) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(
    weight_s = map_dbl(
      weight, 
      ~ weight_seq[(str_sub(.x, 4L) %>% as.integer())]
    ), 
    weight_s2 = weight_s^2, 
    weight_s3 = weight_s^3
  ) %>%
  group_by(weight_s, weight_s2, weight_s3) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(
    height = coef(m4_6)[["a"]] + 
      (coef(m4_6)[["b1"]] * weight_s) + 
      (coef(m4_6)[["b2"]] * weight_s2) + 
      (coef(m4_6)[["b3"]] * weight_s3)
  )

sim_heights_cubic <- sim(
  m4_6, 
  data = tibble(
    weight_s = weight_seq, 
    weight_s2 = weight_seq^2, 
    weight_s3 = weight_seq^3
  )
) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  transmute(
    .id, 
    weight_s = map_dbl(
      weight, 
      ~ weight_seq[(str_sub(.x, 4L) %>% as.integer())]
    ), 
    weight_s2 = weight_s^2, 
    weight_s3 = weight_s^3, 
    height
  )

height_PI_cubic <- sim_heights_cubic %>% 
  group_by(weight_s, weight_s2, weight_s3) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)

d_cubic %>% 
  ggplot(aes(x = weight_s)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI_cubic, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary_cubic, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  scale_x_continuous(
    trans = 
  ) + 
  coord_cartesian(xlim = range(d_cubic$weight_s))
```

### Splines

We use the cherry blossom data for this. 

```{r}
data("cherry_blossoms")
cherry <- cherry_blossoms %>% 
  as_tibble()
precis(cherry)
```

```{r}
cherry_full <- cherry %>% 
  filter(!is.na(doy))

knot_list <- cherry_full %>% 
  summarise(knots = quantile(year, probs = seq(0, 1, length.out = 15))) %>% 
  rowid_to_column("id")

B <- bs(
  cherry_full[["year"]], 
  knots = knot_list %>% filter(!(id %in% c(1L, 15L))) %>% pull(knots), 
  degree = 3, 
  intercept = TRUE
)
head(B)
```

Now we can plot these basis splines on their own. The dashed blue line at the year 1200 is there as it was in RM's plot, showing that there are four non-zero values at that value. 

```{r}
B %>% 
  as_tibble() %>% 
  mutate(year = cherry_full[["year"]]) %>% 
  pivot_longer(-year, names_to = "basis_num") %>% 
  # Need this to change the values to just a double vector, rather than having 
  # class bs/basis/matrix
  mutate(value = as.double(value)) %>% 
  mutate(dot_val = (year == 1200L) * value) %>% 
  ggplot(aes(year, value, group = basis_num)) + 
  geom_line(size = 1, alpha = 0.5) + 
  geom_vline(xintercept = 1200, linetype = 2, colour = "steelblue", size = 1) + 
  scale_y_continuous("basis value", breaks = c(0, 1), minor_breaks = NULL)
```

Now create the `quap()` model using the splines. Before that though, the model definition. 

$$
\begin{align}
D_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &=    \alpha + \sum_{k = 1}^K w_k B_{k, i} \\
\alpha &\sim \mathcal{N}(100, 10) \\
w_k    &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \text{Exponential}(1)
\end{align}
$$

```{r}
m4_7 <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (B %*% w), 
    a ~ dnorm(100, 10), 
    w ~ dnorm(0, 10), 
    sigma ~ dexp(1)
  ), 
  data = list(D = cherry_full[["doy"]], B = B), 
  start = list(w = rep(0, ncol(B)))
)
```

We don't get much from `precis()`. 

```{r}
precis(m4_7, depth = 2)
```

Hard to interpret this: what should we make of the `w[10]` coefficient? Need to plot really. 

```{r}
set.seed(733)
# Don't coerce the result to a tibble because it's already a list, including 
# vectors/matrices for each parameter separately.
cherry_post <- extract.samples(m4_7)

# In general this section is made a bit easier by waiting to turn this into
# tibbles, do a tiny bit of matrix maths first. 
w <- colMeans(cherry_post$w)

(t(w * t(B))) %>% 
  as_tibble() %>% 
  mutate(year = cherry_full$year) %>% 
  pivot_longer(-year, names_to = "basis_num") %>% 
  ggplot(aes(year, value, group = basis_num)) + 
  geom_line(size = 1, alpha = 0.5) + 
  geom_vline(xintercept = 1200, linetype = 2, colour = "steelblue", size = 1) + 
  scale_y_continuous("basis value", breaks = 0, minor_breaks = NULL) + 
  coord_cartesian(ylim = c(-6, 6))
```

Then we can plot the 97% interval for $\mu$, with the original data points. 

```{r}
set.seed(821)
cherry_mu <- link(m4_7) %>% 
  as_tibble(.name_repair = ~ str_c("yr_", cherry_full$year)) %>% 
  pivot_longer(everything(), names_to = "year", names_prefix = "yr_") %>% 
  mutate(year = as.integer(year))

cherry_mu_PI <- cherry_mu %>% 
  group_by(year) %>% 
  summarise(
    tibble(x = PI(value, prob = 0.97), bound = c("lower", "upper")), 
    .groups = "drop"
  ) %>% 
  pivot_wider(names_from = "bound", values_from = "x")

cherry_p <- cherry_full %>% 
  ggplot(aes(x = year)) + 
  geom_point(aes(y = doy), colour = "steelblue", alpha = 0.3) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = cherry_mu_PI, 
    alpha = 0.5
  )
cherry_p
```

## Practice

### Easy

4E1. 

The likelihood is $y_i \sim \mathcal{N}(\mu, \sigma)$. The other two lines are priors on the $\mu$ and $\sigma$ parameters. 

4E2. 

Two: $\mu$ and $\sigma$. 

4E3. 

$$
\text{P}(\mu, \sigma | y) = 
  \frac
    { \mathcal{N}(y|\mu, \sigma) \mathcal{N}(\mu|0, 10) \text{Expo}(\sigma|1) }
    {\int \int \mathcal{N}(y|\mu, \sigma) \mathcal{N}(\mu|0, 10) \text{Expo}(\sigma|1) d\mu d\sigma }
$$

4E4. 

The second line, $\mu_i = \alpha + \beta x_i$. 

4E5. 

Three parameters: $\alpha$, $\beta$, and $\sigma$. 

### Medium

4M1. 

```{r}
y <- rnorm(N, mean = rnorm(N, 0, 10), sd = rexp(N, 1))
```


4M2. 

The associated formula for `quap()`. 

```{r}
hw_4m2_formula <- alist(
  y ~ dnorm(mu, sigma), 
  mu ~ dnorm(0, 10), 
  sigma ~ dexp(1)
)
hw_4m2_formula
```

4M3. 

$$
\begin{align}
y_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &=    \alpha + \beta x_i \\
\alpha &\sim \mathcal{N}(0, 10) \\
\beta  &\sim \text{Uniform}(0, 1) \\
\sigma &\sim \text{Expo}(1)
\end{align}
$$

4M4. 

$$
\begin{align}
h_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta y_i \\
\alpha &\sim \mathcal{N}(100, 10) \\
\beta &\sim \text{Expo}(1) \\
\sigma &\sim \text{Expo}(1/5)
\end{align}
$$

The prior on $\beta$ ensures that children cannot shrink in a year. Other than that the model is quite basic. 

4M5. 

Every student getting taller was already in the model via $\beta \sim \text{Expo}(1)$. 

4M6. 

The parameter that governs the variance is $\sigma$, so we need to set this to keep the variance under 64cm. Could achieve this by changing the prior to $\sigma \sim \text{Uniform}(0, 8)$. 

4M7. 

Start by fitting the model again and then drawing posterior samples. 

```{r}
set.seed(1815)
m4_3_hw <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b * (weight)), 
    a ~ dnorm(178, 20), 
    b ~ dlnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = adult_kung
)


post_4_3_hw <- extract.samples(m4_3_hw) %>% 
  as_tibble()
```

Compares the covariance between the predictors now to what we saw in the original model. 

```{r}
round(vcov(m4_3_hw), 3)

round(vcov(m4_3), 3)
```

There is now some covariance between the predictors where there was almost none before. Easier to compare as correlations. 

```{r}
round(cov2cor(vcov(m4_3_hw)), 3)
round(cov2cor(vcov(m4_3)), 3)
```

There is now a strong negative correlation between `a` and `b`. What does that mean? As one increases the other goes down by the same amount. This makes some sense: if we increase our estimate for one part of the linear term in our model the other must go down to compensate. 

We can generate predictions from the posterior and compare these to the original model. 

```{r}
set.seed(1828)
mu_hw <- link(m4_3_hw, data = tibble(weight = seq(25, 70))) %>% 
  as_tibble(.name_repair = ~ str_c(seq(25, 70))) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.integer(weight))

mu_summary_hw <- mu_hw %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(height = mean(post_4_3$a) + mean(post_4_3$b) * (weight - xbar))

sim_heights_hw <- sim(m4_3_hw, data = tibble(weight = seq(25, 70))) %>% 
  as_tibble(.name_repair = ~ str_c(seq(25, 70))) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.integer(weight))

height_PI_hw <- sim_heights_hw %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)

hw_4m7_p1 <- adult_kung %>% 
  ggplot(aes(x = weight)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI_hw, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary_hw, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(adult_kung$weight))
```


```{r}
hw_4m7_p2 <- adult_kung %>% 
  ggplot(aes(x = weight)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(adult_kung$weight))
```

```{r}
hw_4m7_p1 + hw_4m7_p2
```

We get near-identical predictions. The main thing that changes is the interpretation of $\alpha$: instead of being the expected height of a person of average weight, it's the expected height of a person weighing nothing. 

4M8.

It will speed things up to functionalise the process. 

```{r}
hw_plot_cherry <- function(k, w_sd) {
  knot_list <- cherry_full %>% 
    summarise(knots = quantile(year, probs = seq(0, 1, length.out = k))) %>% 
    rowid_to_column("id")
  
  B <- bs(
    cherry_full[["year"]], 
    knots = knot_list %>% filter(!(id %in% c(1L, 15L))) %>% pull(knots), 
    degree = 3, 
    intercept = TRUE
  )
  
  quap_fit <- quap(
    flist = alist(
      D ~ dnorm(mu, sigma), 
      mu <- a + (B %*% w), 
      a ~ dnorm(100, 10), 
      w ~ dnorm(0, w_sd), 
      sigma ~ dexp(1)
    ), 
    data = list(D = cherry_full[["doy"]], B = B, w_sd = w_sd), 
    start = list(w = rep(0, ncol(B)))
  )
  
  set.seed(733)
  # Don't coerce the result to a tibble because it's already a list, including 
  # vectors/matrices for each parameter separately.
  cherry_post <- extract.samples(quap_fit)
  
  # In general this section is made a bit easier by waiting to turn this into
  # tibbles, do a tiny bit of matrix maths first. 
  w <- colMeans(cherry_post$w)
  
  cherry_mu <- link(quap_fit) %>% 
    as_tibble(.name_repair = ~ str_c("yr_", cherry_full$year)) %>% 
    pivot_longer(everything(), names_to = "year", names_prefix = "yr_") %>% 
    mutate(year = as.integer(year))
  
  cherry_mu_PI <- cherry_mu %>% 
    group_by(year) %>% 
    summarise(
      tibble(x = PI(value, prob = 0.97), bound = c("lower", "upper")), 
      .groups = "drop"
    ) %>% 
    pivot_wider(names_from = "bound", values_from = "x")
  
  cherry_full %>% 
    ggplot(aes(x = year)) + 
    geom_point(aes(y = doy), colour = "steelblue", alpha = 0.3) + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper), 
      data = cherry_mu_PI, 
      alpha = 0.5
    ) + 
    labs(
      title = sprintf("Spline fit with %s knots, sd = %s", k, w_sd)
    )
}
```

```{r}
hw_plot_cherry(20, 1)
```

```{r}
hw_plot_cherry(20, 10)
```


```{r}
hw_plot_cherry(20, 20)
```

```{r}
hw_plot_cherry(50, 1)
```

```{r}
hw_plot_cherry(50, 10)
```


```{r}
hw_plot_cherry(50, 20)
```

Together the number of knots and the prior on the weights control the wiggliness of the curve, either by limiting the number of different splines being fit or by allowing the weights to vary more. 

### Hard

4H1. 

Our weights are: 

```{r}
hw_new_weights <- tibble(
  weight = c(46.95, 43.72, 64.78, 32.59, 54.63)
)
hw_new_weights
```

First check: are all of these weights in the range for adults?

```{r}
hw_new_weights %>% 
  filter(weight < min(adult_kung$weight))
```

There are none below the minimum adult weight. 

```{r}
hw_new_weights %>% 
  mutate(excess = pmax(0, weight - max(adult_kung$weight)))
```

There's one of these weights that is slightly above the heaviest person in our dataset, but it's close enough that we can work with it. Can reuse `m4_3` from earlier to generate the predictions. 

```{r}
set.seed(1112)
sim(m4_3, data = hw_new_weights) %>% 
  as_tibble(.name_repair = ~ str_c("wt_", seq_len(5))) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  summarise(
    across(
      value, 
      .fns = list(
        mean = mean, 
        PI_lower = ~ PI(.x)[1], 
        PI_upper = ~ PI(.x)[2]
      ), 
      .names = "{.fn}"
    )
  )
```

4H2. 

```{r}
child_kung <- Howell1 %>% 
  as_tibble() %>% 
  filter(age < 18)

child_xbar <- mean(child_kung$weight)

hw_m4h2 <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b * (weight - child_xbar)), 
    a ~ dnorm(140, 20), 
    b ~ dlnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = child_kung
)
```


```{r}
precis(hw_m4h2)
```

The MAP estimate for `b` is 2.72: for every 1 unit of weight we expect height to increase by 2.72. For part (b) we can plot the posterior with the raw data to get a better idea of how good the model is. 

```{r}
set.seed(1236)
hw_4h2_weights <- seq(0, ceiling(max(child_kung$weight)), by = 0.5)

post_hw4h2 <- extract.samples(hw_m4h2) %>% 
  as_tibble()

mu_hw4h2 <- link(hw_m4h2, data = tibble(weight = hw_4h2_weights)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h2_weights)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.double(weight))

mu_summary_hw4h2 <- mu_hw4h2 %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(
    height = mean(post_hw4h2$a) + mean(post_hw4h2$b) * (weight - child_xbar)
  )

sim_heights_hw4h2 <- sim(hw_m4h2, data = tibble(weight = hw_4h2_weights)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h2_weights)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.double(weight))

height_PI_hw4h2 <- sim_heights_hw4h2 %>% 
  group_by(weight) %>% 
  summarise(get_89_PI(height), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)

child_kung %>% 
  ggplot(aes(x = weight)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI_hw4h2, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary_hw4h2, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(child_kung$weight))
```

The model seems a poor fit: we have many observations outside the 89% interval. Our model overestimates height for very light and heavy children, and underestimates it for those in the middle. This suggests that our assumption of a linear relationship isn't sound. 

4H3.

For part (a) we just fit the model. 

```{r}
all_kung <- Howell1 %>% 
  as_tibble()

hw_m4h3 <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b * log(weight)), 
    a ~ dnorm(178, 20), 
    b ~ dlnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = all_kung
)
```

Inspect the results. 

```{r}
precis(hw_m4h3)
```

Now for part (b) we plot it. 

```{r}
set.seed(1236)
hw_4h3_weights <- seq(0, ceiling(max(all_kung$weight)), by = 0.5)

post_hw4h3 <- extract.samples(hw_m4h3) %>% 
  as_tibble()

mu_hw4h3 <- link(hw_m4h3, data = tibble(weight = hw_4h3_weights)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h3_weights)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.double(weight))

mu_summary_hw4h3 <- mu_hw4h3 %>% 
  group_by(weight) %>% 
  summarise(
    tibble(x = PI(height, prob = 0.97), bound = c("lower", "upper")), 
    .groups = "drop"
  ) %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(
    height = mean(post_hw4h3$a) + mean(post_hw4h3$b) * log(weight)
  )

sim_heights_hw4h3 <- sim(hw_m4h3, data = tibble(weight = hw_4h3_weights)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h3_weights)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.double(weight))

height_PI_hw4h3 <- sim_heights_hw4h3 %>% 
  group_by(weight) %>% 
  summarise(
    tibble(x = PI(height, prob = 0.97), bound = c("lower", "upper")), 
    .groups = "drop"
  ) %>% 
  pivot_wider(names_from = "bound", values_from = x)

all_kung %>% 
  ggplot(aes(x = weight)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = height_PI_hw4h3, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = height), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = height, ymin = lower, ymax = upper), 
    data = mu_summary_hw4h3, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(all_kung$weight))
```

The fit is much better, although there are still observations outside the interval. Far fewer than before though, and we expect that there should be some exceptional people in the sample. 

The fit is log-linear, so we get this logarithmic curve when plotting with the raw data. 

4H4.

```{r}
m4_5 <- quap(
  flist = alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (b1 * weight_s) + (b2 * weight_s2), 
    a ~ dnorm(178, 20), 
    b1 ~ dlnorm(0, 1), 
    b2 ~ dnorm(0, 1), 
    sigma ~ dunif(0, 50)
  ), 
  data = d
)
precis(m4_5)
```

```{r}
hw4h4_plot_prior <- function(N, 
                             mu_a, 
                             sd_a, 
                             mu_b1, 
                             sd_b1, 
                             mu_b2, 
                             sd_b2) {
  set.seed(2971)
  tibble(
    id = seq_len(N), 
    a = rnorm(N, mu_a, sd_a), 
    b1 = rnorm(N, mu_b1, sd_b1), 
    b2 = rnorm(N, mu_b2, sd_b2)
  ) %>% 
    expand(
      nesting(id, a, b1, b2), 
      weight_s = seq(-2.2, 2.2, length.out = 100)
    ) %>% 
    mutate(weight_s2 = weight_s^2) %>% 
    mutate(height = a + (b1 * weight_s) + (b2 * weight_s2)) %>% 
    ggplot(aes(x = weight_s, y = height, group = id)) + 
    geom_line(alpha = 0.2) + 
    geom_hline(yintercept = 0, linetype = 2) + 
    geom_hline(yintercept = 272)
}
```

Let's start with the prior from earlier. 

```{r}
hw4h4_plot_prior(100, 178, 20, 0, 10, 0, 10)
```

We get quite a few impossible heights, mostly because they are too high. Let's try making the prior for the square term tighter. 

```{r}
hw4h4_plot_prior(100, 178, 20, 0, 10, 0, sd_b2 = 5)
```

This does a lot better. We do have a fair number of positive parabolas, which doesn't make sense. We can try setting the mean for the prior on `b2` to be negative. Also tighten the variance on `b2`.

```{r}
hw4h4_plot_prior(100, 178, 20, 0, 10, mu_b2 = -1, sd_b2 = 3)
```

These look more reasonable. It's still a bit troubling that highest prediction for height comes at the lowest weight, but this is a weakness of parabolic regression. 

4H5. 

Let's start with a simple linear model to see how well we do. Let $y$ by the blossom date and $x$ be the March temperature. 

$$
\begin{align}
y_i    &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i  &=    \alpha + \beta x_i \\
\alpha &\sim \mathcal{N}(100, 10) \\
\beta  &\sim \mathcal{N}(0, 5) \\
\sigma &\sim \text{Expo}(5)
\end{align}
$$


```{r}
cherry_hw <- cherry %>% 
  filter(!(is.na(doy) | is.na(temp)))
hw_m4h5 <- quap(
  flist = alist(
    doy ~ dnorm(mean = mu, sd = sigma), 
    mu <- a + (b * temp), 
    a ~ dnorm(mean = 100, sd = 10), 
    b ~ dnorm(mean = 0, sd = 10), 
    sigma ~ dexp(1)
  ), 
  data = cherry_hw
)
precis(hw_m4h5)
```

```{r}
set.seed(1236)
hw_4h5_temps <- seq(4.5, 8.5, by = 0.1)

post_hw4h5 <- extract.samples(hw_m4h5) %>% 
  as_tibble()

mu_hw4h5 <- link(hw_m4h5, data = tibble(temp = hw_4h5_temps)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h5_temps)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "temp", values_to = "doy") %>% 
  mutate(temp = as.double(temp))

mu_summary_hw4h5 <- mu_hw4h5 %>% 
  group_by(temp) %>% 
  summarise(get_89_PI(doy), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x) %>% 
  mutate(
    doy = mean(post_hw4h5$a) + mean(post_hw4h5$b) * temp
  )

sim_doy_hw4h5 <- sim(hw_m4h5, data = tibble(temp = hw_4h5_temps)) %>% 
  as_tibble(.name_repair = ~ paste(hw_4h5_temps)) %>% 
  rowid_to_column(".id") %>% 
  pivot_longer(-.id, names_to = "temp", values_to = "doy") %>% 
  mutate(temp = as.double(temp))

doy_PI_hw4h5 <- sim_doy_hw4h5 %>% 
  group_by(temp) %>% 
  summarise(get_89_PI(doy), .groups = "drop") %>% 
  pivot_wider(names_from = "bound", values_from = x)

cherry_hw %>% 
  ggplot(aes(x = temp)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = doy_PI_hw4h5, 
    alpha = 0.3
  ) + 
  geom_point(aes(y = doy), alpha = 0.3, colour = "steelblue") + 
  geom_smooth(
    aes(y = doy, ymin = lower, ymax = upper), 
    data = mu_summary_hw4h5, 
    stat = "identity", 
    colour = "grey25"
  ) + 
  coord_cartesian(xlim = range(cherry_hw$temp))
```

There does seem to be a negative association between `temp` and `doy` (i.e. as temperature rises the blossom date is lower), which makes intuitive sense: warmer temperatures should mean earlier blossom. 

4H6. 

This question sort of follows on from 4M8, where we established that the prior on the weights will control the wiggliness of the curves. Let's look at the prior predictive simulation from the model to get the whole picture. 

```{r}
# Limit the number of simulations to 100 so that the plot renders within a
# reasonable time. 
prior_pred_cherry <- function(model, B, N = 100) {
  set.seed(1640)
  
  c(alpha, sigma, W) %<-% extract.prior(model, n = 100)
  
  lines <- (W %*% t(B)) %>% 
    as_tibble(.name_repair = ~ str_c(1:827)) %>% 
    rowid_to_column(".id") %>% 
    mutate(alpha = alpha, sigma = sigma) %>% 
    pivot_longer(-c(.id, alpha, sigma), names_to = "spline_num") %>% 
    mutate(mu = alpha + value) %>% 
    mutate(y = rnorm(n = nrow(.), mean = mu, sd = sigma)) %>% 
    mutate(year = rep(cherry_full$year, 100))
  
  summary <- lines %>% 
    group_by(year) %>% 
    summarise(
      across(
        y, 
        .fns = list(
          avg = mean, 
          lower = ~ quantile(.x, 0.055), 
          upper = ~ quantile(.x, 0.945)
        ), 
        .names = "{.fn}"
      )
    )
  
  lines %>% 
    ggplot(aes(x = year)) + 
    geom_line(aes(y = y, group = .id), alpha = 0.3, colour = "grey50") + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper), 
      data = summary, 
      alpha = 0.4, 
      fill = "steelblue"
    ) + 
    geom_line(aes(y = avg), data = summary, colour = "steelblue") + 
    labs(
      title = "Prior predictive simulation", 
      subtitle = sprintf(
        "89pct interval ranges from %.1f to %.1f", 
        min(summary$lower), 
        min(summary$upper)
      )
    )
}
prior_pred_cherry(m4_7, B)
```

We can try running this again with tighter priors on the weights. 

```{r}
prior_pred_cherry(
  quap(
    flist = alist(
      D ~ dnorm(mu, sigma), 
      mu <- a + (B %*% w), 
      a ~ dnorm(100, 10), 
      w ~ dnorm(0, 1), 
      sigma ~ dexp(1)
    ), 
    data = list(D = cherry_full[["doy"]], B = B), 
    start = list(w = rep(0, ncol(B)))
  ), 
  B
)
```

Not only are the predictions much closer to one another (note the values on the y axis) but the lines themselves are much flatter, with smaller fluctuations from year to year. Try again now with a wider prior. 

```{r}
prior_pred_cherry(
  quap(
    flist = alist(
      D ~ dnorm(mu, sigma), 
      mu <- a + (B %*% w), 
      a ~ dnorm(100, 10), 
      w ~ dnorm(0, 15), 
      sigma ~ dexp(1)
    ), 
    data = list(D = cherry_full[["doy"]], B = B), 
    start = list(w = rep(0, ncol(B)))
  ), 
  B
)
```

The overall range and the wiggliness have increased. 

4H8. 

```{r}
hw_m4h8 <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- B %*% w, # removed `a + ` from before the spline term
    # a ~ dnorm(100, 10), Don't need this line at all, this was the prior on the 
    #                     intercept. 
    w ~ dnorm(0, 10), 
    sigma ~ dexp(1)
  ), 
  data = list(D = cherry_full[["doy"]], B = B), 
  start = list(w = rep(0, ncol(B)))
)
precis(hw_m4h8)
```

```{r}
set.seed(733)
# Don't coerce the result to a tibble because it's already a list, including 
# vectors/matrices for each parameter separately.
hw_cherry_post <- extract.samples(hw_m4h8)

hw_cherry_mu <- link(hw_m4h8) %>% 
  as_tibble(.name_repair = ~ str_c("yr_", cherry_full$year)) %>% 
  pivot_longer(everything(), names_to = "year", names_prefix = "yr_") %>% 
  mutate(year = as.integer(year))

hw_cherry_mu_PI <- hw_cherry_mu %>% 
  group_by(year) %>% 
  summarise(
    tibble(x = PI(value, prob = 0.97), bound = c("lower", "upper")), 
    .groups = "drop"
  ) %>% 
  pivot_wider(names_from = "bound", values_from = "x")

cherry_full %>% 
  ggplot(aes(x = year)) + 
  geom_point(aes(y = doy), colour = "steelblue", alpha = 0.3) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper), 
    data = hw_cherry_mu_PI, 
    alpha = 0.5
  )
```

Compare with the same plot for the original model. 

```{r}
cherry_p
```

The non-intercept model fits worse at the ends, but then is almost indistinguishable from the intercept model for the rest of the range. 
