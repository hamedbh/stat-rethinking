# Conditional Manatees

```{r}
#| label: setup
#| output: false
library(rethinking)
library(patchwork)
library(zeallot)
library(dagitty)
library(ggdag)
library(ggrepel)
library(magrittr)
library(tidyverse)
# set up the theme
theme_set(
  theme_light() + 
    theme(panel.grid = element_blank())
)
walk(list.files(here::here("R"), full.names = TRUE), source)
```

The chapter introduces the idea of interactions. 

## Building an interaction

RM introduces the dataset on terrain ruggedness and economic output. 

```{r}
data(rugged)
drugged <- rugged |> 
  as_tibble() |> 
  drop_na(rgdppc_2000) |> 
  mutate(
    log_gdp_std = log(rgdppc_2000) / mean(log(rgdppc_2000)), 
    rugged_std = rugged / max(rugged), 
    cid = as.integer(cont_africa + 1L)
  )

drugged |> 
  transmute(
    country, 
    gdp = log_gdp_std, 
    rugged = rugged_std, 
    continent = if_else(
      cont_africa == 1L, 
      "African nations", 
      "Non-African nations"
    )
  ) |> 
  mutate(gdp = gdp / mean(gdp)) |> 
  ggplot(aes(rugged, gdp, colour = continent)) + 
  geom_point() + 
  stat_smooth(
    colour = "grey30", 
    method = "lm", 
    formula = "y ~ x", 
    fullrange = TRUE
  ) + 
  geom_text_repel(
    aes(label = country), 
    colour = "black", 
    data = . %>% 
      filter(
        country %in% c("Lesotho", "Seychelles", "Tajikistan", "Switzerland")
      )
  ) + 
  scale_fill_manual(
    values = c("steelblue", "grey50"), 
    aesthetics = c("colour", "fill")
  ) + 
  facet_wrap(~ continent, scales = "free_y") + 
  labs(
    x = "ruggedness (standardized)", 
    y = "log GDP (as proportion of mean)"
  ) + 
  theme(
    legend.position = "none"
  )
```

The relationship is opposite within each group. RM suggests a DAG that may be consistent with this relationship: 

```{r}
dagify(
  G ~ R + C + U, 
  R ~ U, 
  coords = tibble(
    name = c("R", "G", "C", "U"), 
    x = c(1, 2, 3, 2), 
    y = c(1, 1, 1, 0)
  )
) |> 
  ggdag() + 
  theme_dag()
```

So that GDP is some function of the ruggedness, R, and the continent, C. 

RM discounts the idea of splitting the data and fitting models separately. One reason is that some parameters (e.g. the variance) should not necessarily vary across continents. The argument seems to be against sleepwalking into this sort of thing: if the variance should be different in each continent, model that explicitly. 

He also points out that anything using information criteria will not allow for comparing models using all the data to approaches fitting two separate models for Africa and the rest. 

### Making a rugged model

Start by fitting one model to all of the data. First attempt at the model: 

$$
\begin{align}
\log(y_i) &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i     &=    \alpha + \beta(r_i - \bar{r}) \\
\alpha    &\sim \mathcal{N}(1, 1) \\
\beta     &\sim \mathcal{N}(0, 1) \\
\sigma    &\sim \text{Exponential}(1)
\end{align}
$$


```{r}
m8_1_bad <- quap(
  flist = alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a + (b * (rugged_std - 0.215)), 
    a ~ dnorm(1, 1), 
    b ~ dnorm(0, 1), 
    sigma ~ dexp(1)
  ), 
  data = drugged
)
```

Now do the prior predictive simulation to see if these priors give something reasonable. 

```{r}
rugged_seq <- seq(-0.1, 1.1, length.out = 30)
set.seed(7)
fig_8_3_1 <- link(
  m8_1_bad, 
  post = extract.prior(m8_1_bad), 
  data = tibble(rugged_std = rugged_seq)
) |> 
  as_tibble(.name_repair = ~ str_c(rugged_seq)) |> 
  rowid_to_column() |> 
  slice_sample(n = 50) |> 
  pivot_longer(
    cols = -rowid, 
    names_to = "rugged_std", 
    names_transform = list(rugged_std = parse_number), 
    values_to = "log_gdp_std"
  ) |> 
  ggplot(aes(rugged_std, log_gdp_std, group = rowid)) + 
  geom_line(alpha = 0.5, colour = "grey50") + 
  coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + 
  geom_hline(yintercept = range(drugged$log_gdp_std), linetype = 2) + 
  labs(
    title = sprintf("a ~ dnorm(1, 1)\nb ~ dnorm(0, 1)"), 
    x = "ruggedness", 
    y = "log GDP (prop of mean)"
  ) + 
  theme(plot.title = element_text(hjust = 0.49))
fig_8_3_1
```

This prior is useless: far too many of these lines are simply implausible. 

We make the priors stricter, so our new model is: 

$$
\begin{align}
\log(y_i) &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i     &=    \alpha + \beta(r_i - \bar{r}) \\
\alpha    &\sim \mathcal{N}(1, 0.1) \\
\beta     &\sim \mathcal{N}(0, 0.3) \\
\sigma    &\sim \text{Exponential}(1)
\end{align}
$$

Now refit the model and recreate the prior plot. 

```{r}
m8_1 <- quap(
  flist = alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a + (b * (rugged_std - 0.215)), 
    a ~ dnorm(1, 0.1), 
    b ~ dnorm(0, 0.3), 
    sigma ~ dexp(1)
  ), 
  data = drugged
)
```

```{r}
set.seed(7)
fig_8_3_2 <- link(
  m8_1, 
  post = extract.prior(m8_1), 
  data = tibble(rugged_std = rugged_seq)
) |> 
  as_tibble(.name_repair = ~ str_c(rugged_seq)) |> 
  rowid_to_column() |> 
  slice_sample(n = 50) |> 
  pivot_longer(
    cols = -rowid, 
    names_to = "rugged_std", 
    names_transform = list(rugged_std = parse_number), 
    values_to = "log_gdp_std"
  ) |> 
  ggplot(aes(rugged_std, log_gdp_std, group = rowid)) + 
  geom_line(alpha = 0.5, colour = "grey50") + 
  coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + 
  geom_hline(yintercept = range(drugged$log_gdp_std), linetype = 2) + 
  labs(
    title = sprintf("a ~ dnorm(1, 0.1)\nb ~ dnorm(0, 0.3)"), 
    x = "ruggedness", 
    y = "log GDP (prop of mean)"
  ) + 
  theme(plot.title = element_text(hjust = 0.49))
fig_8_3_1 + fig_8_3_2
```

There are still some improbably strong relationships in that prior, but it's much more concentrated around values that could reasonably occur. 

```{r}
precis(m8_1)
```

The model fails to detect any relationship between ruggedness and GDP. 

### Adding an indicator variable isn't enough

The approach we used earlier with indicator variables won't work now. Why? Because that will only estimate different intercepts for each group (i.e. Africa and not-Africa). We can see from the earlier plot though that there are different _slopes_ in each group as well. So the model needs to account for this. 

RM then illustrates why it isn't enough to just estimate separate intercepts for each group, which I'll skip as the lesson seems clear. I'll fit the model though as we use it later for comparison. 

```{r}
m8_2 <- quap(
  flist = alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + (b * (rugged_std - 0.215)), 
    a[cid] ~ dnorm(1, 0.1), 
    b ~ dnorm(0, 0.3), 
    sigma ~ dexp(1)
  ), 
  data = drugged
)
```


### Adding an interaction does work

Now we build the better model with varying intercepts and slopes. The linear model part is: 

$$
\mu_i = \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]}(r_i - \bar{r})
$$

Here ${\text{CID}[i]}$ is the indicator for continent. RM notes that this is better than the conventional approach to specifying an interaction like this, which would be: 

$$
\mu_i = \alpha_{\text{CID}[i]} + (\beta + \gamma A_i)(r_i - \bar{r})
$$

This would require us to set a prior on $\gamma$ (which is hard, since it doesn't have an obvious interpretation), and would repeat the issue mentioned above of more variability in Africa than elsewhere. 

Now fit the model. 

```{r}
m8_3 <- quap(
  flist = alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + (b[cid] * (rugged_std - 0.215)), 
    a[cid] ~ dnorm(1, 0.1), 
    b[cid] ~ dnorm(0, 0.3), 
    sigma ~ dexp(1)
  ), 
  data = drugged
)
```

```{r}
precis(m8_3, depth = 2)
```

Now we get different slopes for each group: negative for non-African countries and positive for Africa (group 1 and 2 respectively). We also get estimates of the intercepts (i.e. the mean) that make sense: African countries on average have c. 90% of average GDP. 

Now compare the models: 

```{r}
set.seed(1)
compare(m8_1, m8_2, m8_3, func = PSIS)
```

We get a warning about high values for $k$, which we can plot. 

```{r}
set.seed(1)
PSIS(m8_3, pointwise = TRUE) |> 
  rowid_to_column() |> 
  ggplot(aes(rowid, k)) + 
  geom_point(colour = "steelblue", alpha = .6) + 
  geom_hline(yintercept = 0.5, linetype = 2, alpha = 0.6) + 
  scale_y_continuous(breaks = c(0.5, 1)) + 
  labs(x = NULL) + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

### Plotting an interaction

In this case it's relatively easy as we have a binary variable in our interaction, so we can just make two plots side by side. 

```{r}
set.seed(1105)
mu8_3 <- map_dfr(
  1:2, 
  ~ link(
    m8_3, 
    data = tibble(
      cid = .x, 
      rugged_std = rugged_seq
    )
  ) |> 
    as_tibble(.name_repair = ~ str_c(rugged_seq)) |> 
    rowid_to_column("sample_id") |> 
    pivot_longer(
      -sample_id, 
      names_to = "rugged_std", names_transform = list(rugged_std = parse_number)
    ) |> 
    mutate(cid = .x)
)
mu8_3_summary <- mu8_3 |>
  reframe_mean_PI(c(cid, rugged_std), width = 0.97) |> 
  pivot_wider() |> 
  mutate(cid = if_else(cid == 1L, "Non-African nations", "African nations"))

mu8_3_summary |> 
  ggplot(aes(x = rugged_std)) + 
  geom_line(aes(y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = cid), alpha = 0.4) + 
  geom_point(
    aes(y = log_gdp_std, colour = cid), 
    data = drugged |> 
      mutate(
        cid = if_else(
          cid == 1L, 
          "Non-African nations", 
          "African nations"
        )
      )
  ) + 
  geom_text_repel(
    aes(y = log_gdp_std, label = country), 
    data = drugged |> 
      mutate(
        cid = if_else(
          cid == 1L, 
          "Non-African nations", 
          "African nations"
        )
      ) |> 
      filter(
        country %in% c(
          "Equatorial Guinea", 
          "South Africa", 
          "Seychelles", 
          "Swaziland", 
          "Lesotho", 
          "Rwanda", 
          "Burundi", 
          "Luxembourg", 
          "Switzerland", 
          "Greece", 
          "Lebanon", 
          "Nepal", 
          "Tajikistan", 
          "Yemen"
        )
      ), 
    size = 3
  ) + 
  scale_fill_manual(
    aesthetics = c("fill", "colour"), values = c("steelblue", "black")
  ) + 
  facet_wrap(~ cid, scales = "free_y") + 
  labs(
    x = "ruggedness (standardised)", 
    y = "log GDP (as proportion of mean)"
  ) + 
  theme(legend.position = "none")
```

## Symmetry of interactions

RM points out that our framing of the interaction can flip but the underlying maths stays the same. Either we are: 

- Estimating the association between ruggedness and GDP conditional on the country being in Africa; or 
- Estimating the association between being in Africa and GDP conditional on the ruggedness of the country. 

The model for $\mu_i$ is: 

$$
\begin{align}
\mu_i = \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]}(r - \bar{r})
\end{align}
$$

This implies (particularly through the notation) that the slope is conditional on the continent. Rewriting the expression makes it easier to see the alternative interpretation though. 

$$
\begin{align}
\mu_i = (2 - \text{CID}_i)(\alpha_1 + \beta_1(r - \bar{r})) + (\text{CID}_i - 1)(\alpha_2 + \beta_2(r - \bar{r}))
\end{align}
$$

Exactly one of the terms in the sum will be non-zero for each country, and we can 'flip' the continent for any country and get a different estimate for $\mu$ based on its ruggedness. 

```{r}
set.seed(1105)
mu8_3_delta <- map(
  1:2, 
  ~ link(
    m8_3, 
    data = tibble(
      cid = .x, 
      rugged_std = rugged_seq
    )
  )
) |> 
  reduce(~ .y - .x) |> 
  as_tibble(.name_repair = ~ str_c(rugged_seq)) |> 
  rowid_to_column("sample_id") |> 
  pivot_longer(
    -sample_id, 
    names_to = "rugged_std", names_transform = list(rugged_std = parse_number)
  )

mu8_3_delta_summary <- mu8_3_delta |>
  reframe_mean_PI(rugged_std, width = 0.97) |> 
  pivot_wider()

mu8_3_delta_summary |> 
  ggplot(aes(x = rugged_std)) + 
  geom_line(aes(y = mean)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.4) + 
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.7) + 
  geom_text(
    aes(x = x, y = y, label = label), 
    data = tibble(
      x = c(0, 0), 
      y = c(-0.05, 0.05), 
      label = c("Africa lower GDP", "Africa higher GDP")
    ), 
    size = 4
  ) + 
  labs(
    x = "ruggedness", 
    y = "expected difference log GDP"
  )
```

## Continuous interactions

The previous example was relatively easy to visualise, but this becomes much harder with continuous interactions. 

### A winter flower

```{r}
data(tulips)
dtulips <- tulips |> 
  as_tibble() |> 
  mutate(
    W = water - mean(water),
    S = shade - mean(shade), 
    B = scales::rescale(blooms), 
    bed_idx = as.integer(bed)
  )
glimpse(dtulips)
```

We predict blooms as a function of water and shade. However these variables are both continuous and we expect them to interact. 

### The models

Can set up two models: with and without the interaction. First the model without. 

$$
\begin{align}
B_i   &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &=    \alpha + \beta_W(W_i - \bar{W}) + \beta_S(S_i - \bar{S})
\end{align}
$$

So each predictor has been mean-centred. We also scale $B$ by its maximum, to make the interpretation easier. (All done above to make this step easier.)

RM also goes through the priors, which I'll skip for time. 

```{r}
m8_4 <- quap(
  flist = alist(
    B ~ dnorm(mu, sigma), 
    mu <- a + (bW * W) + (bS * S), 
    a ~ dnorm(0.5, 0.25), 
    c(bW, bS) ~ dnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dtulips
)
```

RM suggests simulating from the prior, so we can do that. 

```{r}
set.seed(930)
prior8_4 <- extract.prior(m8_4) |> 
  as_tibble() |> 
  rowid_to_column()

(
  prior8_4 |> 
  slice_sample(n = 100) |> 
  expand(
    nesting(rowid, a, bW), 
    W = seq(-1.5, 1.5, length.out = 50)
  ) |> 
  mutate(B = a + (bW * W)) |> 
  ggplot(aes(W, B, group = rowid)) + 
  geom_line(alpha = 0.3) + 
  geom_hline(yintercept = c(0, 1), linetype = 2, alpha = 0.8)
) + (
  prior8_4 |> 
  slice_sample(n = 100) |> 
  expand(
    nesting(rowid, a, bS), 
    S = seq(-1.5, 1.5, length.out = 50)
  ) |> 
  mutate(B = a + (bS * S)) |> 
  ggplot(aes(S, B, group = rowid)) + 
  geom_line(alpha = 0.3) + 
  geom_hline(yintercept = c(0, 1), linetype = 2, alpha = 0.8)
)
```

There are some improbable lines but most of them are within the range of reasonable values. 

Now we can build the model including the interaction. RM shows that the traditional presentation of an interaction term derives from replacing the coefficient for one of the predictors with a linear model, and then expanding it out. 

$$
\begin{align}
\mu_i         &= \alpha + \gamma_{W, i} W_i + \beta_S S_i \\
\gamma_{W, i} &= \beta_W + \beta_{WS} S_i \\
\mu_i         &= \alpha + (\beta_W + \beta_{WS} S_i) W_i + \beta_S S_i \\
              &= \alpha + \beta_W W_i + \beta_S S_i + \beta_{WS} W_i S_i
\end{align}
$$

The process is completely symmetrical, so we could have replaced $\beta_S$ with a linear model instead and got the same answer. If we did both at once we would just find that instead of $\beta_{WS}$ as the coefficient for the product we would get $\beta_{WS} + \beta_{SW}$, which we can relabel easily as $\beta_{WS}$. So all of these are mathematically equivalent. 

So the interaction model is: 

$$
\begin{align}
B_i   &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &=    \alpha + \beta_W W_i + \beta_S S_i + \beta_{WS} W_i S_i
\end{align}
$$

```{r}
m8_5 <- quap(
  flist = alist(
    B ~ dnorm(mu, sigma), 
    mu <- a + (bW * W) + (bS * S) + (bWS * W * S), 
    a ~ dnorm(0.5, 0.25), 
    c(bW, bS, bWS) ~ dnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dtulips
)

precis(m8_5)
```

### Plotting posterior predictions

RM suggests triptych plots: each shows the effect of one predictor on the outcome with the other predictor held constant. The three plots would have low, medium, and high values for the constant predictor. It's easier here because we have just three possible values for each predictor. 

```{r}
plot_tulips <- function(quap_fit) {
  mod_name <- deparse(match.call()[[2]])
  map_dfr(
    c(-1, 0, 1), 
    ~ link(quap_fit, data = tibble(S = .x, W = c(-1, 0, 1))) |> 
      as_tibble(.name_repair = ~ str_c(c(-1, 0, 1))) |> 
      slice_sample(n = 20) |> 
      rowid_to_column("sample_id") |> 
      pivot_longer(
        -sample_id, 
        names_to = "W", 
        names_transform = list(W = parse_number), 
        values_to = "B"
      ) |> 
      mutate(S = .x)
  ) |> 
    mutate(S = sprintf("%s post: shade = %s", mod_name, S)) |> 
    ggplot(aes(W, B)) + 
    geom_point(
      data = dtulips |> 
        mutate(S = sprintf("%s post: shade = %s", mod_name, S)), 
      colour = "steelblue"
    ) + 
    geom_line(aes(group = sample_id), alpha = 0.4) + 
    scale_y_continuous("blooms", breaks = c(0, 0.5, 1)) + 
    scale_x_continuous("water", breaks = c(0, 0.5, 1)) + 
    facet_wrap(~ S)
}
set.seed(1033)
plot_tulips(m8_4) /
  plot_tulips(m8_5)
```

### Plotting prior predictions

```{r}
plot_prior_tulips <- function(quap_fit) {
  mod_name <- deparse(match.call()[[2]])
  map_dfr(
    c(-1, 0, 1), 
    ~ link(
      quap_fit, 
      data = tibble(S = .x, W = c(-1, 0, 1)), 
      post = extract.prior(quap_fit)
    ) |> 
      as_tibble(.name_repair = ~ str_c(c(-1, 0, 1))) |> 
      slice_sample(n = 20) |> 
      rowid_to_column("sample_id") |> 
      pivot_longer(
        -sample_id, 
        names_to = "W", 
        names_transform = list(W = parse_number), 
        values_to = "B"
      ) |> 
      mutate(S = .x)
  ) |> 
    mutate(S = sprintf("%s prior: shade = %s", mod_name, S)) |> 
    ggplot(aes(W, B)) + 
    geom_line(aes(group = sample_id), alpha = 0.4) + 
    geom_hline(yintercept = c(0, 1), linetype = 2, alpha = 0.8) + 
    scale_y_continuous("blooms", breaks = c(0, 0.5, 1)) + 
    scale_x_continuous("water", breaks = c(0, 0.5, 1), limits = c(-1, 1)) + 
    facet_wrap(~ S)
}
set.seed(1033)
plot_prior_tulips(m8_4) /
  plot_prior_tulips(m8_5)
```

## Summary

The embedded linear model thing was genuinely eye-opening. Also interesting that interactions are not specified in a DAG: the DAG shows only which variables affect another, but not how they may combine to do so. 

## Practice

### Easy

8E1. 

1. Ambient temperature; 
2. Geographical location; 
3. Which gear the car is in. 

8E2. 

Only the first one is an interaction, the others could be independent of one another (i.e. additive only). 

8E3. 

For the onions: 

$$
\mu_i = \alpha + \beta_T T_i + \beta_M M_i + \beta_{TM} T_i M_i
$$

For the car: 

$$
\mu_i = \alpha + \beta_C C_i + \beta_F F_i
$$

For the political beliefs: 

$$
\mu_i = \alpha + \beta_P P_i + \beta_F F_i
$$

For intelligence: 

$$
\mu_i = \alpha + \beta_S S_i + \beta_M M_i
$$

### Medium

8M1. 

If there were no blooms at all at the higher temperature $T$ then we can say there's an interaction between all three predictors: we know that water and shade interact, but they now interact with temperature also. 

8M2. 

We could rewrite the model from above as follows, supposing that $T$ takes the value 0 when it is low and 1 when it's high. 

$$
\begin{align}
\mu_i &= (1 - T_i)(\alpha + \beta_W W_i + \beta_S S_i + \beta_{WS} W_i S_i) \\
      &= \alpha + \beta_W W_i + \beta_S S_i + \beta_{WS} W_i S_i - 
          T_i \alpha - T_i \beta_W W_i - T_i \beta_S S_i - T_i \beta_{WS} W_i S_i \\
      &=  (1 - T_i)\alpha + (1 - T_i)\beta_W W_i + (1 - T_i)\beta_S S_i + (1 - T_i)\beta_{WS} W_i S_i
\end{align}
$$

The last form is maybe the most suggestive: we see that $1 - T_i$ is just killing all of the terms whenever the temperature is high. 

8M3. 

The question says that ravens depend on wolves, but not the reverse. So the DAG would be: 

```{r}
dagify(R ~ W) |> 
  ggdag() + 
  theme_dag()
```

There's no real interaction here though: when wolves go up so will ravens, but that association isn't conditional on anything else (at least not according to the question). Some example data might be: 

```{r}
tibble(wolves = rpois(10, 20)) |> 
  mutate(ravens = rpois(10, 3 * (wolves + rnorm(10, sd = 4))))
```

8M4. 

```{r}
set.seed(1126)
quap(
    flist = alist(
      B ~ dnorm(mu, sigma), 
      mu <- a + (bW * W) - (bS * S) + (bWS * W * S), 
      a ~ dnorm(0.5, 0.25),
      c(bW, bS) ~ dlnorm(0, 0.25), 
      bWS ~ dnorm(0, 0.25), 
      sigma ~ dexp(1)
    ), 
    data = dtulips
  ) |> 
  {\(x) plot_prior_tulips(x)}()
```

The priors need to be a bit tighter, these lines are too steep and the differences across the three panels seem a bit too stark. 

```{r}
set.seed(1126)
m8m4 <- quap(
  flist = alist(
    B ~ dnorm(mu, sigma), 
    mu <- a + (bW * W) - (bS * S) + (bWS * W * S), 
    a ~ dnorm(0.5, 0.25),
    c(bW, bS) ~ dlnorm(-2, 0.1), 
    bWS ~ dnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dtulips
) 

m8m4 |> 
  plot_prior_tulips()
```

This seems better. 

### Hard

8H1.

```{r}
m8h1 <- quap(
  flist = alist(
    B ~ dnorm(mu, sigma), 
    mu <- a[bed_idx] + (bW * W) - (bS * S) + (bWS * W * S), 
    a[bed_idx] ~ dnorm(0.5, 0.25), 
    c(bW, bS, bWS) ~ dnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dtulips
)
```

8H2. 

```{r}
compare(m8_5, m8h1)
```

There seems to be very little difference between the models: certainly the dWAIC is much smaller then the dSE, and the model with `bed` has only 75% of the weight. 

We can check the posterior estimates for the new model: 

```{r}
coeftab_plot(coeftab(m8h1))
```

The intervals for beds 2 and 3 are almost identical. There seems to be some difference with bed 1, but even that interval overlaps a little with bed 2. So the `bed` variable just isn't that helpful, which explains why the WAIC scores were so similar. 

8H3. 

```{r}
set.seed(1250)
tibble(
  country = drugged[["country"]], 
  k = PSIS(m8_3, pointwise = TRUE)[["k"]], 
  penalty = WAIC(m8_3, pointwise = TRUE)[["penalty"]]
) |> 
  ggplot(aes(k, penalty)) + 
  geom_point(colour = "steelblue") + 
  geom_vline(xintercept = 0.5, linetype = 2, alpha = 0.2) + 
  geom_text_repel(
    aes(label = country), 
    data = . %>% 
      filter(k > 0.5 | penalty > 0.4)
  ) + 
  scale_x_continuous("PSIS Pareto k", breaks = c(0, 0.5, 1))
```

Switzerland has a relatively high penalty, which is presumably because it is very rich and rugged. 

Can now refit the model using robust regression. 

```{r}
m8h3b <- quap(
  flist = alist(
    log_gdp_std ~ dstudent(2, mu, sigma), 
    mu <- a[cid] + (b[cid] * (rugged_std - 0.215)), 
    a[cid] ~ dnorm(1, 0.1), 
    b[cid] ~ dnorm(0, 0.3), 
    sigma ~ dexp(1)
  ), 
  data = drugged
)
set.seed(1250)
tibble(
  country = drugged[["country"]], 
  k = PSIS(m8h3b, pointwise = TRUE)[["k"]], 
  penalty = WAIC(m8h3b, pointwise = TRUE)[["penalty"]]
) |> 
  ggplot(aes(k, penalty)) + 
  geom_point(colour = "steelblue") + 
  geom_vline(xintercept = 0.5, linetype = 2, alpha = 0.2) + 
  geom_text_repel(
    aes(label = country), 
    data = . %>% 
      filter(k > 0.5 | penalty > 0.4)
  ) + 
  scale_x_continuous("PSIS Pareto k", breaks = c(0, 0.5, 1))
```

```{r}
compare(m8_3, m8h3b)
```

We can also compute the effect of the continent for each model. 

```{r}
set.seed(1310)
map2_dfr(
  list(m8_3, m8h3b), 
  c("Normal", "Student-t"), 
  ~ extract.samples(.x) |> 
    pluck("b") |> 
    as_tibble(.name_repair = ~ c("one", "two")) |> 
    transmute(contrast = two - one) |> 
    mutate(model = .y)
) |> 
  ggplot(aes(contrast, colour = model, fill = model)) + 
  geom_density(alpha = 0.4) + 
  scale_fill_viridis_d(aesthetics = c("colour", "fill")) + 
  labs(y = NULL, colour = NULL, fill = NULL) + 
  theme(
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank(), 
    legend.position = "bottom"
  )
```

8H4. 

```{r}
data(nettle)
dnettle <- nettle |> 
  as_tibble() |> 
  transmute(
    country, 
    A = log(area), 
    M = mean.growing.season, 
    S = sd.growing.season, 
    L = log(num.lang / k.pop)
  ) |> 
  mutate(across(where(is.double), standardize))
dnettle
```

First build the model in part a, using $A$ and $M$ as the predictors. 

```{r}
m8h4a <- quap(
  alist(
    L ~ dnorm(mu, sigma), 
    mu <- a + (bA * A) + (bM * M), 
    a ~ dnorm(0, 0.2), 
    c(bA, bM) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dnettle
)
precis(m8h4a)
```

Now the part b model, using $A$ and $S$ as predictors. 

```{r}
m8h4b <- quap(
  alist(
    L ~ dnorm(mu, sigma), 
    mu <- a + (bA * A) + (bS * S), 
    a ~ dnorm(0, 0.2), 
    c(bA, bS) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dnettle
)
precis(m8h4b)
```

Let's pause and plot the parameters. 

```{r}
coeftab(m8h4a, m8h4b) |> 
  coeftab_plot()
```

The estimates for $\beta_A$ are very similar in both models. The coefficient on $M$ seems quite solidly positive, whereas that on $S$ is mostly negative but overlaps with 0. Now we can build the model with the interaction. 

```{r}
m8h4c <- quap(
  alist(
    L ~ dnorm(mu, sigma), 
    mu <- a + (bA * A) + (bS * S) + (bM * M) + (bSM * S * M), 
    a ~ dnorm(0, 0.2), 
    c(bA, bS, bM, bSM) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dnettle
)
precis(m8h4c)
```

Now we can compare the parameter estimates and PSIS scores for the models. 

```{r}
coeftab(m8h4a, m8h4b, m8h4c) |> 
  coeftab_plot()
```

```{r}
compare(m8h4a, m8h4b, m8h4c, func = PSIS)
```

The model with all three predictors is top, but using just mean growing season gets similar performance (within one standard error). We are getting warnings about high values of $k$ though, so we can plot the points and check which are maybe too influential. 

```{r}
set.seed(1250)
tibble(
  country = dnettle[["country"]], 
  k = PSIS(m8h4c, pointwise = TRUE)[["k"]], 
  penalty = WAIC(m8h4c, pointwise = TRUE)[["penalty"]]
) |> 
  mutate(penalty_prop = penalty / sum(penalty)) |> 
  ggplot(aes(k, penalty)) + 
  geom_point(colour = "steelblue") + 
  geom_vline(xintercept = c(0.5, 0.7), linetype = 2, alpha = 0.2) + 
  geom_text_repel(
    aes(label = country), 
    data = . %>% 
      filter(k > 0.5 | penalty_prop > 0.1)
  ) + 
  scale_x_continuous("PSIS Pareto k", breaks = c(0, 0.5, 1))
```

There are some countries with very high values for $k$, in particular Vanuatu. Do the data tell us anything when considered alongside the parameter estimates from this model?

```{r}
dnettle |> 
  filter(country %in% c("Vanuatu", "Cuba", "Papua New Guinea", "Brazil"))
```


```{r}
coeftab(m8h4c) |> 
  coeftab_plot()
```

It might be easier to simulate: generate parameter samples from the posterior, then use those to simulate

```{r}
set.seed(1649)
sim(m8h4c) |> 
  as_tibble(.name_repair = ~ str_c(dnettle[["country"]])) |> 
  rowid_to_column("sample_id") |> 
  select(sample_id, "Vanuatu", "Cuba", "Papua New Guinea", "Brazil") |> 
  pivot_longer(-sample_id, names_to = "country") |> 
  reframe_mean_PI(country) |> 
  pivot_wider() |> 
  inner_join(
    dnettle |> 
      select(country, L), 
    by = "country"
  ) |> 
  ggplot(aes(y = fct_rev(country), colour = country)) + 
  geom_pointrange(aes(x = mean, xmin = lower, xmax = upper)) + 
  geom_point(aes(x = L), colour = "black") + 
  scale_colour_viridis_d(option = "D") + 
  labs(
    title = "Posterior predictions for influential observations", 
    subtitle = "Observed values in black", 
    x = "log of languages per capita", 
    y = NULL
  ) + 
  theme(legend.position = "none")
```

Now the picture is clearer: in three of the four countries the observed value is entirely outside the 89% posterior interval. The observed value for Brazil is in the interval but still near the edge. 

Does a robust regression change things?

```{r}
m8h4d <- quap(
  alist(
    L ~ dstudent(2, mu, sigma), 
    mu <- a + (bA * A) + (bS * S) + (bM * M) + (bSM * S * M), 
    a ~ dnorm(0, 0.2), 
    c(bA, bS, bM, bSM) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dnettle
)
precis(m8h4d)
```

We can repeat the plot of the posterior predictive simulation. 

```{r}
set.seed(1712)
sim(m8h4d) |> 
  as_tibble(.name_repair = ~ str_c(dnettle[["country"]])) |> 
  rowid_to_column("sample_id") |> 
  select(sample_id, "Vanuatu", "Cuba", "Papua New Guinea", "Brazil") |> 
  pivot_longer(-sample_id, names_to = "country") |> 
  reframe_mean_PI(country) |> 
  pivot_wider() |> 
  inner_join(
    dnettle |> 
      select(country, L), 
    by = "country"
  ) |> 
  ggplot(aes(y = fct_rev(country), colour = country)) + 
  geom_pointrange(aes(x = mean, xmin = lower, xmax = upper)) + 
  geom_point(aes(x = L), colour = "black") + 
  scale_colour_viridis_d(option = "D") + 
  labs(
    title = "Posterior predictions for influential observations", 
    subtitle = "Observed values in black", 
    x = "log of languages per capita", 
    y = NULL
  ) + 
  theme(legend.position = "none")
```

The observed values are still solidly outside the intervals for three of the four. 

```{r}
compare(m8h4c, m8h4d, func = PSIS)
```

The two models are nearly identical, although the robust regression does not throw up the warning about $k$. We can check whether the parameter estimates are similar also. 

```{r}
coeftab(m8h4c, m8h4d) |> 
  coeftab_plot()
```

The most noticeable change is that $\sigma$ is much smaller in the robust regression: to make the values in the tails more plausible the standard regression must estimate a higher variance. 

8H5. 

```{r}
data(Wines2012)
dwines <- Wines2012 |> 
  as_tibble() |> 
  transmute(
    judge = as.integer(judge), 
    flight = as.integer(flight), 
    wine = as.integer(wine), 
    score = standardize(score), 
    wine_US = wine.amer + 1L, 
    judge_US = judge.amer + 1L, 
    wine_US_ind = wine.amer, 
    judge_US_ind = judge.amer, 
    flight_ind = as.integer(flight) - 1L
  )
```

Our linear model specification is: 

$$
\begin{align}
S_i   &\sim \mathcal{N}(\mu, \sigma) \\
\mu_i &=    \alpha_{\text{JID}[i]} + \alpha_{\text{WID}[i]}
\end{align}
$$

In other words each score is just a weighted sum of the estimated means for the judge and wine. 

```{r}
m8h5 <- quap(
  alist(
    score ~ dnorm(mu, sigma), 
    mu <- aJ[judge] + aW[wine], 
    aJ[judge] ~ dnorm(0, 0.5), 
    aW[wine] ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dwines
)
```


```{r}
m8h5 |> 
  precis(depth = 2) |> 
  as_tibble(rownames = "name") |> 
  filter(str_detect(name, "^a")) |> 
  set_names(c("name", "mean", "sd", "lower", "upper")) |> 
  mutate(param = if_else(str_detect(name, "^aJ"), "judge", "wine")) |> 
  ggplot(aes(y = reorder(name, mean), colour = param)) + 
  geom_pointrange(aes(xmin = lower, x = mean, xmax = upper)) + 
  scale_colour_manual(values = c("steelblue", "firebrick")) + 
  labs(x = "score", y = NULL, colour = NULL)
```

Based on the plot there seems to be more variation among the judges than the wines. 

8H6. 

```{r}
m8h6 <- quap(
  alist(
    score ~ dnorm(mu, sigma), 
    mu <- aF[flight] + aWUS[wine_US] + aJUS[judge_US], 
    aF[flight] ~ dnorm(0, 0.5), 
    aWUS[wine_US] ~ dnorm(0, 0.5), 
    aJUS[judge_US] ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dwines
)
```


```{r}
m8h6 |> 
  precis(depth = 2) |> 
  as_tibble(rownames = "name") |> 
  filter(str_detect(name, "^a")) |> 
  set_names(c("name", "mean", "sd", "lower", "upper")) |> 
  mutate(
    param = case_when(
      str_detect(name, "^aF") ~ "flight", 
      str_detect(name, "^aWUS") ~ "wine_US", 
      str_detect(name, "^aJUS") ~ "judge_US", 
      TRUE ~ "uncaught_exception"
    ), 
    name = fct_recode(
      name, 
      red_wine = "aF[1]", 
      white_wine = "aF[2]", 
      wine_US = "aWUS[2]", 
      wine_non_US = "aWUS[1]", 
      judge_US = "aJUS[2]", 
      judge_non_US = "aJUS[1]"
    )
  ) |> 
  ggplot(aes(y = name, colour = param)) + 
  geom_pointrange(aes(xmin = lower, x = mean, xmax = upper)) + 
  geom_vline(xintercept = 0, linetype = 2, alpha = 0.4) + 
  labs(x = "score", y = NULL, colour = NULL)
```

The plot is suggestive of some things: 

- American wines may tend to get lower scores; 
- American judges may tend to give higher scores; 
- No difference between red and white wine. 

However to do this properly we need to calculate the contrasts and consider those directly. 

```{r}
set.seed(1135)
contrasts_8h6 <- extract.samples(m8h6)[(c("aF", "aWUS", "aJUS"))] |> 
  reduce(cbind) |> 
  as_tibble(.name_repair = ~ str_c("V", 1:6)) |> 
  transmute(
    contrast_red_white = V1 - V2, 
    contrast_wine_US = V4 - V3,
    contrast_judge_US = V6 - V5
  ) |> 
  pivot_longer(
    everything(), 
    names_to = "contrast", 
    names_pattern = "contrast_(.+)"
  )

contrasts_8h6 |> 
  ggplot(aes(value, colour = contrast, fill = contrast)) + 
  geom_density(alpha = 0.3) + 
  geom_vline(xintercept = 0, linetype = 2, alpha = 0.4) + 
  labs(y = NULL, x = "contrast") + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

The flight contrast is squarely around zero, so we can set that aside. How likely is it that the US judges are giving higher scores, or that the US wines get lower scores?

```{r}
contrasts_8h6 |> 
  filter(contrast != "red_white") |> 
  mutate(split_loc = map_int(contrast, ~ str_locate(.x, "_")[, 1])) |> 
  mutate(contrast = str_sub(contrast, end = split_loc - 1L)) |> 
  group_by(contrast) |> 
  summarise(p_gt_0 = round(mean(value > 0), digits = 2))
```

So there is a 95% chance of a US judge giving higher scores, and only a 10% chance of US wines getting higher scores. 

8H7. 

Now we revert to using indicator variables as suggested. 

```{r}
m8h7 <- quap(
  alist(
    score ~ dnorm(mu, sigma), 
    mu <- a + 
      # Based on the previous question this term should be close to zero, as we 
      # expect little difference between red and white wine. 
      (bF * flight_ind) + 
      # The previous question suggests this term should be negative, as US wines 
      # got lower scores in general
      (bWUS * wine_US_ind) + 
      # Expect this to be positive, US judges tended to give higher scores
      (bJUS * judge_US_ind) + 
      # This interaction term 'fires' when the wine was white and from the US
      (bFW * flight_ind * wine_US_ind) + 
      # This interaction fires when the wine is white and the judge is from the
      # US
      (bFJ * flight_ind * judge_US_ind) + 
      # This interaction fires when the wine and judge are both from the US
      (bWJ * wine_US_ind * judge_US_ind), 
    # There is little domain knowledge we can bring to bear. However these 
    # priors are all centred at zero, which is as much as we can do really. 
    a ~ dnorm(0, 0.5), 
    c(bF, bWUS, bJUS, bFW, bFJ, bWJ) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dwines
)
```

```{r}
coeftab(m8h7) |> 
  coeftab_plot()
```

- The `bF` parameter is not centred at zero as we would expect. The unconditional expectation is that red wines score lower. 
- `bWUS` is centred below zero as expected, but overlapping with zero. 
- `bJUS` is centred above zero as expected, but overlapping with zero. 
- `bFW` (the interaction of wine being white and from the US) is centred well above zero. This is new, not something we could have seen from the earlier models. 
- `bFJ` and `bWJ` are both centred near zero. 

We can then use `link()` as RM suggests to get draws from the linear model. The plot is inspired by [Gregor Mathes' effort](https://gregor-mathes.netlify.app/2021/03/22/rethinking-chapter-8/#hard-practices). 

```{r}
sim_grid_8h7 <- expand_grid(
  flight_ind = 0:1, 
  wine_US_ind = 0:1, 
  judge_US_ind = 0:1
) |> 
  mutate(
    label = str_c(
      if_else(judge_US_ind == 1L, "A", "F"), 
      if_else(wine_US_ind == 1L, "A", "F"),
      if_else(flight_ind == 1L, "W", "R")
    )
  )
set.seed(1649)
link(
  m8h7, 
  data = sim_grid_8h7
) |> 
  as_tibble(.name_repair = ~ sim_grid_8h7[["label"]]) |> 
  rowid_to_column("sample_id") |> 
  pivot_longer(-sample_id, names_to = "grp") |> 
  reframe_mean_PI(grp) |> 
  pivot_wider() |> 
  ggplot(aes(y = reorder(grp, mean))) + 
  geom_pointrange(aes(xmin = lower, x = mean, xmax = upper)) + 
  geom_vline(xintercept = 0, linetype = 2, alpha = 0.4) + 
  labs(
    subtitle = "AFR means American judge drinking French Red", 
    x = "score", 
    y = NULL
  )
```

The really low score for French judges drinking American reds is maybe due to wine 18, which got a terrible score (as seen in the earlier plot). However that wine got bad scores in general, not just from the French judges. 

What happens when we do posterior predictive simulation?

```{r}
set.seed(1703)
sim(m8h7, data = sim_grid_8h7) |> 
  as_tibble(.name_repair = ~ sim_grid_8h7[["label"]]) |> 
  rowid_to_column("sample_id") |> 
  pivot_longer(-sample_id, names_to = "grp") |> 
  reframe_mean_PI(grp) |> 
  pivot_wider() |> 
  ggplot(aes(y = reorder(grp, mean))) + 
  geom_pointrange(aes(xmin = lower, x = mean, xmax = upper)) + 
  geom_vline(xintercept = 0, linetype = 2, alpha = 0.4) + 
  labs(
    subtitle = "AFR means American judge drinking French Red", 
    x = "score", 
    y = NULL
  )
```

Now there is very little to choose between them: the variation is greater than the contrasts. 
