---
title: "Chapter 3: Sampling the Imaginary"
output: 
  html_document: 
    highlight: kate
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r}
library(rethinking)
library(tidyverse)
```

Vampire example from the book:

$$
\text{P(vampire|positive)} = 
\frac
{\text{P(positive|vampire)} \times \text{P(vampire)}}
{\text{P(positive)}}
$$
Rewrite the denominator:

$$
{\text{P(positive)}} = 
{\text{P(positive|vampire)P(vampire)} + 
\text{P(positive|not vampire)P(not vampire)}}
$$ 

Next we look at sampling from a grid-like posterior.

```{r}
# how many samples?
N <- 1e3
n_success <- 6
n_trials <- 9

# set up the grid
d <- tibble(
    p = seq(0, 1, length.out = N), 
    prior = 1) %>% 
    mutate(likelihood = dbinom(x = n_success, 
                               size = n_trials, 
                               prob = p)) %>% 
    mutate(posterior = (prior * likelihood)/(sum(prior * likelihood)))
```

Now ready to sample.

```{r}
sample_size <- 1e4
samples <- tibble(value = sample(d$p, 
                                 size = sample_size, 
                                 prob = d$posterior, 
                                 replace = TRUE)) %>% 
    mutate(sample_number = row_number())
head(samples)
```

Can plot the samples to see their distribution.

```{r}
# line plot to show that samples vary randomly around the centre of the 
# posterior
samples %>% 
    ggplot(aes(sample_number, value)) + 
    geom_line(size = 0.1) + 
    labs(x = "sample number",
         y = "proportion of water (p)")

# density plot to show posterior distribution, overlaid with the beta curve
samples %>% 
    ggplot(aes(value)) + 
    geom_density(fill = "black") + 
    stat_function(aes(seq(0, 1, length.out = sample_size), ..y..), 
                  fun = dbeta, 
                  n = sample_size,
                  colour = "blue", 
                  args = list(shape1 = 6 + 1, 
                              shape2 = 9 - 6 + 1)) + 
    labs(x = "proportion of water (p)")
```

Can use the grid to summarise.

```{r}
# How much of the posterior mass is for proportions of water below 0.5?
d %>% 
    filter(p < 0.5) %>% 
    summarise(sum = sum(posterior))
```

Or the samples.

```{r}
# How much mass is below 0.5?
samples %>% 
    filter(value < 0.5) %>% 
    summarise(sum = n()/sample_size)
# Or between 0.5 and 0.75?
samples %>% 
    filter(value > 0.5, value < 0.75) %>% 
    summarise(sum = n()/sample_size)
```

Can also create intervals of defined mass.

```{r}
# showing the mass below p = 0.5
d %>% 
    ggplot(aes(p)) + 
    geom_line(aes(y = posterior)) + 
    geom_ribbon(data = d %>% filter(p < 0.5), 
                aes(ymin = 0, ymax = posterior)) + 
    labs(x = "proportion of water (p)",
         y = "density")
# and between 0.5 and 0.75
d %>% 
    ggplot(aes(p)) + 
    geom_line(aes(y = posterior)) + 
    geom_ribbon(data = d %>% filter(p > 0.5, p < 0.75), 
                aes(ymin = 0, ymax = posterior)) + 
    labs(x = "proportion of water (p)",
         y = "density")
```

Can easily extract quantiles.

```{r}
(q_80 <- quantile(samples$value, 0.8))
(q_10_and_90 <- quantile(samples$value, c(0.1, 0.9)))
(iq_range <- quantile(samples$value, c(0.25, 0.75)))
(CI_89 <- quantile(samples$value, c(0.055, 0.945)))
```

Can build some plots with these values.

```{r}
d %>% 
  ggplot(aes(x = p)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p < q_80),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "lower 80%") +
  labs(x = "proportion of water (p)",
       y = "density")

d %>% 
  ggplot(aes(x = p)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p > q_10_and_90[1] & p < q_10_and_90[2]),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "middle 80%") +
  labs(x = "proportion of water (p)",
       y = "density")
```

Can easily update `d` and `samples` with new values for $n$ and $w$.

```{r}
# here we update the `dbinom()` parameters
n_success <- 3
n_trials  <- 3

# update d
d2 <- d %>% 
    mutate(likelihood = dbinom(n_success, size = n_trials, prob = p)) %>% 
    mutate(posterior  = (likelihood * prior)/sum((likelihood * prior)))

# here's our new samples tibble
(samples2 <- tibble(value = sample(d2$p, 
                                   prob = d2$posterior, 
                                   size = 1e4, 
                                   replace = TRUE)))
```

`rethinking` has convenience functions for getting these credible intervals as well as the Highest Posterior Density Interval.

```{r}
PI(samples2$value, prob = 0.89)
HPDI(samples2$value, prob = 0.89)
```

Also can use the `tidybayes` package.

```{r}
library(tidybayes)
```

```{r}
# get the CI qith qi() functions
qi(samples2$value, .width = 0.89)
# hdi gives the HPDI
hdi(samples2$value, .width = 0.89)
# can do the same with %>% 
samples2 %>% 
    pull(value) %>% 
    hdi(.width = 0.89)
# the functions of the form [mean|median\mode]_hdi() do the same with different 
# measure of central tendency, and can take a tibble as argument with the column 
# name specified. They have consistent column names so can be bound easily
bind_rows(
    samples2 %>% 
        mean_hdi(value, .width = 0.89), 
    samples2 %>% 
        median_hdi(value, .width = 0.89), 
    samples2 %>% 
        mode_hdi(value, .width = 0.89))
```

Can easily extract elements with the consistent naming.

```{r}
median_qi(samples2$value, .width = .5)[, "ymin"]
```

Use this to build plots from the book (although with the same data as above).

```{r}
d2 %>% 
    ggplot(aes(x = p)) + 
    geom_ribbon(data = d2 %>% filter(p > median_qi(samples2$value, 
                                                  .width = 0.5)[["ymin"]], 
                                    p < median_qi(samples2$value, 
                                                  .width = 0.5)[["ymax"]]), 
                aes(ymin = 0, ymax = posterior), 
                fill = "grey40") + 
    geom_line(aes(y = posterior)) + 
    labs(subtitle = "50% Percentile Interval",
       x = "proportion of water (p)",
       y = "density")

d2 %>% 
    ggplot(aes(x = p)) + 
    geom_ribbon(data = d2 %>% filter(p > median_hdi(samples2$value, 
                                                  .width = 0.5)[["ymin"]], 
                                    p < median_hdi(samples2$value, 
                                                  .width = 0.5)[["ymax"]]), 
                aes(ymin = 0, ymax = posterior), 
                fill = "grey40") + 
    geom_line(aes(y = posterior)) + 
    labs(subtitle = "50% HPDI",
       x = "proportion of water (p)",
       y = "density")
```

Can also get point estimates easily in several ways:

```{r}
# 1. by sorting the tibble, first for the original d then for d2 (with all 
# throws giving water)
map(list(d, d2), 
    ~ .x %>% 
        arrange(desc(posterior)) %>% 
        mutate(posterior = round(posterior, 4)))
# 2. Using rethinking::chainmode()
map(list(samples, samples2), 
    ~ .x %>% 
        pull(value) %>% 
        chainmode(adjust = 0.01))

# 3. Using functions from tidybayes
map(list(samples, samples2), 
    ~ .x %>% 
        mode_hdi(value))
map(list(samples, samples2), 
    ~ .x %>% 
        mode_qi(value))
# 4. Simply summarising the posterios samples
map(list(samples, samples2), 
    ~ .x %>% 
        summarise(mean = mean(value, na.rm = TRUE), 
                  median = median(value, na.rm = TRUE)))
```

Easy to combine these in a tibble for plotting.

```{r}
point_estimates <- bind_rows(samples2 %>% 
                                 mean_qi(value), 
                             samples2 %>% 
                                 median_qi(value), 
                             samples2 %>% 
                                 mode_qi(value)) %>% 
    select(value, .point) %>% 
    mutate(x = value + c(-.03, .03, -.03), 
           y = c(.0005, .00125, .002))

d2 %>% 
    ggplot(aes(x = p)) +
    geom_ribbon(aes(ymin = 0, ymax = posterior),
                fill = "grey75") +
    geom_vline(xintercept = point_estimates$value) +
    geom_text(data = point_estimates,
              aes(x = x, y = y, label = .point),
              angle = 90) +
    labs(x = "proportion of water (p)",
         y = "density") +
    theme(panel.grid = element_blank())
```

Different loss functions imply different point estimates. Can illustrate this with an example: we make a guess, $d$, of the true value of $p$. Win £100 if we guess exactly right, and lose prize money proportional to $p - d$. So if we were to guess $d = 0.5$, expected loss would be:

```{r}
d2 %>% 
    mutate(loss = posterior * abs(0.5 - p)) %>% 
    summarise(expected_loss = sum(loss))
```

Now what guess would minimise expected loss?

```{r}
make_loss <- function(guess) {
    d2 %>% 
        mutate(loss = posterior * abs(guess - p)) %>% 
        summarise(expected_loss = sum(loss)) %>% 
        pull(expected_loss)
}

(loss <- d2 %>% 
    select(decision = p) %>% 
    mutate(weighted_avg_loss = map_dbl(decision, make_loss)))
loss %>% 
    top_n(-1, weighted_avg_loss)
```

Plot the loss as a function of $d$.

```{r}
min_loss <- loss %>% 
    top_n(-1, weighted_avg_loss) %>% 
    as.numeric()
loss %>% 
    mutate(weighted_avg_loss = 100 * weighted_avg_loss) %>% 
    ggplot(aes(x = decision)) + 
    geom_ribbon(aes(ymin = 0, 
                    ymax = weighted_avg_loss), 
                fill = "grey75") + 
    geom_vline(xintercept = min_loss[1], 
               linetype = 2) + 
    geom_hline(yintercept = 100 * min_loss[2], 
               linetype = 2) + 
    labs(x = "decision", 
         y = "Expected Loss (£)")
```

Which of the point estimates corresponds to this optimum value of $d$?

```{r}
bind_rows(point_estimates %>% 
              select(-x, -y), 
          tibble(value = min_loss[1], 
                 .point = "best_decision")) %>% 
    arrange(value)
```

The median corresponds (within sampling error) to the best decision. What if we change the loss function to a quadratic term: $(d - p)^2$?

```{r}
make_loss <- function(guess) {
    d2 %>% 
        mutate(loss = posterior * (guess - p)^2) %>% 
        summarise(expected_loss = sum(loss)) %>% 
        pull(expected_loss)
}

(loss <- d2 %>% 
    select(decision = p) %>% 
    mutate(weighted_avg_loss = map_dbl(decision, make_loss)))
loss %>% 
    top_n(-1, weighted_avg_loss)
min_loss <- loss %>% 
    top_n(-1, weighted_avg_loss) %>% 
    as.numeric()
loss %>% 
    mutate(weighted_avg_loss = 100 * weighted_avg_loss) %>% 
    ggplot(aes(x = decision)) + 
    geom_ribbon(aes(ymin = 0, 
                    ymax = weighted_avg_loss), 
                fill = "grey75") + 
    geom_vline(xintercept = min_loss[1], 
               linetype = 2) + 
    geom_hline(yintercept = 100 * min_loss[2], 
               linetype = 2) + 
    labs(x = "decision", 
         y = "Expected Loss (£)")
```

This is now the posterior mean from `samples2`, $\pm \epsilon$.

# Sampling

Several good reasons given for sampling:

1. Dummy data;
2. Software validation;
3. Research design;
4. Forecasting.

## Dummy Data

Can easily generate dummy data for the binomial with various parameters (i.e. $n$ and $p$ in the binomial likelihood).

```{r}
n_draws <- 1e5
d <- tibble(n = c(3, 6, 9)) %>% 
    expand(n, p = c(.3, .6, .9)) %>% 
    mutate(draws = map2(n, p, function(n, p) {
        set.seed(1401)
        rbinom(n_draws, n, p)
    })) %>% 
    mutate(n = str_c("n = ", n), 
           p = str_c("p = ", p)) %>% 
    unnest()
head(d)
```

Can then plot to see different distributions.

```{r}
d %>% 
    ggplot(aes(x = draws)) + 
    geom_histogram(binwidth = 1, 
                   center = 1, 
                   size = 0.1) + 
    scale_x_continuous("dummy water count",
                     breaks = seq(from = 0, to = 9, by = 2)) +
    ylab("frequency") + 
    coord_cartesian(xlim = 0:9) + 
    theme(panel.grid = element_blank()) + 
    facet_grid(n ~ p)
```

# Practice

## Easy

```{r}
d <- tibble(p = seq(from = 0, to = 1, length.out = 1000), 
            prior = rep(1, 1000)) %>% 
    mutate(likelihood = dbinom(6 , size = 9 , prob = p)) %>% 
    mutate(posterior = (likelihood * prior)/sum(likelihood * prior))

samples <- tibble(samples = sample(d$p,
                                   prob = d$posterior,
                                   size = 1e4,
                                   replace = TRUE)) %>% 
    mutate(i = row_number())
```

3E1. How much posterior probability lies below p = 0.2?

```{r}
mean(samples$samples < 0.2)
```

3E2. How much posterior probability lies above p = 0.8?

```{r}
mean(samples$samples > .8)
```

3E3. How much posterior probability lies between p = 0.2 and p = 0.8? 

```{r}
mean(samples$samples > 0.2 & samples$samples < 0.8)
```

3E4. 20% of the posterior probability lies below which value of p?

```{r}
quantile(samples$samples, 0.2)
```

3E5. 20% of the posterior probability lies above which value of p?

```{r}
quantile(samples$samples, 0.8)
```
3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?

```{r}
HPDI(samples$samples, prob = 0.66)
```

3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?

```{r}
PI(samples$samples, prob = 0.66)
```

## Medium

3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

```{r}
d <- tibble(p = seq(from = 0,
                    to = 1,
                    length.out = 1000),
            prior = rep(1, 1000)) %>%
    mutate(likelihood = dbinom(8, size = 15, prob = p)) %>%
    mutate(posterior = (likelihood * prior) / sum(likelihood * prior))

# Can plot the posterior, overlaid with the normal approximation to the binomial
d %>% 
    ggplot(aes(x = p, y = posterior)) + 
    geom_line() + 
    geom_vline(xintercept = d$p[which.max(d$posterior)], 
               colour = "grey50", 
               linetype = 2) + 
    geom_hline(yintercept = max(d$posterior), 
               colour = "grey50", 
               linetype = 2)
```

3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to cal-
culate the 90% HPDI for p.

```{r}
set.seed(100)
samples <- sample(d$p, prob = d$posterior, size = 1e4, replace = TRUE)
HPDI(samples, .9)
```

3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?

```{r}
PPD <- rbinom(n = length(samples), size = 15, prob = samples)
mean(PPD == 8)
```

3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.

```{r}
PPD <- rbinom(n = length(samples), size = 9, prob = samples)
mean(PPD == 6)
```

3M5. Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7.

Q1.

```{r}
d <- tibble(p = seq(from = 0,
                    to = 1,
                    length.out = 1000),
            prior = if_else(p < 0.5, 0, 1)) %>%
    mutate(likelihood = dbinom(8, size = 15, prob = p)) %>%
    mutate(posterior = (likelihood * prior) / sum(likelihood * prior))

# Can plot the posterior, overlaid with the normal approximation to the binomial
d %>% 
    ggplot(aes(x = p, y = posterior)) + 
    geom_line() + 
    geom_vline(xintercept = d$p[which.max(d$posterior)], 
               colour = "grey50", 
               linetype = 2) + 
    geom_hline(yintercept = max(d$posterior), 
               colour = "grey50", 
               linetype = 2)
```

Q2. 

```{r}
set.seed(100)
samples <- sample(d$p,
                  prob = d$posterior,
                  size = 1e4,
                  replace = TRUE)
HPDI(samples, .9)
```

Q3.

```{r}
PPD <- rbinom(n = length(samples), size = 15, prob = samples)
mean(PPD == 8)
```

Q4.

```{r}
PPD <- rbinom(n = length(samples), size = 9, prob = samples)
mean(PPD == 6)
```

## Hard

```{r}
data(homeworkch3)
```

3H1.

```{r}
d <- tibble(p = seq(0, 1, length.out = 1000),
            prior = 1) %>%
    mutate(likelihood = dbinom(x = sum(birth1, birth2),
                               size = length(birth1) + length(birth2),
                               prob = p)) %>%
    mutate(posterior = (prior * likelihood)/sum((prior * likelihood)))
d %>%
    ggplot(aes(x = p, y = posterior)) +
    geom_line() +
    geom_text(aes(x = d$p[which.max(d$posterior)],
                  y = max(d$posterior),
                  label = paste0("p = ",
                                 round(d$p[which.max(d$posterior)], 2))),
               nudge_x = 0.07,
               check_overlap = TRUE) + 
    ylab("posterior density") + 
    theme_light()
```

3H2.

```{r}
samples <- sample(x = d$p, 
                  size = 1e4, 
                  replace = TRUE, 
                  prob = d$posterior)
HPDI(samples, c(.5, .89, .97))
```


3H3.

```{r}
PPD <- tibble(samples = rbinom(n = 1e4, 
                               size = length(birth1) + length(birth2), 
                               prob = samples))
PPD %>% 
    ggplot(aes(samples)) + 
    geom_density() + 
    geom_vline(xintercept = sum(birth1, birth2), linetype = 2) + 
    theme_light()
```

3H4.

Two ways to interpret. Can draw from the samples we have already.

```{r}
PPD <- tibble(samples = rbinom(n = 1e4, 
                               size = length(birth1), 
                               prob = samples))
PPD %>% 
    ggplot(aes(samples)) + 
    geom_density() + 
    geom_vline(xintercept = sum(birth1), linetype = 2) + 
    theme_light()
```

Or set up a new likelihood, posterior and PPD for just first births.

```{r}
d <- tibble(p = seq(0, 1, length.out = 1000), 
               prior = 1) %>% 
    mutate(likelihood = dbinom(sum(birth1), length(birth1), prob = p)) %>% 
    mutate(posterior = (prior * likelihood)/sum(prior * likelihood))
samples <- sample(d$p, size = 1e4, replace = TRUE, prob = d$posterior)
PPD <- tibble(samples = rbinom(1e4, size = 100, prob = samples))
PPD %>% 
    ggplot(aes(samples)) + 
    geom_density() + 
    geom_vline(xintercept = sum(birth1), linetype = 2) + 
    theme_light()
```

3H5.

```{r}
tibble(samples = rbinom(n = 1e4,
                        size = sum(!birth1),
                        prob = samples)) %>%
    ggplot(aes(samples)) +
    geom_density(bw = 0.25) +
    geom_vline(xintercept = sum(birth2[!birth1]))
```

