---
title: "Chapter 5: Multivariate Linear Models"
output: 
  html_document: 
    highlight: kate
    theme: journal
---

# Notes

## Intro

Multiple regression is a common technique; some of the reasons it is popular:

1. 'Controlling' for confounds: when there are confounding factors multiple regression can take this into account by including all of the causal variables. However this is not always so simple, and needs to be done with care.
2. Multiple causation.
3. Interactions.

Can illustrate some of this with the `WaffleDivorce` dataset.

```{r}
# load data
library(dagitty)
library(ggdag)
library(bayesplot)
library(tidybayes)
library(fiftystater)
library(brms)
library(ggrepel)
library(tidyverse)
data("WaffleDivorce", package = "rethinking")
d <- WaffleDivorce %>% 
    as_tibble() %>% 
    set_names(nm = names(.) %>% 
                  str_replace_all("\\.", "_")) %>% 
    # build z-score versions of the main variables we'll use
    mutate(Divorce_z           = scale(Divorce), 
           MedianAgeMarriage_z = scale(MedianAgeMarriage), 
           Marriage_z          = scale(Marriage))
# check out the data
str(d)
glimpse(d)
summary(d)
```

## Spurious Association

Plot divorce rate against the prevalence of Waffle Houses.

```{r}
d %>% 
    ggplot(aes(WaffleHouses/Population, 
               Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                size = 0.5, 
                alpha = 0.2, 
                colour = "steelblue4", 
                fill = "steelblue") + 
    geom_point(size = 1.5, alpha = 0.5, colour = "steelblue4") + 
    geom_text_repel(data = d %>% 
                        filter(Loc %in% c("ME", "OK", "AR", "AL", 
                                          "GA", "SC", "NJ")), 
                    aes(label = Loc), 
                    size = 3, 
                    seed = 1042) + 
    scale_x_continuous(limits = c(0, 55)) + 
    coord_cartesian(xlim = c(0, 50), 
                    ylim = c(5, 15)) + 
    labs(x = "Waffle Houses per million", 
         y = "Divorce Rate") + 
    theme_classic()
```

There is a positive correlation between them, but this is not evidence of causality. The two main predictors we might want to consider are the median age of marriage (`MedianAgeMarriage`) and the marriage rate (`Marriage`). Can visualise all of these in multiple ways, firstly as a heat map on the US.

```{r}
d %>% 
    # do this to match format in fifty_states
    mutate(Location = str_to_lower(Location)) %>% 
    select(ends_with("_z"), # use z-score version to keep scales consistent
           Location) %>% 
    {
        # will otherwise generate warnings about attributes being dropped
        suppressWarnings(gather(., key, value, -Location))
    } %>% 
    ggplot(aes(map_id = Location)) + 
    geom_map(aes(fill = value), 
             map = fifty_states, 
             colour = "grey50",
             size = 0.02) + 
    expand_limits(x = fifty_states$long, y = fifty_states$lat) + 
    scale_x_continuous(NULL, breaks = NULL) + 
    scale_y_continuous(NULL, breaks = NULL) + 
    scale_fill_viridis_c(option = "B") + 
    coord_map() + 
    theme_bw() +
    theme(panel.grid       = element_blank(),
          strip.background = element_rect(fill = "transparent", 
                                          color = "transparent")) +
    facet_wrap(~key)
```

Then can create pairs plots for the variables.

```{r}
d %>% 
    select(ends_with("_z")) %>% 
    pairs(col = "steelblue")
```

Can see some evidence of association, but which are meaningful?

## Simple Linear Regression

Start by building a simple linear model with `MedianAgeMarriage_z` ($A$) predicting `Divorce_z` ($D$). Model specification is:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_A}{A_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_1 <- brm(
    Divorce_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_1"
)
summary(b5_1)
```

These priors seem quite strict, but this is because the variables have been normalised and therefore seemingly small changes can be very large on the natural scale. See this by generating plots of the prior predictive distribution, which we can get by rebuilding the model but setting `sample_prior = "only"`.

```{r}
ppd_5_1 <- brm(
    Divorce_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    sample_prior = "only", 
    file = "Stan/ch_05/ppd_5_1", 
    control = list(adapt_delta = 0.9)
)
```

Now we can plot the marginal effects to see the range of plausible associations between predictor and outcome.

```{r}
g <- marginal_effects(ppd_5_1, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 

g$MedianAgeMarriage_z +
    labs(x = "Median Age Marriage (z-score)", 
         y = "Divorce Rate (z-score)") + 
    ggtitle("Prior predictive distribution") + 
    theme_minimal()
```

There is still quite a range of plausible associations, so the priors are not as restrictive as they may seem.

The summary for the model shows that the relationship between predictor and outcome is quite certainly negative. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_age_seq <- tibble(MedianAgeMarriage_z = seq(-3, 3, length.out = 100))

fitted_5_1 <- marriage_age_seq %>% 
    bind_cols(fitted(b5_1, 
                     newdata = marriage_age_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_1 %>% 
    ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(MedianAgeMarriage_z, 
                   Divorce_z), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Divorce (z-score)", 
         x = "Median Age Marriage (z-score)") + 
    coord_cartesian(xlim = range(d$MedianAgeMarriage_z), 
                    ylim = range(d$Divorce_z)) + 
    theme_classic()
```

Now follow the same set of steps, but for `Marriage_z` ($M$) as the predictor.

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_2 <- brm(
    Divorce_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_2"
)
summary(b5_2)
```

Generate the prior predictive distribution plots again to check the priors.

```{r}
ppd_5_2 <- brm(
    Divorce_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    sample_prior = "only", 
    file = "Stan/ch_05/ppd_5_2", 
    control = list(adapt_delta = 0.9)
)

g <- marginal_effects(ppd_5_2, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 

g$Marriage_z +
    labs(x = "Marriage Rate (z-score)", 
         y = "Divorce Rate (z-score)") + 
    ggtitle("Prior predictive distribution") + 
    theme_minimal()
```

The summary for the model shows that the relationship between predictor and outcome is probably positive. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_rate_seq <- tibble(Marriage_z = seq(-3, 3, length.out = 100))

fitted_5_2 <- marriage_rate_seq %>% 
    bind_cols(fitted(b5_2, 
                     newdata = marriage_rate_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_2 %>% 
    ggplot(aes(Marriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(Marriage_z, 
                   Divorce_z), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Divorce (z-score)", 
         x = "Marriage Rate (z-score)") + 
    coord_cartesian(xlim = range(d$Marriage_z), 
                    ylim = range(d$Divorce_z)) + 
    theme_classic()
```

Before building a multivariate model, need to consider issues of causality.

## Causality and DAGs

Can use directed acyclic graphs (DAGs) to illustrate causality in models and highlight different types of confounding.

```{r}
dag_5_1 <- dagify(
    D ~ A, 
    M ~ A, 
    D ~ M, 
    outcome = "D", 
    coords = list(x = c(A = 0, D = 1, M = 2), 
                  y = c(A = 1, D = 0, M = 1))) %>% 
    tidy_dagitty()
ggdag(dag_5_1) + 
    theme_dag()
```

This graph shows that there are two paths by which the marriage age may affect divorce rate: either directly or via the marriage rate. If this graph is correct then a model with both predictors will be better than either of the models with a single predictor.

## Multiple Regression

Now try combining the predictors, uncover how they vary in the same model. First specify the model:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i} + {\beta_A}{A_i} 
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Then build the model.

```{r}
b5_3 <- brm(
    Divorce_z ~ 1 + Marriage_z + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_3"
)
summary(b5_3)
```

Now the estimate for $\beta_A$ is still strongly negative, while that for $\beta_M$ is close to zero. What does this mean? The coefficients answer the question: once all the other predictors are know, what does knowing this predictor tell us about the outcome? So once the median age of marriage is known we get very little extra information from the marriage rate.

Can see this in a dotplot of the coefficients.

```{r}
stanplot(b5_3) + 
    theme(text = element_text(family = "Avenir"))
```

There are ways to create this plot with `bayesplot::mcmc_intervals()` or `tidybayes::stat_pointintervalh()` also; both work with the `posterior_samples()` from the `brm` output. The main merit of these is that they allow for more customisation or the plot as the dotplot is just another geom to be layered. 

For plotting the posterior of multivariate regressions need to use a few techniques.

## Predictor Reisdual Plots

Use one of the predictors to model the other.

```{r}
b5_4a <- brm(
    Marriage_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 10), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2500, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_4a"
)
summary(b5_4a)
```

Use the `fitted()` values to get the first plot from the book.

```{r}
fitted_5_4a <- fitted(b5_4a, 
                      probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

fitted_5_4a %>% 
    ggplot(aes(MedianAgeMarriage_z, Marriage_z)) + 
    geom_point(shape = 1, size = 2, colour = "steelblue4") + 
    geom_segment(aes(xend = MedianAgeMarriage_z, yend = Estimate), 
                 size = 0.25) + 
    geom_line(aes(y = Estimate), 
              colour = "steelblue") + 
    theme_classic()
```

Now get the residuals and plot them.

```{r}
resids_5_4a <- residuals(b5_4a, 
                         probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.5, 0.5), 
                    Divorce = 14.1, 
                    label = c("slower", "faster"))

resids_5_4a %>% 
    ggplot(aes(Estimate, Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                colour = "steelblue4", 
                fill = "steelblue4", 
                alpha = 0.2, 
                size = 0.5) + 
    geom_vline(xintercept = 0, 
               linetype = 2, 
               colour = "grey50") + 
    geom_point(size = 2, 
               colour = "steelblue4", 
               alpha = 0.6) + 
    geom_text(data = plot_text, 
              aes(label = label)) + 
    scale_x_continuous(limits = c(-2, 2)) + 
    coord_cartesian(xlim = range(resids_5_4a$Estimate), 
                    ylim = c(6, 14.1)) + 
    labs(x = "Marriage Rate Residuals") + 
    theme_classic()
```

Now do the same in reverse.

```{r}
b5_4b <- brm(
    MedianAgeMarriage_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 10), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2500, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_4b"
)
summary(b5_4b)
```

Now get the residuals and plot them.

```{r}
resids_5_4b <- residuals(b5_4b, 
                         probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.7, 0.5), 
                    Divorce = 14.1, 
                    label = c("younger", "older"))

resids_5_4b %>% 
    ggplot(aes(Estimate, Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                colour = "steelblue4", 
                fill = "steelblue4", 
                alpha = 0.2, 
                size = 0.5) + 
    geom_vline(xintercept = 0, 
               linetype = 2, 
               colour = "grey50") + 
    geom_point(size = 2, 
               colour = "steelblue4", 
               alpha = 0.6) + 
    geom_text(data = plot_text, 
              aes(label = label)) + 
    scale_x_continuous(limits = c(-2, 3)) + 
    coord_cartesian(xlim = range(resids_5_4b$Estimate), 
                    ylim = c(6, 14.1)) + 
    labs(x = "Age of Marriage Residuals") + 
    theme_classic()
```

## Counterfactual Plots

Answer the question: how would $y$ change if only one variable varied and the rest were set to the mean?

Start with doing this for marriage rate.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
    Marriage_z = seq(from = -3, to = 3, length.out = 100), 
    MedianAgeMarriage_z = mean(d$MedianAgeMarriage_z)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
                newdata = new_data, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data) %>% 
    ggplot(aes(Marriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$Marriage_z)) + 
    labs(subtitle = "Counterfactual plot for \nMedianAgeMarriage_z = 0", 
         y = "Divorce_z") + 
    theme_classic() -> g1

g1
```

Now do the same for Median Age of Marriage.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
    Marriage_z = mean(d$Marriage_z), 
    MedianAgeMarriage_z = seq(from = -3, to = 3.5, length.out = 100)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
                newdata = new_data, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data) %>% 
    ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$MedianAgeMarriage_z)) + 
    labs(subtitle = "Counterfactual plot for \nMarriage_z = 0", 
         y = "Divorce_z") + 
    theme_classic() -> g2

g2
```

Now plot both side by side.

```{r}
source("R/multiplot.R")
multiplot(g1, g2, cols = 2)
```

## Posterior Prediction Plots

Model checking to answer:

1. Did model fit correctly?
2. How did the model fail?

```{r}
fitted(b5_3, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d) %>% 
    ggplot(aes(Divorce_z, Estimate)) + 
    geom_abline(linetype = 2, colour = "grey50", size = 0.5) + 
    geom_point(size = 1.5, colour = "steelblue4", alpha = 0.7) + 
    geom_linerange(aes(ymin = Q5.5, ymax = Q94.5), 
                   size = 0.25, colour = "steelblue4") + 
    geom_linerange(aes(ymin = Estimate - Est.Error, 
                       ymax = Estimate + Est.Error), 
                   size = 0.5, colour = "steelblue4") + 
    geom_text(data = . %>% filter(Loc %in% c("ID", "UT")), 
              aes(label = Loc), 
              hjust = 0, 
              nudge_x = -0.2) + 
    labs(x = "Observed Divorce", y = "Predicted Divorce") + 
    theme_classic()
```

```{r}
residuals(b5_3, 
          probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5,
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5,
                      pred_hi = Q94.5)) %>% 
    bind_cols(d) %>%
    # convert the pred_ intervals to a deviance metric
    mutate(pred_low = Divorce_z - pred_low,
           pred_hi = Divorce_z - pred_hi) %>% 
    ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) +
    geom_hline(yintercept = 0, 
               size = 0.5, 
               colour = "steelblue4", 
               alpha = 0.1) +
    geom_pointrange(aes(ymin = fit_low, ymax = fit_hi),
                    size = 0.4, 
                    shape = 20, 
                    colour = "steelblue4") + 
    geom_segment(aes(y    = Estimate - Est.Error, 
                     yend = Estimate + Est.Error,
                     x    = Loc, 
                     xend = Loc),
                 size = 1, 
                 colour = "steelblue4") +
    geom_segment(aes(y    = pred_low, 
                     yend = pred_hi,
                     x    = Loc, 
                     xend = Loc),
                 size = 3, 
                 colour = "steelblue4", 
                 alpha = 0.1) +
    labs(x = NULL, y = NULL) +
    coord_flip() +
    theme_classic() + 
    theme(panel.grid   = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y  = element_text(hjust = 0))
```

## Simulating Spurious Association

Can illustrate the problem with a simulation where we know the true nature of relationships between variables.

```{r}
N <- 100
d2 <- tibble(x_real = rnorm(N)) %>% 
    mutate(x_spur = rnorm(N, mean = x_real), 
           y      = rnorm(N, mean = x_real))
head(d2)
```

Have a look at the pairs plots.

```{r}
pairs(d2, col = "steelblue4")
```

The DAG for these variables:

```{r}
dagify(x_spur ~ x_real, 
       y      ~ x_real, 
       outcome = "y") %>% 
    tidy_dagitty() %>% 
    ggdag()
```

Now model the relationships as:

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{real}x_{real} + \beta_{spur}x_{spur}
\\
\alpha \sim \mathcal{N}(0, 1)
\\
\beta_{real} \sim \mathcal{N}(0, 1)
\\
\beta_{spur} \sim \mathcal{N}(0, 1)
\\
\sigma \sim \text{HalfCauchy}(0, 1)
$$

```{r}
b5_4c <- brm(
    y ~ 1 + x_real + x_spur, 
    data = d2, 
    prior = c(prior(normal(0, 1), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_4c"
)
fixef(b5_4c, probs = c(0.055, 0.945)) %>% 
    round(digits = 2)
```

As it should the model gets the coefficients 'right': for $x_{real}$ c. 1; for $x_{spur}$ about 0.

## Masked Relationships

Can use a different dataset to explore a different sort of confounding structure.

```{r}
data("milk", package = "rethinking")
d <- milk %>% 
    as_tibble() %>% 
    set_names(nm = names(.) %>% 
                  str_replace_all("\\.", "_"))
head(d)
```

Can look at pairs plots for the three variables of interest.

```{r}
d %>% 
    select(kcal_per_g, mass, neocortex_perc) %>% 
    pairs(col = "steelblue4")
```

Cannot tell much from this, so start modelling! Start with a simple model using only `necortex_perc` to predict `kcal_per_g`:

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{N}N_i
$$

Before getting into priors need to drop the NA values. (The `quap` function used in the book requires this while `brms::brm()` will just do casewise deletion, but doing this deliberately is good practice anyway.) Use `scale()` to normalise the variables at the same time.

```{r}
d2 <- d %>% 
    select(kcal_per_g, mass, neocortex_perc) %>% 
    filter(complete.cases(.)) %>% 
    mutate(K = scale(kcal_per_g), 
           M = scale(log(mass)), 
           N = scale(neocortex_perc))
```

Can set this up as a prior-only `brm()` model and then plot the two sets of priors given to see if they are plausible.

```{r}
ppd_5_5_a <- brm(
    K ~ 1 + N, 
    data = d2, 
    prior = c(prior(normal(0, 1), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(exponential(1), class = sigma)), 
    sample_prior = "only", 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/ppd_5_5_a"
)

ppd_5_5_b <- brm(
    K ~ 1 + N, 
    data = d2, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    sample_prior = "only", 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/ppd_5_5_b"
)
```

Now plot the PPD side by side.

```{r}
g1 <- marginal_effects(ppd_5_5_a, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 

g2 <- marginal_effects(ppd_5_5_b, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 
g1$N +
    labs(x = "Neocortex Percentage (z-score)", 
         y = "Kcal per g (z-score)") + 
    ggtitle("PPD: Looser Priors") + 
    coord_cartesian(ylim = c(-6, 6)) + 
    theme_minimal() -> plot_1
g2$N +
    labs(x = "Neocortex Percentage (z-score)", 
         y = "Kcal per g (z-score)") + 
    ggtitle("PPD: Tighter Priors") + 
    coord_cartesian(ylim = c(-6, 6)) + 
    theme_minimal() -> plot_2

multiplot(plot_1, plot_2, cols = 2)
```

As shown earlier even priors that seem to be extremely strict are quite permissive when dealing with variables that have been normalised.

Now build the model with the stricter priors:

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{N}N_i
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_{N} \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

```{r}
b5_5 <- brm(
    K ~ 1 + N, 
    data = d2, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_5"
)
print(b5_5, digits = 3)
```

Set up a plot for this regression.

```{r}
new_data <- tibble(N = seq(-4, 4, length.out = 100))
fitted(b5_5,  
       newdata = new_data,
       probs = c(0.055, 0.945)) %>%
    as_tibble() %>%
    bind_cols(new_data) %>% 
    
    ggplot(aes(x = N, y = Estimate)) +
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
                fill = "steelblue", 
                alpha = 0.2) +
    geom_line(color = "steelblue4", 
              size = 0.5) +
    geom_point(data = d2, 
               aes(x = N, y = K),
               size = 2, 
               color = "steelblue4") +
    coord_cartesian(xlim = range(d2$N), 
                    ylim = range(d2$K)) + 
    labs(y = "K") + 
    ggtitle("K regressed on N") + 
    theme_classic() -> g3
```

Can build a similar model using log of mass `M` as the predictor.

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{M}M_i
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_{M} \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

```{r}
b5_6 <- brm(
    K ~ 1 + M, 
    data = d2, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_6"
)
print(b5_6, digits = 3)
```

And view both plots side by side.

```{r}
new_data <- tibble(M = seq(-4, 4, length.out = 100))
fitted(b5_6,  
       newdata = new_data,
       probs = c(0.055, 0.945)) %>%
    as_tibble() %>%
    bind_cols(new_data) %>% 
    
    ggplot(aes(x = M, y = Estimate)) +
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
                fill = "steelblue", 
                alpha = 0.2) +
    geom_line(color = "steelblue4", 
              size = 0.5) +
    geom_point(data = d2, 
               aes(x = M, y = K),
               size = 2, 
               color = "steelblue4") +
    coord_cartesian(xlim = range(d2$M), 
                    ylim = range(d2$K)) + 
    labs(y = "K") +
    ggtitle("K regressed on M") + 
    theme_classic() -> g4
multiplot(g3, g4, cols = 2)
```

The associations look fairly weak. But now we can build the multiple regression model.

```{r}
b5_7 <- brm(
    K ~ 1 + M + N, 
    data = d2, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_7"
)
print(b5_7, digits = 3)
```

The coefficients imply stronger associations than in the bivariate regressions. Can see this more clearly in a plot.


```{r}
stanplot(b5_7) + 
    theme(text = element_text(family = "Avenir"))
```

The counterfactual plots show this well.

```{r}
new_data_g5 <- tibble(N = seq(-4, 4, length.out = 100), 
                      M = 0)
new_data_g6 <- tibble(M = seq(-4, 4, length.out = 100), 
                      N = 0)
g5 <- fitted(b5_7,  
             newdata = new_data_g5,
             probs = c(0.055, 0.945)) %>%
    as_tibble() %>%
    bind_cols(new_data_g5) %>% 
    
    ggplot(aes(x = N, y = Estimate)) +
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
                fill = "steelblue", 
                alpha = 0.2) +
    geom_line(color = "steelblue4", 
              size = 0.5) +
    geom_point(data = d2, 
               aes(x = N, y = K),
               size = 2, 
               color = "steelblue4") +
    coord_cartesian(xlim = range(d2$N), 
                    ylim = range(d2$K)) + 
    labs(y = "K") +
    ggtitle("Counterfactual holding M = 0") + 
    theme_classic()

g6 <- fitted(b5_7,  
             newdata = new_data_g6,
             probs = c(0.055, 0.945)) %>%
    as_tibble() %>%
    bind_cols(new_data_g6) %>% 
    
    ggplot(aes(x = M, y = Estimate)) +
    geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
                fill = "steelblue", 
                alpha = 0.2) +
    geom_line(color = "steelblue4", 
              size = 0.5) +
    geom_point(data = d2, 
               aes(x = M, y = K),
               size = 2, 
               color = "steelblue4") +
    coord_cartesian(xlim = range(d2$M), 
                    ylim = range(d2$K)) + 
    labs(y = "K") +
    ggtitle("Counterfactual holding N = 0") + 
    theme_classic()
multiplot(g5, g6, cols = 2)
```

The associations are now much clearer: why? Because while both the log-mass and neocortex percentage are predictive of the energy in the primate's milk, but in opposite directions: can look at the pairs plots of the normalised variables to see this:

```{r}
d2 %>% 
    select(K, N, M) %>% 
    pairs(col = "steelblue")
```

Neocortex percentage is positively correlated with milk energy, while log-mass is negatively correlated with it. Log-mass and neocortex percentage are also positively correlated with each other. All of this masks the true relationship unless we consider them all together. 

Can illustrate this with DAGs, at least three of which are consistent with these data and results.

```{r}
# create reusable coordinates
dag_coords <- list(x = c(M = 0, K = 1, N = 2, U = 1), 
                   y = c(M = 1, K = 0, N = 1, U = 1))
dag_5_7a <- dagify(N ~ M, 
                   K ~ M, 
                   K ~ N, 
                   outcome = "K", 
                   coords = dag_coords) %>% 
    tidy_dagitty() %>% 
    ggdag()
dagify(N ~ M, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
    tidy_dagitty() %>% 
    ggdag()
dagify(M ~ N, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
    tidy_dagitty() %>% 
    ggdag()
dagify(N ~ U,
       M ~ U, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

In the first log-mass causes neocortex percentage; in the second that is reversed; in the third some unobserved variable causes both of them.

## When Adding Variables Hurts

Adding a predictor can make a model worse though. An example with legs, based on the reasonable assumption that leg-length is a good predictor of height.

```{r}
# simulate the data
N <- 100
set.seed(531)

d <- tibble(height = rnorm(N, mean = 10, sd = 2), 
            leg_prop = runif(N, min = 0.4, max = 0.5)) %>% 
    mutate(leg_left = leg_prop * height + rnorm(N, sd = 0.02), 
           leg_right = leg_prop * height + rnorm(N, sd = 0.02))
```

The two leg lengths are strongly correlated:

```{r}
d %>% 
    select(leg_left, leg_right) %>% 
    cor() %>% 
    round(digits = 5)

d %>% 
    ggplot(aes(leg_left, leg_right)) + 
    stat_smooth(method = "lm", 
                colour = "grey50", 
                size = 0.5) + 
    geom_point(colour = "steelblue") + 
    theme_classic()
```

So what happens if we use both for predicting height?

```{r}
b5_8 <- brm(
    height ~ 1 + leg_left + leg_right,
    data = d, 
    prior = c(prior(normal(10, 100), class = Intercept),
              prior(normal(2, 10), class = b),
              prior(cauchy(0, 5), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_8")
print(b5_8, digits = 3)
```

The error on the parameter estimates is huge, as we can see in a plot.

```{r}
stanplot(b5_8) + 
    theme(text = element_text(family = "Avenir"))
```

This problem of collinearity is why it's important to be careful with variable selection when building models: adding both causes confusion in this case.

## Categorical Variables

Can use the !Kung data again for working with a categorical variable, which in this case is sex.

```{r}
data("Howell1", package = "rethinking")
d <- Howell1 %>% 
    as_tibble()
rm(Howell1)
head(d)
```

The first approach will be to use the `male` variable as-is: a _dummy variable_. NB. we need one fewer variable than there are levels of the factor.

Build a model of height using only `male` as the predictor.

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_m}{m_i}
\\
\alpha \sim \mathcal{N}(178, 20)
\\
{\beta_m} \sim \mathcal{N}(0, 10)
\\
\sigma \sim \text{HalfCauchy}(0, 2)
$$

NB. The book uses a uniform prior on $\sigma$ but that causes problems for the simulation.

```{r}
b5_9 <- brm(
    height ~ 1 + male, 
    data = d, 
    prior = c(prior(normal(178, 20), class = Intercept), 
              prior(normal(0, 10), class = b), 
              prior(cauchy(0, 2), class = sigma)), 
    cores = 4, 
    chains = 4, 
    file = "Stan/ch_05/b5_9"
)
print(b5_9, digits = 3)
```

With this approach $\beta_m$ represents the expected difference between a man and a woman. However there is an imbalance: all heights are predicted using $\alpha$, but only males' heights use $\beta_m$. This creates additional uncertainty in males' heights, as they rely on two parameter estimates. Can see this by simulating from the priors.

```{r}
tibble(mu_female = rnorm(1e4, 178, 20), 
       mu_male   = rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)) %>% 
    gather() %>% 
    group_by(key) %>% 
    summarise(mean = mean(value), 
              std_dev = sd(value))
```

An alternative approach given in the book is to use index variables, so that sex is recoded, as below:

```{r}
d$sex <- factor(if_else(d$male == 1, "male", "female"))

head(d)
```

Then the model specification would be:

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha_{SEX[i]}
\\
\alpha_j \sim \mathcal{N}(178, 20), \text{for} j = 1, 2
\\
\sigma \sim \text{HalfCauchy}(0, 2)
$$

Doing this with `brms` is easy if the predictor is a factor: just have to remember to remove the intercept in the formula.

```{r}
b5_10 <- brm(
    height ~ 0 + sex, 
    data = d, 
    prior = c(prior(normal(178, 20), class = b), 
              prior(cauchy(0, 2), class = sigma)), 
    cores = 4, 
    chains = 4, 
    file = "Stan/ch_05/b5_10"
)
print(b5_10, digits = 3)
```

For data with many categories the approach is similar, and the index variable approach scales better. An example with the `milk` dataset.

```{r}
# several values for clade
unique(milk$clade)
# mutate this to an integer id, also normalise the kilocalories
d <- milk %>% 
    mutate(clade_id = as.integer(clade)) %>% 
    mutate(K = as.numeric(scale(kcal.per.g)))
```

Now build a model for average milk energy in each clade.

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha_{CLADE[i]}
\\
\alpha_{j} \sim \mathcal{N}(0, 0.5) \text{ for } j \in \{1, 2, 3, 4\}
\\
\sigma \sim \text{Exponential}(1)
$$

```{r}
b5_11 <- brm(
    K ~ 0 + clade, 
    data = d, 
    prior = c(prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    cores = 4, 
    chains = 4, 
    file = "Stan/ch_05/b5_11"
)
print(b5_11, digits = 3)
```

Can plot the coefficient estimates.

```{r}
stanplot(b5_11, pars = "^b") + 
    theme(text = element_text(family = "Avenir"))
```

Can easily add another categorical predictor: the example given in the book is to put each species in a house from Hogwarts.

```{r}
# set the seed to get the same results as the book
set.seed(63)
d2 <- d %>% 
    mutate(house = factor(sample(rep(c("Gryffindor", "Hufflepuff", 
                                       "Ravenclaw", "Slytherin"), each = 8), 
                                 size = nrow(d))))
# now build the model
b5_12 <- brm(
    K ~ 0 + (1 | house) + (1 |clade), 
    data = d2, 
    prior = c(prior(normal(0, 0.5), class = sd), 
              prior(exponential(1), class = sigma)), 
    cores = 4, 
    chains = 4, 
    file = "Stan/ch_05/b5_12"
)
print(b5_12, digits = 3)
```

Can build the plot manually.

```{r}
posterior_summary(b5_12, probs = c(0.055, 0.25, 0.5, 0.75, 0.945)) %>% 
    as_tibble(rownames = "parameter") %>% 
    filter(str_detect(parameter, "^r\\_")) %>% 
    mutate(parameter = factor(str_replace(parameter, 
                                          "r\\_(.+)\\[(.+),.+\\]", 
                                          "\\1_\\2"))) %>% 
    ggplot(aes(fct_rev(parameter), Q50)) + 
    geom_hline(yintercept = 0, 
               colour = "grey90", 
               size = 0.8) + 
    geom_linerange(aes(ymin = Q5.5, ymax = Q94.5), 
                   size = 0.5, 
                   colour = "steelblue") + 
    geom_linerange(aes(ymin = Q25, ymax = Q75), 
                   size = 2, 
                   colour = "navy") + 
    geom_point(shape = 21, 
               size = 5, 
               colour = "navy", 
               fill = "skyblue") + 
    theme_minimal() + 
    theme(text = element_text(family = "Avenir"), 
          panel.grid = element_blank()) + 
    labs(x = NULL, y = NULL) + 
    coord_flip()
```

# Practice

## Easy

### 5E1

_2_ and _3_ are multiple linear regressions. _1_ has a single predictor, and _3_ creates a single predictor variable that is the difference of $x_i$ and $z_i$.

### 5E2

The model specification would be:

$$
A \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{L}{L} + \beta_{P}{P} 
\\
\alpha \sim \mathcal{N}(0, 1)
\\
\beta_{L} \sim \mathcal{N}(0, 1)
\\
\beta_{P} \sim \mathcal{N}(0, 1)
\\
\sigma \sim \text{Exponential}(1)
$$

Priors might need some work depending on the dataset.

### 5E3

One possible model is:

$$
\log(T) \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{F}{F} + \beta_{S}{S} 
\\
\alpha \sim \mathcal{N}(0, 1)
\\
\beta_{F} \sim \mathcal{N}(0, 1)
\\
\beta_{S} \sim \mathcal{N}(0, 1)
\\
\sigma \sim \text{Exponential}(1)
$$

The log transformation constrains $T$ to be positive. If the model's assumptions about the effects are correct then both $\beta_{F}$ and $\beta_{S}$ should be positive.

### 5E4

The odd one out is $\mu_i = \alpha + \beta_{A}A_i + \beta_{B}B_i + \beta_{C}C_i + \beta_{D}D_i$ as it specifies five parameters, whereas the others all have four.

## Medium

### 5M1

```{r}
set.seed(2144)
spur_corr <- tibble(x_cause = rnorm(100)) %>% 
    mutate(x_spur = rnorm(100, mean = x_cause), 
           y = rnorm(100, mean = x_cause))
cor(spur_corr)
```

```{r}
b5_13 <- brm(
    y ~ 1 + x_cause + x_spur, 
    data = spur_corr, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 1), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_13"
)
stanplot(b5_13) + 
    theme(text = element_text(family = "Avenir"))
```

### 5M2

Use the same method as in the book to create a masked relationship. In this case the outcome is correlated with both predictors in opposite directions, and the predictors are positively correlated with each other.

```{r}
set.seed(2112)
N <- 100
rho <- 0.8
masking <- tibble(
    x_pos = rnorm(N), 
    x_neg = rnorm(N, rho * x_pos, sqrt(1 - rho^2)), 
    y     = rnorm(N, (x_pos - x_neg)))
pairs(masking, col = "steelblue")
```


```{r}
b5_14 <- brm(
    y ~ 1 + x_pos + x_neg, 
    data = masking, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 1), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_14"
)
stanplot(b5_14) + 
    theme(text = element_text(family = "Avenir"))
```

### 5M3

We could hypothesise that a higher divorce rate makes people available to remarry, and that because these people have already been married at least once they are more likely to marry again. A model might use divorce rate as a predictor for marriage rate, and add median age of marriage on the basis that a young age of marriage gives more time for remarrying.

```{r}
data("WaffleDivorce", package = "rethinking")
waffle <- WaffleDivorce %>% 
    as_tibble() %>% 
    mutate(marriage_norm = scale(Marriage) %>% 
               as.double(), 
           divorce_norm = scale(Divorce) %>% 
               as.double(), 
           median_norm = scale(MedianAgeMarriage) %>% 
               as.numeric())
head(waffle)
```

```{r}
b5_15 <- brm(
    marriage_norm ~ 1 + divorce_norm + median_norm, 
    data = waffle, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 1), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_15"
)
stanplot(b5_15) + 
    theme(text = element_text(family = "Avenir"))
```

The model doesn't give much support for the hypothesis: the median age of marriage has a strongly negative parameter estimate, whereas that for divorce rate is quite small.

### 5M4

Use the data previously grabbed from [Wikipedia][1] on LDS membership.

```{r}
waffle_LDS <- waffle %>% 
    inner_join(read_rds("./data/LDS.rds"), 
               by = c("Location" = "state")) %>% 
    mutate(LDS_norm = scale(LDS) %>% 
               as.numeric())
```

```{r}
b5_16 <- brm(
    divorce_norm ~ 1 + marriage_norm + median_norm + LDS_norm, 
    data = waffle_LDS, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 1), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_16"
)
stanplot(b5_16) + 
    theme(text = element_text(family = "Avenir"))
```

The parameter estimates give us confidence that there are negative effects for both the median age of marriage and the proportion of LDS on the divorce rate.

### 5M5

We need to consider the idea that an increase in gasoline prices would be linked to a drop in obesity rates. Suppose that $Y$ is the proportion of the population that is obese on the last day of a given month, and $X_1$ is the inflation-adjusted, normalised average price of a litre of petrol in the preceding month. That means for $a > b$:

$$
E(Y | X_1 = a) < E(Y | X_1 = b)
$$

The first mechanism suggested connects $X_1$ and $Y$ via the amount of driving, supposing that higher petrol prices might lead to less driving and more exercise. We could use a predictor, such as average miles driven per person in the preceding month, $X_2$. In that case our multiple regression model might be:

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i}
\\
\alpha \sim \mathcal{N}(0, 0.5)
\\
\beta_j \sim \mathcal{N}(0, 0.5) \ \text{for} \ j \in \{1, 2\}
\\
\sigma \sim \text{Exponential}(1)
$$

The second causal mechanism proposed is that higher petrol prices lead to people driving less and therefore eating fewer large meals in restaurants. If we knew the average number of restaurant meals eaten per person in the preceding month, $X_3$, we could use that as a predictor. We could add $X_3$ as an additional predictor, or use it instead of $X_2$.

## Hard

Set up the foxes dataset.

```{r}
data("foxes", package = "rethinking")
d <- foxes %>% 
    as_tibble() %>% 
    # normalise the outcome variable
    mutate(weight_norm = scale(weight) %>% as.double())
head(d)
```

### 5H1

Build the two regressions with `brms`.

```{r}
# first for just area as predictor
# set sample_prior = "yes" for both to check priors afterwards
b5_17a <- brm(
    weight_norm ~ 1 + area, 
    data = d, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 0.5), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    sample_prior = "yes", 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_17a"
)
# now using groupsize
b5_17b <- brm(
    weight_norm ~ 1 + groupsize, 
    data = d, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 0.5), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    sample_prior = "yes", 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_17b"
)
summary(b5_17a)
summary(b5_17b)
```

The results don't suggest much influence for either predictor, but this could be because of the sort of masked relationship we saw earlier. Can plot the posterior predictions.

```{r}
area_seq <- tibble(area = seq(0, 6, length.out = 100))

fitted_5_17a <- area_seq %>% 
    bind_cols(fitted(b5_17a, 
                     newdata = area_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_17a %>% 
    ggplot(aes(area, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(area, 
                   weight_norm), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Weight (z-score)", 
         x = "Area") + 
    coord_cartesian(xlim = range(d$area), 
                    ylim = range(d$weight_norm)) + 
    theme_classic()
```

Little evidence for a relationship, and the bowtie is fairly tight around the MAP line having zero-slope.

```{r}
groupsize_seq <- tibble(groupsize = seq(1, 12, length.out = 100))

fitted_5_17b <- groupsize_seq %>% 
    bind_cols(fitted(b5_17b, 
                     newdata = groupsize_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_17b %>% 
    ggplot(aes(groupsize, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(groupsize, 
                   weight_norm), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Weight (z-score)", 
         x = "Group Size") + 
    coord_cartesian(xlim = range(d$groupsize), 
                    ylim = range(d$weight_norm)) + 
    theme_classic()
```

Weak negative trend for groupsize, but not at all conclusive. 

### 5H2

```{r}
b5_18 <- brm(
    weight_norm ~ 1 + area + groupsize, 
    data = d, 
    prior = c(prior(normal(0, 1), class = "Intercept"), 
              prior(normal(0, 0.5), class = "b"), 
              prior(exponential(1), class = "sigma")), 
    sample_prior = "yes", 
    chains = 4, 
    iter = 2000, 
    warmup = 1000, 
    cores = 4, 
    file = "./Stan/ch_05/b5_18"
)
summary(b5_18)
```

Now plot the counterfactual plots, holding one predictor at its mean at a time.

```{r}
# build the data holding groupsize at its mean
new_data_group <- tibble(
    area = seq(0, 6, length.out = 100), 
    groupsize = mean(d$groupsize)
)

fitted(b5_18, 
       newdata = new_data_group, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_18, 
                newdata = new_data_group, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data_group) %>% 
    ggplot(aes(area, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$area)) + 
    labs(subtitle = sprintf("Counterfactual plot for \ngroupsize = %.2f",
                            mean(d$groupsize)), 
         y = "Weight") + 
    theme_classic() -> g7

# now hold area at its mean
new_data_area <- tibble(
    area = mean(d$area), 
    groupsize = seq(1, 12, length.out = 100)
)

fitted(b5_18, 
       newdata = new_data_area, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_18, 
                newdata = new_data_area, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data_area) %>% 
    ggplot(aes(groupsize, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$groupsize)) + 
    labs(subtitle = sprintf("Counterfactual plot for \narea = %.2f",
                            mean(d$area)), 
         y = "Weight") + 
    theme_classic() -> g8
multiplot(g7, g8, cols = 2)
```

The evidence is now fairly strong that area has a positive association with weight, and that group size has a negative one. These are probably masked. Can test this by looking at the correlation between groupsize and area.

```{r}
d %>% 
    select(groupsize, area) %>% 
    cor()
```

Because they are strongly correlated with each other but relate to the outcome in opposite directions they mask each other, much as in the example with primates in this chapter.


[1]: https://en.wikipedia.org/wiki/The_Church_of_Jesus_Christ_of_Latter-day_Saints_membership_statistics_(United_States)