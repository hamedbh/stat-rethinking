---
title: "Chapter 5: Multivariate Linear Models"
output: 
  html_document: 
    highlight: kate
    theme: journal
---

# Notes

## Intro

Multiple regression is a common technique; some of the reasons it is popular:

1. 'Controlling' for confounds: when there are confounding factors multiple regression can take this into account by including all of the causal variables. However this is not always so simple, and needs to be done with care.
2. Multiple causation.
3. Interactions.

Can illustrate some of this with the `WaffleDivorce` dataset.

```{r}
# load data
library(dagitty)
library(ggdag)
library(bayesplot)
library(tidybayes)
library(fiftystater)
library(brms)
library(ggrepel)
library(tidyverse)
data("WaffleDivorce", package = "rethinking")
d <- WaffleDivorce %>% 
  as_tibble() %>% 
  set_names(nm = names(.) %>% 
              str_replace_all("\\.", "_")) %>% 
  # build z-score versions of the main variables we'll use
  mutate(Divorce_z           = scale(Divorce), 
         MedianAgeMarriage_z = scale(MedianAgeMarriage), 
         Marriage_z          = scale(Marriage))
# check out the data
str(d)
glimpse(d)
summary(d)
```

## Spurious Association

Plot divorce rate against the prevalence of Waffle Houses.

```{r}
d %>% 
  ggplot(aes(WaffleHouses/Population, 
             Divorce)) + 
  stat_smooth(method = "lm", 
              fullrange = TRUE, 
              size = 0.5, 
              alpha = 0.2, 
              colour = "steelblue4", 
              fill = "steelblue") + 
  geom_point(size = 1.5, alpha = 0.5, colour = "steelblue4") + 
  geom_text_repel(data = d %>% 
                    filter(Loc %in% c("ME", "OK", "AR", "AL", 
                                      "GA", "SC", "NJ")), 
                  aes(label = Loc), 
                  size = 3, 
                  seed = 1042) + 
  scale_x_continuous(limits = c(0, 55)) + 
  coord_cartesian(xlim = c(0, 50), 
                  ylim = c(5, 15)) + 
  labs(x = "Waffle Houses per million", 
       y = "Divorce Rate") + 
  theme_classic()
```

There is a positive correlation between them, but this is not evidence of causality. The two main predictors we might want to consider are the median age of marriage (`MedianAgeMarriage`) and the marriage rate (`Marriage`). Can visualise all of these in multiple ways, firstly as a heat map on the US.

```{r}
d %>% 
  # do this to match format in fifty_states
  mutate(Location = str_to_lower(Location)) %>% 
  select(ends_with("_z"), # use z-score version to keep scales consistent
         Location) %>% 
  {
    # will otherwise generate warnings about attributes being dropped
    suppressWarnings(gather(., key, value, -Location))
  } %>% 
  ggplot(aes(map_id = Location)) + 
  geom_map(aes(fill = value), 
           map = fifty_states, 
           colour = "grey50",
           size = 0.02) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) + 
  scale_x_continuous(NULL, breaks = NULL) + 
  scale_y_continuous(NULL, breaks = NULL) + 
  scale_fill_viridis_c(option = "B") + 
  coord_map() + 
  theme_bw() +
  theme(panel.grid       = element_blank(),
        strip.background = element_rect(fill = "transparent", 
                                        color = "transparent")) +
  facet_wrap(~key)
```

Then can create pairs plots for the variables.

```{r}
d %>% 
  select(ends_with("_z")) %>% 
  pairs(col = "steelblue")
```

Can see some evidence of association, but which are meaningful?

## Simple Linear Regression

Start by building a simple linear model with `MedianAgeMarriage_z` ($A$) predicting `Divorce_z` ($D$). Model specification is:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_A}{A_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_1 <- brm(
  Divorce_z ~ 1 + MedianAgeMarriage_z, 
  data = d, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2000, 
  warmup = 1000, 
  file = "Stan/ch_05/b5_1"
)
summary(b5_1)
```

These priors seem quite strict, but this is because the variables have been normalised and therefore seemingly small changes can be very large on the natural scale. See this by generating plots of the prior predictive distribution, which we can get by rebuilding the model but setting `sample_prior = "only"`.

```{r}
ppd_5_1 <- brm(
  Divorce_z ~ 1 + MedianAgeMarriage_z, 
  data = d, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2000, 
  warmup = 1000, 
  sample_prior = "only", 
  file = "Stan/ch_05/ppd_5_1", 
  control = list(adapt_delta = 0.9)
)
```

Now we can plot the marginal effects to see the range of plausible associations between predictor and outcome.

```{r}
g <- marginal_effects(ppd_5_1, method = "predict") %>% 
  plot(points = TRUE, plot = FALSE) 

g$MedianAgeMarriage_z +
  labs(x = "Median Age Marriage (z-score)", 
       y = "Divorce Rate (z-score)") + 
  ggtitle("Prior predictive distribution") + 
  theme_minimal()
```

There is still quite a range of plausible associations, so the priors are not as restrictive as they may seem.

The summary for the model shows that the relationship between predictor and outcome is quite certainly negative. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_age_seq <- tibble(MedianAgeMarriage_z = seq(-3, 3, length.out = 100))

fitted_5_1 <- marriage_age_seq %>% 
  bind_cols(fitted(b5_1, 
                   newdata = marriage_age_seq, 
                   probs = c(0.055, 0.945)) %>% 
              as_tibble())

fitted_5_1 %>% 
  ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
  geom_ribbon(aes(ymin = Q5.5, 
                  ymax = Q94.5), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_line(colour = "steelblue4") + 
  geom_point(data = d, 
             aes(MedianAgeMarriage_z, 
                 Divorce_z), 
             size = 2, 
             colour = "steelblue4") + 
  labs(y = "Divorce (z-score)", 
       x = "Median Age Marriage (z-score)") + 
  coord_cartesian(xlim = range(d$MedianAgeMarriage_z), 
                  ylim = range(d$Divorce_z)) + 
  theme_classic()
```

Now follow the same set of steps, but for `Marriage_z` ($M$) as the predictor.

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_2 <- brm(
  Divorce_z ~ 1 + Marriage_z, 
  data = d, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2000, 
  warmup = 1000, 
  file = "Stan/ch_05/b5_2"
)
summary(b5_2)
```

Generate the prior predictive distribution plots again to check the priors.

```{r}
ppd_5_2 <- brm(
  Divorce_z ~ 1 + Marriage_z, 
  data = d, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2000, 
  warmup = 1000, 
  sample_prior = "only", 
  file = "Stan/ch_05/ppd_5_2", 
  control = list(adapt_delta = 0.9)
)

g <- marginal_effects(ppd_5_2, method = "predict") %>% 
  plot(points = TRUE, plot = FALSE) 

g$Marriage_z +
  labs(x = "Marriage Rate (z-score)", 
       y = "Divorce Rate (z-score)") + 
  ggtitle("Prior predictive distribution") + 
  theme_minimal()
```

The summary for the model shows that the relationship between predictor and outcome is probably positive. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_rate_seq <- tibble(Marriage_z = seq(-3, 3, length.out = 100))

fitted_5_2 <- marriage_rate_seq %>% 
  bind_cols(fitted(b5_2, 
                   newdata = marriage_rate_seq, 
                   probs = c(0.055, 0.945)) %>% 
              as_tibble())

fitted_5_2 %>% 
  ggplot(aes(Marriage_z, Estimate)) + 
  geom_ribbon(aes(ymin = Q5.5, 
                  ymax = Q94.5), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_line(colour = "steelblue4") + 
  geom_point(data = d, 
             aes(Marriage_z, 
                 Divorce_z), 
             size = 2, 
             colour = "steelblue4") + 
  labs(y = "Divorce (z-score)", 
       x = "Marriage Rate (z-score)") + 
  coord_cartesian(xlim = range(d$Marriage_z), 
                  ylim = range(d$Divorce_z)) + 
  theme_classic()
```

Before building a multivariate model, need to consider issues of causality.

## Causality and DAGs

Can use directed acyclic graphs (DAGs) to illustrate causality in models and highlight different types of confounding.

```{r}
dag_5_1 <- dagify(
  D ~ A, 
  M ~ A, 
  D ~ M, 
  outcome = "D", 
  coords = list(x = c(A = 0, D = 1, M = 2), 
                y = c(A = 1, D = 0, M = 1))) %>% 
  tidy_dagitty()
ggdag(dag_5_1) + 
  theme_dag()
```

This graph shows that there are two paths by which the marriage age may affect divorce rate: either directly or via the marriage rate. If this graph is correct then a model with both predictors will be better than either of the models with a single predictor.

## Multiple Regression

Now try combining the predictors, uncover how they vary in the same model. First specify the model:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i} + {\beta_A}{A_i} 
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Then build the model.

```{r}
b5_3 <- brm(
  Divorce_z ~ 1 + Marriage_z + MedianAgeMarriage_z, 
  data = d, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2000, 
  warmup = 1000, 
  file = "Stan/ch_05/b5_3"
)
summary(b5_3)
```

Now the estimate for $\beta_A$ is still strongly negative, while that for $\beta_M$ is close to zero. What does this mean? The coefficients answer the question: once all the other predictors are know, what does knowing this predictor tell us about the outcome? So once the median age of marriage is known we get very little extra information from the marriage rate.

Can see this in a dotplot of the coefficients.

```{r}
stanplot(b5_3) + 
  theme(text = element_text(family = "Avenir"))
```

There are ways to create this plot with `bayesplot::mcmc_intervals()` or `tidybayes::stat_pointintervalh()` also; both work with the `posterior_samples()` from the `brm` output. The main merit of these is that they allow for more customisation or the plot as the dotplot is just another geom to be layered. 

For plotting the posterior of multivariate regressions need to use a few techniques.

## Predictor Reisdual Plots

Use one of the predictors to model the other.

```{r}
b5_4a <- brm(
  Marriage_z ~ 1 + MedianAgeMarriage_z, 
  data = d, 
  prior = c(prior(normal(0, 10), class = Intercept), 
            prior(normal(0, 1), class = b), 
            prior(cauchy(0, 1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2500, 
  warmup = 1000, 
  file = "Stan/ch_05/b5_4a"
)
summary(b5_4a)
```

Use the `fitted()` values to get the first plot from the book.

```{r}
fitted_5_4a <- fitted(b5_4a, 
                      probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  bind_cols(d)

fitted_5_4a %>% 
  ggplot(aes(MedianAgeMarriage_z, Marriage_z)) + 
  geom_point(shape = 1, size = 2, colour = "steelblue4") + 
  geom_segment(aes(xend = MedianAgeMarriage_z, yend = Estimate), 
               size = 0.25) + 
  geom_line(aes(y = Estimate), 
            colour = "steelblue") + 
  theme_classic()
```

Now get the residuals and plot them.

```{r}
resids_5_4a <- residuals(b5_4a, 
                         probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.5, 0.5), 
                    Divorce = 14.1, 
                    label = c("slower", "faster"))

resids_5_4a %>% 
  ggplot(aes(Estimate, Divorce)) + 
  stat_smooth(method = "lm", 
              fullrange = TRUE, 
              colour = "steelblue4", 
              fill = "steelblue4", 
              alpha = 0.2, 
              size = 0.5) + 
  geom_vline(xintercept = 0, 
             linetype = 2, 
             colour = "grey50") + 
  geom_point(size = 2, 
             colour = "steelblue4", 
             alpha = 0.6) + 
  geom_text(data = plot_text, 
            aes(label = label)) + 
  scale_x_continuous(limits = c(-2, 2)) + 
  coord_cartesian(xlim = range(resids_5_4a$Estimate), 
                  ylim = c(6, 14.1)) + 
  labs(x = "Marriage Rate Residuals") + 
  theme_classic()
```

Now do the same in reverse.

```{r}
b5_4b <- brm(
  MedianAgeMarriage_z ~ 1 + Marriage_z, 
  data = d, 
  prior = c(prior(normal(0, 10), class = Intercept), 
            prior(normal(0, 1), class = b), 
            prior(cauchy(0, 1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  iter = 2500, 
  warmup = 1000, 
  file = "Stan/ch_05/b5_4b"
)
summary(b5_4b)
```

Now get the residuals and plot them.

```{r}
resids_5_4b <- residuals(b5_4b, 
                         probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.7, 0.5), 
                    Divorce = 14.1, 
                    label = c("younger", "older"))

resids_5_4b %>% 
  ggplot(aes(Estimate, Divorce)) + 
  stat_smooth(method = "lm", 
              fullrange = TRUE, 
              colour = "steelblue4", 
              fill = "steelblue4", 
              alpha = 0.2, 
              size = 0.5) + 
  geom_vline(xintercept = 0, 
             linetype = 2, 
             colour = "grey50") + 
  geom_point(size = 2, 
             colour = "steelblue4", 
             alpha = 0.6) + 
  geom_text(data = plot_text, 
            aes(label = label)) + 
  scale_x_continuous(limits = c(-2, 3)) + 
  coord_cartesian(xlim = range(resids_5_4b$Estimate), 
                  ylim = c(6, 14.1)) + 
  labs(x = "Age of Marriage Residuals") + 
  theme_classic()
```

## Counterfactual Plots

Answer the question: how would $y$ change if only one variable varied and the rest were set to the mean?

Start with doing this for marriage rate.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
  Marriage_z = seq(from = -3, to = 3, length.out = 100), 
  MedianAgeMarriage_z = mean(d$MedianAgeMarriage_z)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  rename(fit_low = Q5.5, 
         fit_hi  = Q94.5) %>% 
  bind_cols(
    predict(b5_3, 
            newdata = new_data, 
            probs = c(0.055, 0.945)) %>% 
      as_tibble() %>% 
      transmute(pred_low = Q5.5, 
                pred_hi  = Q94.5)
  ) %>% 
  bind_cols(new_data) %>% 
  ggplot(aes(Marriage_z, Estimate)) + 
  geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_line(colour = "steelblue4") + 
  coord_cartesian(xlim = range(d$Marriage_z)) + 
  labs(subtitle = "Counterfactual plot for \nMedianAgeMarriage_z = 0", 
       y = "Divorce_z") + 
  theme_classic() -> g1

g1
```

Now do the same for Median Age of Marriage.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
  Marriage_z = mean(d$Marriage_z), 
  MedianAgeMarriage_z = seq(from = -3, to = 3.5, length.out = 100)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  rename(fit_low = Q5.5, 
         fit_hi  = Q94.5) %>% 
  bind_cols(
    predict(b5_3, 
            newdata = new_data, 
            probs = c(0.055, 0.945)) %>% 
      as_tibble() %>% 
      transmute(pred_low = Q5.5, 
                pred_hi  = Q94.5)
  ) %>% 
  bind_cols(new_data) %>% 
  ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
  geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
              fill = "steelblue", 
              alpha = 0.2) + 
  geom_line(colour = "steelblue4") + 
  coord_cartesian(xlim = range(d$MedianAgeMarriage_z)) + 
  labs(subtitle = "Counterfactual plot for \nMarriage_z = 0", 
       y = "Divorce_z") + 
  theme_classic() -> g2

g2
```

Now plot both side by side.

```{r}
source("R/multiplot.R")
multiplot(g1, g2, cols = 2)
```

## Posterior Prediction Plots

Model checking to answer:

1. Did model fit correctly?
2. How did the model fail?

```{r}
fitted(b5_3, 
       probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  bind_cols(d) %>% 
  ggplot(aes(Divorce_z, Estimate)) + 
  geom_abline(linetype = 2, colour = "grey50", size = 0.5) + 
  geom_point(size = 1.5, colour = "steelblue4", alpha = 0.7) + 
  geom_linerange(aes(ymin = Q5.5, ymax = Q94.5), 
                 size = 0.25, colour = "steelblue4") + 
  geom_linerange(aes(ymin = Estimate - Est.Error, 
                     ymax = Estimate + Est.Error), 
                 size = 0.5, colour = "steelblue4") + 
  geom_text(data = . %>% filter(Loc %in% c("ID", "UT")), 
            aes(label = Loc), 
            hjust = 0, 
            nudge_x = -0.2) + 
  labs(x = "Observed Divorce", y = "Predicted Divorce") + 
  theme_classic()
```

```{r}
residuals(b5_3, 
          probs = c(0.055, 0.945)) %>% 
  as_tibble() %>% 
  rename(fit_low = Q5.5,
         fit_hi  = Q94.5) %>% 
  bind_cols(
    predict(b5_3, 
            probs = c(0.055, 0.945)) %>% 
      as_tibble() %>% 
      transmute(pred_low = Q5.5,
                pred_hi = Q94.5)) %>% 
  bind_cols(d) %>%
  # convert the pred_ intervals to a deviance metric
  mutate(pred_low = Divorce_z - pred_low,
         pred_hi = Divorce_z - pred_hi) %>% 
  ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) +
  geom_hline(yintercept = 0, 
             size = 0.5, 
             colour = "steelblue4", 
             alpha = 0.1) +
  geom_pointrange(aes(ymin = fit_low, ymax = fit_hi),
                  size = 0.4, 
                  shape = 20, 
                  colour = "steelblue4") + 
  geom_segment(aes(y    = Estimate - Est.Error, 
                   yend = Estimate + Est.Error,
                   x    = Loc, 
                   xend = Loc),
               size = 1, 
               colour = "steelblue4") +
  geom_segment(aes(y    = pred_low, 
                   yend = pred_hi,
                   x    = Loc, 
                   xend = Loc),
               size = 3, 
               colour = "steelblue4", 
               alpha = 0.1) +
  labs(x = NULL, y = NULL) +
  coord_flip() +
  theme_classic() + 
  theme(panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

## Simulating Spurious Association

Can illustrate the problem with a simulation where we know the true nature of relationships between variables.

```{r}
N <- 100
d2 <- tibble(x_real = rnorm(N)) %>% 
  mutate(x_spur = rnorm(N, mean = x_real), 
         y      = rnorm(N, mean = x_real))
head(d2)
```

Have a look at the pairs plots.

```{r}
pairs(d2, col = "steelblue4")
```

The DAG for these variables:

```{r}
dagify(x_spur ~ x_real, 
       y      ~ x_real, 
       outcome = "y") %>% 
  tidy_dagitty() %>% 
  ggdag()
```

Now model the relationships as:

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{real}x_{real} + \beta_{spur}x_{spur}
\\
\alpha \sim \mathcal{N}(0, 1)
\\
\beta_{real} \sim \mathcal{N}(0, 1)
\\
\beta_{spur} \sim \mathcal{N}(0, 1)
\\
\sigma \sim \text{HalfCauchy}(0, 1)
$$

```{r}
b5_4c <- brm(
  y ~ 1 + x_real + x_spur, 
  data = d2, 
  prior = c(prior(normal(0, 1), class = Intercept), 
            prior(normal(0, 1), class = b), 
            prior(cauchy(0, 1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/b5_4c"
)
fixef(b5_4c, probs = c(0.055, 0.945)) %>% 
  round(digits = 2)
```

As it should the model gets the coefficients 'right': for $x_{real}$ c. 1; for $x_{spur}$ about 0.

## Masked Relationships

Can use a different dataset to explore a different sort of confounding structure.

```{r}
data("milk", package = "rethinking")
d <- milk %>% 
  as_tibble() %>% 
  set_names(nm = names(.) %>% 
              str_replace_all("\\.", "_"))
head(d)
```

Can look at pairs plots for the three variables of interest.

```{r}
d %>% 
  select(kcal_per_g, mass, neocortex_perc) %>% 
  pairs(col = "steelblue4")
```

Cannot tell much from this, so start modelling! Start with a simple model using only `necortex_perc` to predict `kcal_per_g`:

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{N}N_i
$$

Before getting into priors need to drop the NA values. (The `quap` function used in the book requires this while `brms::brm()` will just do casewise deletion, but doing this deliberately is good practice anyway.) Use `scale()` to normalise the variables at the same time.

```{r}
d2 <- d %>% 
  select(kcal_per_g, mass, neocortex_perc) %>% 
  filter(complete.cases(.)) %>% 
  mutate(K = scale(kcal_per_g), 
         M = scale(log(mass)), 
         N = scale(neocortex_perc))
```

Can set this up as a prior-only `brm()` model and then plot the two sets of priors given to see if they are plausible.

```{r}
ppd_5_5_a <- brm(
  K ~ 1 + N, 
  data = d2, 
  prior = c(prior(normal(0, 1), class = Intercept), 
            prior(normal(0, 1), class = b), 
            prior(exponential(1), class = sigma)), 
  sample_prior = "only", 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/ppd_5_5_a"
)

ppd_5_5_b <- brm(
  K ~ 1 + N, 
  data = d2, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  sample_prior = "only", 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/ppd_5_5_b"
)
```

Now plot the PPD side by side.

```{r}
g1 <- marginal_effects(ppd_5_5_a, method = "predict") %>% 
  plot(points = TRUE, plot = FALSE) 

g2 <- marginal_effects(ppd_5_5_b, method = "predict") %>% 
  plot(points = TRUE, plot = FALSE) 
g1$N +
  labs(x = "Neocortex Percentage (z-score)", 
       y = "Kcal per g (z-score)") + 
  ggtitle("PPD: Looser Priors") + 
  coord_cartesian(ylim = c(-6, 6)) + 
  theme_minimal() -> plot_1
g2$N +
  labs(x = "Neocortex Percentage (z-score)", 
       y = "Kcal per g (z-score)") + 
  ggtitle("PPD: Tighter Priors") + 
  coord_cartesian(ylim = c(-6, 6)) + 
  theme_minimal() -> plot_2

multiplot(plot_1, plot_2, cols = 2)
```

As shown earlier even priors that seem to be extremely strict are quite permissive when dealing with variables that have been normalised.

Now build the model with the stricter priors:

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{N}N_i
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_{N} \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

```{r}
b5_5 <- brm(
  K ~ 1 + N, 
  data = d2, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/b5_5"
)
print(b5_5, digits = 3)
```

Set up a plot for this regression.

```{r}
new_data <- tibble(N = seq(-4, 4, length.out = 100))
fitted(b5_5,  
       newdata = new_data,
       probs = c(0.055, 0.945)) %>%
  as_tibble() %>%
  bind_cols(new_data) %>% 
  
  ggplot(aes(x = N, y = Estimate)) +
  geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
              fill = "steelblue", 
              alpha = 0.2) +
  geom_line(color = "steelblue4", 
            size = 0.5) +
  geom_point(data = d2, 
             aes(x = N, y = K),
             size = 2, 
             color = "steelblue4") +
  coord_cartesian(xlim = range(d2$N), 
                  ylim = range(d2$K)) + 
  labs(y = "K") + 
  ggtitle("K regressed on N") + 
  theme_classic() -> g3
```

Can build a similar model using log of mass `M` as the predictor.

$$
K_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{M}M_i
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_{M} \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

```{r}
b5_6 <- brm(
  K ~ 1 + M, 
  data = d2, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/b5_6"
)
print(b5_6, digits = 3)
```

And view both plots side by side.

```{r}
new_data <- tibble(M = seq(-4, 4, length.out = 100))
fitted(b5_6,  
       newdata = new_data,
       probs = c(0.055, 0.945)) %>%
  as_tibble() %>%
  bind_cols(new_data) %>% 
  
  ggplot(aes(x = M, y = Estimate)) +
  geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
              fill = "steelblue", 
              alpha = 0.2) +
  geom_line(color = "steelblue4", 
            size = 0.5) +
  geom_point(data = d2, 
             aes(x = M, y = K),
             size = 2, 
             color = "steelblue4") +
  coord_cartesian(xlim = range(d2$M), 
                  ylim = range(d2$K)) + 
  labs(y = "K") +
  ggtitle("K regressed on M") + 
  theme_classic() -> g4
multiplot(g3, g4, cols = 2)
```

The associations look fairly weak. But now we can build the multiple regression model.

```{r}
b5_7 <- brm(
  K ~ 1 + M + N, 
  data = d2, 
  prior = c(prior(normal(0, 0.2), class = Intercept), 
            prior(normal(0, 0.5), class = b), 
            prior(exponential(1), class = sigma)), 
  chains = 4, 
  cores = 4, 
  file = "Stan/ch_05/b5_7"
)
print(b5_7, digits = 3)
```

The coefficients imply stronger associations than in the bivariate regressions. Can see this more clearly in a plot.


```{r}
stanplot(b5_7) + 
  theme(text = element_text(family = "Avenir"))
```

The counterfactual plots show this well.

```{r}
new_data_g5 <- tibble(N = seq(-4, 4, length.out = 100), 
                      M = 0)
new_data_g6 <- tibble(M = seq(-4, 4, length.out = 100), 
                      N = 0)
g5 <- fitted(b5_7,  
             newdata = new_data_g5,
             probs = c(0.055, 0.945)) %>%
  as_tibble() %>%
  bind_cols(new_data_g5) %>% 
  
  ggplot(aes(x = N, y = Estimate)) +
  geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
              fill = "steelblue", 
              alpha = 0.2) +
  geom_line(color = "steelblue4", 
            size = 0.5) +
  geom_point(data = d2, 
             aes(x = N, y = K),
             size = 2, 
             color = "steelblue4") +
  coord_cartesian(xlim = range(d2$N), 
                  ylim = range(d2$K)) + 
  labs(y = "K") +
  ggtitle("Counterfactual holding M = 0") + 
  theme_classic()

g6 <- fitted(b5_7,  
             newdata = new_data_g6,
             probs = c(0.055, 0.945)) %>%
  as_tibble() %>%
  bind_cols(new_data_g6) %>% 
  
  ggplot(aes(x = M, y = Estimate)) +
  geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5),
              fill = "steelblue", 
              alpha = 0.2) +
  geom_line(color = "steelblue4", 
            size = 0.5) +
  geom_point(data = d2, 
             aes(x = M, y = K),
             size = 2, 
             color = "steelblue4") +
  coord_cartesian(xlim = range(d2$M), 
                  ylim = range(d2$K)) + 
  labs(y = "K") +
  ggtitle("Counterfactual holding N = 0") + 
  theme_classic()
multiplot(g5, g6, cols = 2)
```

The associations are now much clearer: why? Because while both the log-mass and neocortex percentage are predictive of the energy in the primate's milk, but in opposite directions: can look at the pairs plots of the normalised variables to see this:

```{r}
d2 %>% 
  select(K, N, M) %>% 
  pairs(col = "steelblue")
```

Neocortex percentage is positively correlated with milk energy, while log-mass is negatively correlated with it. Log-mass and neocortex percentage are also positively correlated with each other. All of this masks the true relationship unless we consider them all together. 

Can illustrate this with DAGs, at least three of which are consistent with these data and results.

```{r}
# create reusable coordinates
dag_coords <- list(x = c(M = 0, K = 1, N = 2, U = 1), 
                   y = c(M = 1, K = 0, N = 1, U = 1))
dag_5_7a <- dagify(N ~ M, 
                   K ~ M, 
                   K ~ N, 
                   outcome = "K", 
                   coords = dag_coords) %>% 
  tidy_dagitty() %>% 
  ggdag()
dagify(N ~ M, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
  tidy_dagitty() %>% 
  ggdag()
dagify(M ~ N, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
  tidy_dagitty() %>% 
  ggdag()
dagify(N ~ U,
       M ~ U, 
       K ~ M, 
       K ~ N, 
       outcome = "K", 
       coords = dag_coords) %>% 
  tidy_dagitty() %>% 
  ggdag()
```

In the first log-mass causes neocortex percentage; in the second that is reversed; in the third some unobserved variable causes both of them.

## When Adding Variables Hurts

Adding a predictor can make a model worse though. An example with legs, based on the reasonable assumption that leg-length is a good predictor of height.

```{r}
# simulate the data
N <- 100
set.seed(531)

d <- tibble(height = rnorm(N, mean = 10, sd = 2), 
            leg_prop = runif(N, min = 0.4, max = 0.5)) %>% 
  mutate(leg_left = leg_prop * height + rnorm(N, sd = 0.02), 
         leg_right = leg_prop * height + rnorm(N, sd = 0.02))
```

The two leg lengths are strongly correlated:

```{r}
d %>% 
  select(leg_left, leg_right) %>% 
  cor() %>% 
  round(digits = 5)

d %>% 
  ggplot(aes(leg_left, leg_right)) + 
  stat_smooth(method = "lm", 
              colour = "grey50", 
              size = 0.5) + 
  geom_point(colour = "steelblue") + 
  theme_classic()
```

So what happens if we use both for predicting height?

```{r}
b5_8 <- brm(
      height ~ 1 + leg_left + leg_right,
      data = d, 
      prior = c(prior(normal(10, 100), class = Intercept),
                prior(normal(2, 10), class = b),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000, 
      warmup = 500, 
      chains = 4, 
      cores = 4, 
      file = "Stan/ch_05/b5_8")
print(b5_8, digits = 3)
```

The error on the parameter estimates is huge, as we can see in a plot.

```{r}
stanplot(b5_8) + 
  theme(text = element_text(family = "Avenir"))
```

This problem of collinearity is why it's important to be careful with variable selection when building models: adding both causes confusion in this case.

## Categorical Variables

Can use the !Kung data again for working with a categorical variable, which in this case is sex.

```{r}
data("Howell1", package = "rethinking")
d <- Howell1 %>% 
  as_tibble()
rm(Howell1)
head(d)
```

The first approach will be to use the `male` variable as-is: a _dummy variable_. NB. we need one fewer variable than there are levels of the factor.

Build a model of height using only `male` as the predictor.

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_m}{m_i}
\\
\alpha \sim \mathcal{N}(178, 20)
\\
{\beta_m} \sim \mathcal{N}(0, 10)
\\
\sigma \sim \text{HalfCauchy}(0, 2)
$$

NB. The book uses a uniform prior on $\sigma$ but that causes problems for the simulation.

```{r}
b5_9 <- brm(
  height ~ 1 + male, 
  data = d, 
  prior = c(prior(normal(178, 20), class = Intercept), 
            prior(normal(0, 10), class = b), 
            prior(cauchy(0, 2), class = sigma)), 
  cores = 4, 
  chains = 4, 
  file = "Stan/ch_05/b5_9"
)
print(b5_9, digits = 3)
```

With this approach $\beta_m$ represents the expected difference between a man and a woman. However there is an imbalance: all heights are predicted using $\alpha$, but only males' heights use $\beta_m$. This creates additional uncertainty in males' heights, as they rely on two parameter estimates. Can see this by simulating from the priors.

```{r}
tibble(mu_female = rnorm(1e4, 178, 20), 
       mu_male   = rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)) %>% 
  gather() %>% 
  group_by(key) %>% 
  summarise(mean = mean(value), 
            std_dev = sd(value))
```

An alternative approach given in the book is to use index variables, so that sex is recoded, as below:

```{r}
d$sex <- factor(if_else(d$male == 1, "male", "female"))
  
head(d)
```

Then the model specification would be:

$$
h_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha_{SEX[i]}
\\
\alpha_j \sim \mathcal{N}(178, 20), \text{for} j = 1, 2
\\
\sigma \sim \text{HalfCauchy}(0, 2)
$$

Doing this with `brms` is easy if the predictor is a factor: just have to remember to remove the intercept in the formula.

```{r}
b5_10 <- brm(
  height ~ 0 + sex, 
  data = d, 
  prior = c(prior(normal(178, 20), class = b), 
            prior(cauchy(0, 2), class = sigma)), 
  cores = 4, 
  chains = 4, 
  file = "Stan/ch_05/b5_10"
)
print(b5_10, digits = 3)
```


