---
title: "Chapter 5: Multivariate Linear Models"
output: 
  html_document: 
    highlight: kate
    theme: journal
---

# Notes

## Intro

Multiple regression is a common technique; some of the reasons it is popular:

1. 'Controlling' for confounds: when there are confounding factors multiple regression can take this into account by including all of the causal variables. However this is not always so simple, and needs to be done with care.
2. Multiple causation.
3. Interactions.

Can illustrate some of this with the `WaffleDivorce` dataset.

```{r}
# load data
library(dagitty)
library(ggdag)
library(bayesplot)
library(tidybayes)
library(fiftystater)
library(brms)
library(ggrepel)
library(tidyverse)
data("WaffleDivorce", package = "rethinking")
d <- WaffleDivorce %>% 
    as_tibble() %>% 
    set_names(nm = names(.) %>% 
                  str_replace_all("\\.", "_")) %>% 
    # build z-score versions of the main variables we'll use
    mutate(Divorce_z           = scale(Divorce), 
           MedianAgeMarriage_z = scale(MedianAgeMarriage), 
           Marriage_z          = scale(Marriage))
# check out the data
str(d)
glimpse(d)
summary(d)
```

## Spurious Association

Plot divorce rate against the prevalence of Waffle Houses.

```{r}
d %>% 
    ggplot(aes(WaffleHouses/Population, 
               Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                size = 0.5, 
                alpha = 0.2, 
                colour = "steelblue4", 
                fill = "steelblue") + 
    geom_point(size = 1.5, alpha = 0.5, colour = "steelblue4") + 
    geom_text_repel(data = d %>% 
                        filter(Loc %in% c("ME", "OK", "AR", "AL", 
                                          "GA", "SC", "NJ")), 
                    aes(label = Loc), 
                    size = 3, 
                    seed = 1042) + 
    scale_x_continuous(limits = c(0, 55)) + 
    coord_cartesian(xlim = c(0, 50), 
                    ylim = c(5, 15)) + 
    labs(x = "Waffle Houses per million", 
         y = "Divorce Rate") + 
    theme_classic()
```

There is a positive correlation between them, but this is not evidence of causality. The two main predictors we might want to consider are the median age of marriage (`MedianAgeMarriage`) and the marriage rate (`Marriage`). Can visualise all of these in multiple ways, firstly as a heat map on the US.

```{r}
d %>% 
    # do this to match format in fifty_states
    mutate(Location = str_to_lower(Location)) %>% 
    select(ends_with("_z"), # use z-score version to keep scales consistent
           Location) %>% 
    {
        # will otherwise generate warnings about attributes being dropped
        suppressWarnings(gather(., key, value, -Location))
    } %>% 
    ggplot(aes(map_id = Location)) + 
    geom_map(aes(fill = value), 
             map = fifty_states, 
             colour = "grey50",
             size = 0.02) + 
    expand_limits(x = fifty_states$long, y = fifty_states$lat) + 
    scale_x_continuous(NULL, breaks = NULL) + 
    scale_y_continuous(NULL, breaks = NULL) + 
    scale_fill_viridis_c(option = "B") + 
    coord_map() + 
    theme_bw() +
    theme(panel.grid       = element_blank(),
          strip.background = element_rect(fill = "transparent", 
                                          color = "transparent")) +
    facet_wrap(~key)
```

Then can create pairs plots for the variables.

```{r}
d %>% 
    select(ends_with("_z")) %>% 
    pairs(col = "steelblue")
```

Can see some evidence of association, but which are meaningful?

## Simple Linear Regression

Start by building a simple linear model with `MedianAgeMarriage_z` ($A$) predicting `Divorce_z` ($D$). Model specification is:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_A}{A_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_1 <- brm(
    Divorce_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_1"
)
summary(b5_1)
```

These priors seem quite strict, but this is because the variables have been normalised and therefore seemingly small changes can be very large on the natural scale. See this by generating plots of the prior predictive distribution, which we can get by rebuilding the model but setting `sample_prior = "only"`.

```{r}
ppd_5_1 <- brm(
    Divorce_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    sample_prior = "only", 
    file = "Stan/ch_05/ppd_5_1", 
    control = list(adapt_delta = 0.9)
)
```

Now we can plot the marginal effects to see the range of plausible associations between predictor and outcome.

```{r}
g <- marginal_effects(ppd_5_1, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 

g$MedianAgeMarriage_z +
    labs(x = "Median Age Marriage (z-score)", 
         y = "Divorce Rate (z-score)") + 
    ggtitle("Prior predictive distribution") + 
    theme_minimal()
```

There is still quite a range of plausible associations, so the priors are not as restrictive as they may seem.

The summary for the model shows that the relationship between predictor and outcome is quite certainly negative. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_age_seq <- tibble(MedianAgeMarriage_z = seq(-3, 3, length.out = 100))

fitted_5_1 <- marriage_age_seq %>% 
    bind_cols(fitted(b5_1, 
                     newdata = marriage_age_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_1 %>% 
    ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(MedianAgeMarriage_z, 
                   Divorce_z), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Divorce (z-score)", 
         x = "Median Age Marriage (z-score)") + 
    coord_cartesian(xlim = range(d$MedianAgeMarriage_z), 
                    ylim = range(d$Divorce_z)) + 
    theme_classic()
```

Now follow the same set of steps, but for `Marriage_z` ($M$) as the predictor.

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i}
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Now build the model.

```{r}
b5_2 <- brm(
    Divorce_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_2"
)
summary(b5_2)
```

Generate the prior predictive distribution plots again to check the priors.

```{r}
ppd_5_2 <- brm(
    Divorce_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    sample_prior = "only", 
    file = "Stan/ch_05/ppd_5_2", 
    control = list(adapt_delta = 0.9)
)

g <- marginal_effects(ppd_5_2, method = "predict") %>% 
    plot(points = TRUE, plot = FALSE) 

g$Marriage_z +
    labs(x = "Marriage Rate (z-score)", 
         y = "Divorce Rate (z-score)") + 
    ggtitle("Prior predictive distribution") + 
    theme_minimal()
```

The summary for the model shows that the relationship between predictor and outcome is probably positive. Can plot posterior predictions to see how these compare with the raw data.

```{r}
marriage_rate_seq <- tibble(Marriage_z = seq(-3, 3, length.out = 100))

fitted_5_2 <- marriage_rate_seq %>% 
    bind_cols(fitted(b5_2, 
                     newdata = marriage_rate_seq, 
                     probs = c(0.055, 0.945)) %>% 
                  as_tibble())

fitted_5_2 %>% 
    ggplot(aes(Marriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = Q5.5, 
                    ymax = Q94.5), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    geom_point(data = d, 
               aes(Marriage_z, 
                   Divorce_z), 
               size = 2, 
               colour = "steelblue4") + 
    labs(y = "Divorce (z-score)", 
         x = "Marriage Rate (z-score)") + 
    coord_cartesian(xlim = range(d$Marriage_z), 
                    ylim = range(d$Divorce_z)) + 
    theme_classic()
```

Before building a multivariate model, need to consider issues of causality.

## Causality and DAGs

Can use directed acyclic graphs (DAGs) to illustrate causality in models and highlight different types of confounding.

```{r}
dag_5_1 <- dagify(
    D ~ A, 
    M ~ A, 
    D ~ M, 
    outcome = "D", 
    coords = list(x = c(A = 0, D = 1, M = 2), 
                             y = c(A = 1, D = 0, M = 1))) %>% 
    tidy_dagitty()
ggdag(dag_5_1) + 
    theme_dag()
```

This graph shows that there are two paths by which the marriage age may affect divorce rate: either directly or via the marriage rate. If this graph is correct then a model with both predictors will be better than either of the models with a single predictor.

## Multiple Regression

Now try combining the predictors, uncover how they vary in the same model. First specify the model:

$$
D_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + {\beta_M}{M_i} + {\beta_A}{A_i} 
\\
\alpha \sim \mathcal{N}(0, 0.2)
\\
\beta_M \sim \mathcal{N}(0, 0.5)
\\
\beta_A \sim \mathcal{N}(0, 0.5)
\\
\sigma \sim \text{Exponential}(1)
$$

Then build the model.

```{r}
b5_3 <- brm(
    Divorce_z ~ 1 + Marriage_z + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept), 
              prior(normal(0, 0.5), class = b), 
              prior(exponential(1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2000, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_3"
)
summary(b5_3)
```

Now the estimate for $\beta_A$ is still strongly negative, while that for $\beta_M$ is close to zero. What does this mean? The coefficients answer the question: once all the other predictors are know, what does knowing this predictor tell us about the outcome? So once the median age of marriage is known we get very little extra information from the marriage rate.

Can see this in a dotplot of the coefficients.

```{r}
stanplot(b5_3) + 
    theme(text = element_text(family = "Avenir"))
```

There are ways to create this plot with `bayesplot::mcmc_intervals()` or `tidybayes::stat_pointintervalh()` also; both work with the `posterior_samples()` from the `brm` output. The main merit of these is that they allow for more customisation or the plot as the dotplot is just another geom to be layered. 

For plotting the posterior of multivariate regressions need to use a few techniques.

## Predictor Reisdual Plots

Use one of the predictors to model the other.

```{r}
b5_4a <- brm(
    Marriage_z ~ 1 + MedianAgeMarriage_z, 
    data = d, 
    prior = c(prior(normal(0, 10), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2500, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_4a"
)
summary(b5_4a)
```

Use the `fitted()` values to get the first plot from the book.

```{r}
fitted_5_4a <- fitted(b5_4a, 
                     probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

fitted_5_4a %>% 
    ggplot(aes(MedianAgeMarriage_z, Marriage_z)) + 
    geom_point(shape = 1, size = 2, colour = "steelblue4") + 
    geom_segment(aes(xend = MedianAgeMarriage_z, yend = Estimate), 
                 size = 0.25) + 
    geom_line(aes(y = Estimate), 
              colour = "steelblue") + 
    theme_classic()
```

Now get the residuals and plot them.

```{r}
resids_5_4a <- residuals(b5_4a, 
                        probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.5, 0.5), 
                    Divorce = 14.1, 
                    label = c("slower", "faster"))

resids_5_4a %>% 
    ggplot(aes(Estimate, Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                colour = "steelblue4", 
                fill = "steelblue4", 
                alpha = 0.2, 
                size = 0.5) + 
    geom_vline(xintercept = 0, 
               linetype = 2, 
               colour = "grey50") + 
    geom_point(size = 2, 
               colour = "steelblue4", 
               alpha = 0.6) + 
    geom_text(data = plot_text, 
              aes(label = label)) + 
    scale_x_continuous(limits = c(-2, 2)) + 
    coord_cartesian(xlim = range(resids_5_4a$Estimate), 
                    ylim = c(6, 14.1)) + 
    labs(x = "Marriage Rate Residuals") + 
    theme_classic()
```

Now do the same in reverse.

```{r}
b5_4b <- brm(
    MedianAgeMarriage_z ~ 1 + Marriage_z, 
    data = d, 
    prior = c(prior(normal(0, 10), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    iter = 2500, 
    warmup = 1000, 
    file = "Stan/ch_05/b5_4b"
)
summary(b5_4b)
```

Now get the residuals and plot them.

```{r}
resids_5_4b <- residuals(b5_4b, 
                        probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d)

# to add some annotation
plot_text <- tibble(Estimate = c(-0.7, 0.5), 
                    Divorce = 14.1, 
                    label = c("younger", "older"))

resids_5_4b %>% 
    ggplot(aes(Estimate, Divorce)) + 
    stat_smooth(method = "lm", 
                fullrange = TRUE, 
                colour = "steelblue4", 
                fill = "steelblue4", 
                alpha = 0.2, 
                size = 0.5) + 
    geom_vline(xintercept = 0, 
               linetype = 2, 
               colour = "grey50") + 
    geom_point(size = 2, 
               colour = "steelblue4", 
               alpha = 0.6) + 
    geom_text(data = plot_text, 
              aes(label = label)) + 
    scale_x_continuous(limits = c(-2, 3)) + 
    coord_cartesian(xlim = range(resids_5_4b$Estimate), 
                    ylim = c(6, 14.1)) + 
    labs(x = "Age of Marriage Residuals") + 
    theme_classic()
```

## Counterfactual Plots

Answer the question: how would $y$ change if only one variable varied and the rest were set to the mean?

Start with doing this for marriage rate.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
    Marriage_z = seq(from = -3, to = 3, length.out = 100), 
    MedianAgeMarriage_z = mean(d$MedianAgeMarriage_z)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
                newdata = new_data, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data) %>% 
    ggplot(aes(Marriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$Marriage_z)) + 
    labs(subtitle = "Counterfactual plot for \nMedianAgeMarriage_z = 0", 
         y = "Divorce_z") + 
    theme_classic() -> g1

g1
```

Now do the same for Median Age of Marriage.

```{r}
# need to build a tibble with possible data for M and A set to its mean
new_data <- tibble(
    Marriage_z = mean(d$Marriage_z), 
    MedianAgeMarriage_z = seq(from = -3, to = 3.5, length.out = 100)
)

fitted(b5_3, 
       newdata = new_data, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5, 
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
                newdata = new_data, 
                probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5, 
                      pred_hi  = Q94.5)
    ) %>% 
    bind_cols(new_data) %>% 
    ggplot(aes(MedianAgeMarriage_z, Estimate)) + 
    geom_ribbon(aes(ymin = pred_low, ymax = pred_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_ribbon(aes(ymin = fit_low, ymax = fit_hi), 
                fill = "steelblue", 
                alpha = 0.2) + 
    geom_line(colour = "steelblue4") + 
    coord_cartesian(xlim = range(d$MedianAgeMarriage_z)) + 
    labs(subtitle = "Counterfactual plot for \nMarriage_z = 0", 
         y = "Divorce_z") + 
    theme_classic() -> g2

g2
```

Now plot both side by side.

```{r}
source("R/multiplot.R")
multiplot(g1, g2, cols = 2)
```

## Posterior Prediction Plots

Model checking to answer:

1. Did model fit correctly?
2. How did the model fail?

```{r}
fitted(b5_3, 
       probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    bind_cols(d) %>% 
    ggplot(aes(Divorce_z, Estimate)) + 
    geom_abline(linetype = 2, colour = "grey50", size = 0.5) + 
    geom_point(size = 1.5, colour = "steelblue4", alpha = 0.7) + 
    geom_linerange(aes(ymin = Q5.5, ymax = Q94.5), 
                   size = 0.25, colour = "steelblue4") + 
    geom_linerange(aes(ymin = Estimate - Est.Error, 
                       ymax = Estimate + Est.Error), 
                   size = 0.5, colour = "steelblue4") + 
    geom_text(data = . %>% filter(Loc %in% c("ID", "UT")), 
              aes(label = Loc), 
              hjust = 0, 
              nudge_x = -0.2) + 
    labs(x = "Observed Divorce", y = "Predicted Divorce") + 
    theme_classic()
```

```{r}
residuals(b5_3, 
          probs = c(0.055, 0.945)) %>% 
    as_tibble() %>% 
    rename(fit_low = Q5.5,
           fit_hi  = Q94.5) %>% 
    bind_cols(
        predict(b5_3, 
          probs = c(0.055, 0.945)) %>% 
            as_tibble() %>% 
            transmute(pred_low = Q5.5,
                      pred_hi = Q94.5)) %>% 
    bind_cols(d) %>%
    # convert the pred_ intervals to a deviance metric
    mutate(pred_low = Divorce_z - pred_low,
           pred_hi = Divorce_z - pred_hi) %>% 
    ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) +
    geom_hline(yintercept = 0, 
               size = 0.5, 
               colour = "steelblue4", 
               alpha = 0.1) +
    geom_pointrange(aes(ymin = fit_low, ymax = fit_hi),
                    size = 0.4, 
                    shape = 20, 
                    colour = "steelblue4") + 
    geom_segment(aes(y    = Estimate - Est.Error, 
                     yend = Estimate + Est.Error,
                     x    = Loc, 
                     xend = Loc),
                 size = 1, 
                 colour = "steelblue4") +
    geom_segment(aes(y    = pred_low, 
                     yend = pred_hi,
                     x    = Loc, 
                     xend = Loc),
                 size = 3, 
                 colour = "steelblue4", 
                 alpha = 0.1) +
    labs(x = NULL, y = NULL) +
    coord_flip() +
    theme_classic() + 
    theme(panel.grid   = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y  = element_text(hjust = 0))
```

## Simulating Spurious Association

Can illustrate the problem with a simulation where we know the true nature of relationships between variables.

```{r}
N <- 100
d2 <- tibble(x_real = rnorm(N)) %>% 
    mutate(x_spur = rnorm(N, mean = x_real), 
           y      = rnorm(N, mean = x_real))
head(d2)
```

Now model the relationships as:

$$
y_i \sim \mathcal{N}(\mu_i, \sigma)
\\
\mu_i = \alpha + \beta_{real}x_{real} + \beta_{spur}x_{spur}
\\
\alpha \sim \mathcal{N}(0, 1)
\\
\beta_{real} \sim \mathcal{N}(0, 1)
\\
\beta_{spur} \sim \mathcal{N}(0, 1)
\\
\sigma \sim \text{HalfCauchy}(0, 1)
$$

```{r}
b5_4c <- brm(
    y ~ 1 + x_real + x_spur, 
    data = d2, 
    prior = c(prior(normal(0, 1), class = Intercept), 
              prior(normal(0, 1), class = b), 
              prior(cauchy(0, 1), class = sigma)), 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_05/b5_4c"
)
summary(b5_4c)
```

As it should the model gets the coefficients 'right': for $x_{real}$ c. 1; for $x_{spur}$ about 0.

## Masked Relationships

Can use a different dataset to explore a different sort of confounding structure.

```{r}
data("milk", package = "rethinking")
d <- milk %>% 
    as_tibble() %>% 
    set_names(nm = names(.) %>% 
                  str_replace_all("\\.", "_"))
head(d)
```






