---
title: "Chapter 6: The Haunted DAG & The Causal Terror"
---

```{r}
library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(dagitty)
library(ggdag)
```

# Notes

## Intro

Recreate the example shown in the book. Take some styling tips from Aki Vehtari's [Bayesian Data Analysis examples][BDA_R] by way of [Solomon Kurtz][SKurtz_SR_2ed] working his way through the second edition.

```{r}
# number of grant proposals
N <- 200L
# proportion of grant proposals to approve
p <- 0.1
set.seed(1917)
d <- tibble(
    # the two variables are totally uncorrelated
    trustworthiness = rnorm(N), 
    newsworthiness = rnorm(N)
) %>% 
    mutate(total_score = trustworthiness + newsworthiness) %>% 
    mutate(selected = total_score >= quantile(total_score, 1 - p))
```

What is the correlation between newsworthiness and trustwortiness among selected articles?

```{r}
d %>% 
    filter(selected) %>% 
    select(newsworthiness, trustworthiness) %>% 
    cor()
```

Can visualise how this happens.

```{r}
theme_set(theme_minimal())
# create a tibble for the annotation
berksen_text <- tibble(
    newsworthiness  = c(2.25, 1.3), 
    trustworthiness = c(2.25, -2.4),
    selected = c(TRUE, FALSE),
    label    = c("selected", "rejected")
)
d %>% 
    ggplot(
        aes(
            newsworthiness, 
            trustworthiness, 
            colour = selected)) + 
    geom_point(alpha = .6) + 
    geom_text(data = berksen_text, 
              aes(label = label)) + 
    geom_smooth(data = d %>% filter(selected), 
                method = "lm", 
                fullrange = TRUE, 
                fill = "orange", 
                color = "orange", 
                alpha = 0.25, 
                size = 0.2) + 
    scale_colour_manual(values = c("black", "orange")) + 
    coord_cartesian(ylim = range(d$trustworthiness)) +
    theme(legend.position = "none")
```

This chapter focuses on the bad things that can go wrong when adding additional predictors to a linear model:

- Multicollinearity;
- Post-treatment bias; 
- Collider bias.

## Multicollinearity

### Height as a Function of Leg-length

Adding a predictor can make a model worse though. An example with legs, based on the reasonable assumption that leg-length is a good predictor of height.

```{r}
# simulate the data
N <- 100
set.seed(909)

d <- tibble(height = rnorm(N, mean = 10, sd = 2), 
            leg_prop = runif(N, min = 0.4, max = 0.5)) %>% 
    mutate(leg_left = leg_prop * height + rnorm(N, sd = 0.02), 
           leg_right = leg_prop * height + rnorm(N, sd = 0.02)) %>% 
    select(-leg_prop)
```

The two leg lengths are strongly correlated:

```{r}
d %>% 
    select(leg_left, leg_right) %>% 
    cor() %>% 
    round(digits = 5)

d %>% 
    ggplot(aes(leg_left, leg_right)) + 
    stat_smooth(method = "lm", 
                colour = "grey50", 
                size = 0.5) + 
    geom_point(colour = "steelblue")
```

So what happens if we use both for predicting height?

```{r}
b6_1 <- brm(
    height ~ 1 + leg_left + leg_right,
    data = d, 
    prior = c(prior(normal(10, 100), class = Intercept),
              prior(normal(2, 10), class = b),
              prior(exponential(1), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_1")
print(b6_1, digits = 3)
```

The error on the parameter estimates is huge, as we can see in a plot.

```{r}
color_scheme_set("orange")
stanplot(b6_1, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

This problem of collinearity is why it's important to be careful with variable selection when building models: adding both causes confusion in this case. Can get various other plots, such as the pairs.

```{r}
pairs(b6_1, 
      pars = "leg_[lr]")
```

The coefficients now have an almost perfect _negative_ correlation.

Some posterior sampling can illustrate this also.

```{r}
post <- posterior_samples(b6_1)

post %>% 
    ggplot(aes(b_leg_left, b_leg_right)) + 
    geom_point(colour = "steelblue", 
               alpha = 0.1, size = 0.5)
```

Yet another method for illustrating is with `bayesplot::mcmc_scatter()`.

```{r}
post %>% 
    mcmc_scatter(regex_pars = "b_leg_[lr]", 
                 alpha = 0.1, 
                 size = 0.5)
```

The model is answering the question:

> What is the value of knowing each predictor, after already knowing all of the other predictors?

Since the two legs offer almost the same info, the only answer to this question is "very little".

The model is set up as:

$$
\begin{eqnarray}
y_i & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta_{1}x_{1i} + \beta_{2} x_{2i}
\end{eqnarray}
$$

But since $x_{1i}$ and $x_{2i}$ are basically identical we can ditch the extra subscript and just call them a single predictor, $x_i$ for the $i$-th observation. Then the model can be written as.

$$
\begin{eqnarray}
y_i & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha + (\beta_1 + \beta_2) x_i
\end{eqnarray}
$$

So what are the posterior estimates for the sum of $\beta_1$ and $\beta_2$?

```{r}
post %>% 
    ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + 
    geom_halfeyeh(fill = "steelblue", 
                  point_interval = median_qi, 
                  .width = 0.89) + 
    scale_y_continuous(NULL, breaks = NULL) + 
    labs(title    = "Sum the multicollinear coefficients",
         subtitle = "Marked by the median and 89% PIs")
```

Now build a model with only one predictor.

```{r}
b6_2 <- brm(
    height ~ 1 + leg_left,
    data = d, 
    prior = c(prior(normal(10, 100), class = Intercept),
              prior(normal(2, 10), class = b),
              prior(exponential(1), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_2")

stanplot(b6_2, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

The parameter estimate for `leg_left` is exactly as it should be from the way the data were simulated. Can also check the density plot.

```{r}
posterior_samples(b6_2) %>% 
    ggplot(aes(x = b_leg_left, y = 0)) +
    geom_halfeyeh(fill = "steelblue", 
                  point_interval = median_qi, 
                  .width = .89) +
    scale_y_continuous(NULL, breaks = NULL) +
    labs(title    = "Just one coefficient needed",
         subtitle = "Marked by the median and 89% PIs",
         x        = "only b_leg_left, this time")
```

This gets almost identical results. Adding the extra predictor was worse than useless.

### Multicollinear Milk

Another example with the `milk` dataset from `rethinking`.

```{r}
data(milk, package = "rethinking")
d <- milk %>% 
    as_tibble() %>% 
    mutate(K = scale(kcal.per.g), 
           Fat = scale(perc.fat), 
           L = scale(perc.lactose))
```

Start by building two bivariate regressions. First with $F$ (fat percentage in milk) as the predictor:

$$
\begin{eqnarray}
K_i & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta_F F_i \\
\alpha & \sim & \mathcal{N}(0, 0.2) \\
\beta_F & \sim & \mathcal{N}(0, 0.5) \\
\sigma & \sim & \text{Exponential}(1)
\end{eqnarray}
$$

Then with $L$, lactose percentage:

$$
\begin{eqnarray}
K_i & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta_L L_i \\
\alpha & \sim & \mathcal{N}(0, 0.2) \\
\beta_L & \sim & \mathcal{N}(0, 0.5) \\
\sigma & \sim & \text{Exponential}(1)
\end{eqnarray}
$$

```{r}
b6_3 <- brm(
    K ~ 1 + Fat,
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 0.5), class = b),
              prior(exponential(1), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_3"
)
b6_4 <- brm(
    K ~ 1 + L,
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 0.5), class = b),
              prior(exponential(1), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_4"
)
```


```{r}
print(b6_3, digits = 3)
```


```{r}
list(b6_3, b6_4) %>% 
    map(~ posterior_summary(.x, 
                            , probs = c(0.055, 0.945)) %>% 
            round(digits = 2))
```

The posterior distributions for $\beta_F$ and $\beta_L$ are near-identical (up to sign). However this will change if we include both in the model.

```{r}
b6_5 <- brm(
    K ~ 1 + Fat + L,
    data = d, 
    prior = c(prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 0.5), class = b),
              prior(exponential(1), class = sigma)),
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_5"
)
posterior_summary(b6_5, probs = c(0.055, 0.945)) %>% 
    round(digits = 2)
```

The parameter estimates have shrunk towards zero and the error is much wider. Easiest to see in plots.

```{r}
list(b6_3, b6_4, b6_5) %>% 
    map(~ stanplot(.x, 
                   prob_outer = 0.89) +  
            theme(text = element_text(family = "Avenir")))
```

This is a less stark example than the leg-length: because `Fat` and `L` are so tightly correlated their parameter estimates are much more uncertain, because the posterior distribution describes a much wider range for those parameters that are plausible. Can see this easily in a pairs plot.

```{r}
pairs(~ kcal.per.g + perc.fat + perc.lactose, data = d, col = "steelblue")
```

`perc.fat` and `perc.lactose` are so tightly correlated that each renders the other near-redundant. As it says in the book:

> Either helps in predicting kcal.per.g, but neither helps much _once you already know the other_.

How strongly-correlated are they?

```{r}
d %>% 
    select(perc.fat, perc.lactose) %>% 
    cor()
```

(The rule of thumb I've encountered when doing frequentist modelling is that if $|\rho| > 0.8$ then one of the predictors needs to go.)

### Simulating Multicollinearity

```{r}
sim_coll <- function(seed, rho) {
    set.seed(seed)
    tmp <- d %>% 
        mutate(x = rnorm(n(), 
                         mean = perc.fat * rho, 
                         sd = sqrt((1 - rho^2) * var(perc.fat))))
    m <- lm(kcal.per.g ~ perc.fat + x, data = tmp)
    sqrt(diag(vcov(m)))[2]
}
n_seed <- 100
n_rho <- 30
d2 <- tibble(seed = seq_len(n_seed)) %>% 
    expand(seed, rho = seq(from = 0, 
                           to = 0.99, 
                           length.out = n_rho)) %>% 
    mutate(parameter_sd = map2_dbl(seed, 
                                   rho, 
                                   sim_coll)) %>% 
    group_by(rho) %>% 
    summarise(mean = mean(parameter_sd), 
              ll = quantile(parameter_sd, 0.055), 
              ul = quantile(parameter_sd, 0.945))
```

Now visualise how the standard deviation changes as $\rho$ increases.

```{r}
d2 %>% 
    ggplot(aes(rho, mean)) + 
    geom_smooth(aes(ymin = ll, ymax = ul), 
                stat = "identity", 
                fill = "orange", 
                colour = "orange", 
                alpha = .3, 
                size = 0.7) + 
    labs(x = expression(rho), 
         y = "parameter SD") + 
    coord_cartesian(ylim = c(0, .0072))
```


## Post-Treatment Bias

> … mistaken inferences arising from including variables that are consequences of other variables.

Illustrated through simulation.

```{r}
set.seed(71)
# number of plants
N <- 100
d <- tibble(
    h0 = rnorm(N, 10, 2), 
    treatment = rep(0:1, each = N/2)
) %>% 
    mutate(fungus = rbinom(N, size = 1, prob = 0.5 - (treatment * 0.4))) %>% 
    mutate(h1 = h0 + rnorm(N, mean = 5 - (3 * fungus))) %>% 
    select(h0, h1, treatment, fungus)
d %>% 
    gather() %>% 
    group_by(key) %>% 
    mean_qi(.width = 0.89) %>% 
    mutate_if(is.double, round, digits = 2)
```

Now need to set up a model with a sensible prior. Let the height of the $i$th plant at time $t$ be $h_{t,i}$, where $t \in \{0, 1\}$. Then the model is:

$$
\begin{eqnarray}
h_{1,i} & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & h_{0,i} \times p
\end{eqnarray}
$$

Where:

$$
p = \frac{h_{1,i}}{h_{0,i}}
$$

To allow for the possibility of the plants shrinking but to keep $p > 0$ use a log-normal prior. So the priors would be:

$$
\begin{eqnarray}
\log(p) & \sim & \mathcal{N}(0, 0.25) \\
\sigma & \sim & \text{Exponential}(1)
\end{eqnarray}
$$

Can simulate from this to ensure that the results are sensible.

```{r}
sim_p <- tibble(
    p = rlnorm(1e4, 0, 0.25)
)
sim_p %>% 
    gather() %>% 
    group_by(key) %>% 
    mean_qi(.width = 0.89) %>% 
    mutate_if(is.double, round, digits = 2)
```

```{r}
b6_6 <- brm(
    h1 ~ 0 + h0,
    data = d, 
    prior = c(prior(lognormal(0, 0.25), class = b), 
              prior(exponential(1), class = sigma)), 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_6"
)
posterior_summary(b6_6) %>% 
    round(digits = 2)
```

Now change the model to account for `treatment` and `fungus`.

$$
\begin{eqnarray}
h_{1,i} & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & h_{0,i} \times p \\
p & = & \alpha + \beta_T T_i + \beta_F F_i \\
\log(\alpha) & \sim & \mathcal{N}(0, 0.25) \\
\beta_T & \sim & \mathcal{N}(0, 0.5) \\
\beta_F & \sim & \mathcal{N}(0, 0.5) \\
\sigma & \sim & \text{Exponential}(1)
\end{eqnarray}
$$

So $p$ is now itself a linear combination of other parameters. To do this in `brms` needs the nonlinear syntax, which we can justify by substituting the model specification for $p$.

$$
\begin{eqnarray}
p & = & \alpha + \beta_T T_i + \beta_F F_i \\
\mu_i & = & h_{0,i} \times p \\

& = & h_{0,i} \times (\alpha + \beta_T T_i + \beta_F F_i) \\

\end{eqnarray}
$$

So that the model will be:

```{r}
b6_7 <- brm(
    bf(h1 ~ h0 * (a + t * treatment + f * fungus), 
       a + t + f ~ 1, 
       nl = TRUE), 
    data = d, 
    family = "gaussian", 
    prior = c(prior(lognormal(0, 0.2), nlpar = a), 
              prior(normal(0, 0.5), nlpar = t), 
              prior(normal(0, 0.5), nlpar = f), 
              prior(exponential(1), class = sigma)), 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_7"
)
posterior_summary(b6_7) %>% 
    round(digits = 2)
```

The results suggest that the treatment has no effect, whereas the fungus has some negative effect on growth. However the simulation was set up so that the treatment _does_ matter, so something has gone wrong.

Specifically: because `fungus` is largely a consequence of `treatment`. Returning to the question from earlier:

> Once we already know whether or not a plant developed fungus, does soil treatment matter?

Since the answer is no, the parameter estimate for treatment is basically zero. Try remodelling without `fungus`. The model specification becomes:

$$
\begin{eqnarray}
h_{1,i} & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & h_{0,i} \times (\alpha + \beta_T T_i) \\
\log(\alpha) & \sim & \mathcal{N}(0, 0.25) \\
\beta_T & \sim & \mathcal{N}(0, 0.5) \\
\sigma & \sim & \text{Exponential}(1) \\
\end{eqnarray}
$$

Now fit the model.

```{r}
b6_8 <- brm(
    bf(h1 ~ h0 * (a + t * treatment), 
       a + t ~ 1, 
       nl = TRUE), 
    data = d, 
    family = "gaussian", 
    prior = c(prior(lognormal(0, 0.2), nlpar = a), 
              prior(normal(0, 0.5), nlpar = t), 
              prior(exponential(1), class = sigma)), 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_8"
)
posterior_summary(b6_8) %>% 
    round(digits = 2)

stanplot(b6_8, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

Can also represent what's going on in a DAG.

```{r}
fungus_dag <- dagify(F ~ T,
                     H1 ~ H0 + F,
                     coords = tibble(
                         name = c("H0", "T", "F", "H1"),
                         x    = c(1, 5, 4, 3),
                         y    = c(2, 2, 1.5, 1))
)

fungus_dag %>% 
    tidy_dagitty() %>% 
    ggdag()
```

The phrase from the book is that 

> … conditioning on F induces d-separation …

where the d stands for dependence. The `dagitty` package can analyse a DAG for d-separation.

```{r}
dseparated(fungus_dag, "T", "H1")
dseparated(fungus_dag, "T", "H1", "F")
```

And it can do something more automatic with the whole DAG and no variables specified.

```{r}
impliedConditionalIndependencies(fungus_dag)
```

The `_||_` means independent of, so this confirms what the modelling steps showed: that the outcome is independent of the treatment conditional on fungus.

Two really important ideas in the book re. post-treatment effects:

1. This sort of effect can occur in both experimental and observational data, but is much harder to identify in observational.
2. Using model selection (i.e. with information criteria such as WAIC) would not help here since the model with fungus would get better results and make better out-of-sample predictions. The problem is more fundamental: _the model is asking the wrong question_.

## Collider Bias

Return to the publication example, which we can show with a DAG.

```{r}
publishing_dag <- dagify(
    S ~ T + N, 
    coords = tibble(name = c("T", "S", "N"),
                    x    = c(1, 2, 3),
                    y    = c(0, 0, 0))
)
publishing_dag %>% 
    tidy_dagitty() %>% 
    ggdag()
```

S is a collider, which we can spot because of the two arrows pointing into it. When conditioning on S (as we did when plotting the linear model curve on the black and orange plot) we create a connection between T and N. Independently they are unconnected, but conditional on being selected an article with low trustworthiness has higher newsworthiness (and vice versa).

Next example is about sadness and age. The assumptions for the model are:

- the happier people are more likely they are to get married;
- the older people are the more likely they are to have gotten married (just because they've had more chances to meet Mr./Mrs. Right).

The DAG that describes this causal model is:

```{r}
marriage_dag <- dagify(
    M ~ A + H, 
    coords = tibble(name = c("A", "M", "H"),
                    x    = c(1, 2, 3),
                    y    = c(0, 0, 0))
)
marriage_dag %>% 
    tidy_dagitty() %>% 
    ggdag()
```

Now run a simulation for this model using McElreath's convenience function.

```{r}
d <- rethinking::sim_happiness() %>% 
    as_tibble()
glimpse(d)
```

```{r}
d %>% 
    mutate(married = factor(married,
                            labels = c("unmarried", "married"))) %>% 
    ggplot(aes(age, happiness, fill = married)) + 
    geom_point(shape = 21) + 
    scale_fill_manual(NULL, values = c("white", "steelblue")) + 
    labs(x = "Age", y = "Happiness") + 
    theme(panel.grid = element_blank(), 
          legend.position = "top")
```

The pattern is much the same as for the publications. Now we could try building a model that regresses happiness on age, conditioning on marriage. Such a model would be:

$$
\begin{eqnarray}
H_{i} & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha_{MID[i]} + \beta_A A_i \\
\end{eqnarray}
$$

This sets up separate intercepts for those who are married and unmarried. $A$ is just the person's age.

Need to consider the priors also. The scale of $A$ will affect the interpretation of the intercept. Can rescale so that only adults are considered and that the period from 18 to 65 is one unit. The happiness is bounded on $[-2, 2]$, so the strongest possible relationship (i.e. the max of $\beta_A$) is 4. So setting the standard deviation to half of that will keep the majority of the prior in a sensible range. The prior for the intercepts can be a standard normal.

So the full model specification would be:

$$
\begin{eqnarray}
H_{i} & \sim & \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = & \alpha_{MID[i]} + \beta_A A_i \\
\alpha_i & \sim & \mathcal{N}(0, 1) \\
\beta_A & \sim & \mathcal{N}(0, 2) \\
\sigma & \sim & \text{Exponential}(1)
\end{eqnarray}
$$

Can now add the rescaled age variable and the marriage index.

```{r}
d2 <- d %>% 
    filter(age >= 18) %>% 
    mutate(A = (age - 18)/(65 - 18)) %>% 
    mutate(mid = factor(married + 1, 
                        labels = c("single", "married")))
head(d2)
```

And then build the model.

```{r}
b6_9 <- brm(
    happiness ~ 0 + A + mid, 
    prior = c(prior(normal(0, 1), class = b, coef = midmarried), 
              prior(normal(0, 1), class = b, coef = midsingle), 
              prior(normal(0, 2), class = b, coef = A), 
              prior(exponential(1), class = sigma)), 
    data = d2, 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_9"
)

posterior_summary(b6_9) %>% 
    round(digits = 2)

stanplot(b6_9, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

The model concludes a strong negative association between age and happiness, but we know this is false because we simulated the data. Now build a model without marriage status.

```{r}
b6_10 <- brm(
    happiness ~ 0 + intercept + A, 
    prior = c(prior(normal(0, 1), class = b, coef = intercept), 
              prior(normal(0, 2), class = b, coef = A), 
              prior(exponential(1), class = sigma)), 
    data = d2, 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_10"
)

posterior_summary(b6_10) %>% 
    round(digits = 2)

stanplot(b6_10, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

Now the estimate for $\beta_A$ is centered around zero, as it should be. 

This illustrates what can happen when conditioning on a collider: it introduces a spurious association between the causes.

### The Haunted DAG

In that example we could pin down the exact causal relationship because the data were simulated, but that's not typical.

Another possible DAG would model the causal relationship between educational achievement of grandparents ($G$), parents ($P$), and children ($C$).

```{r}
dagify(C ~ P + G, 
       P ~ G, 
       coords = tibble(
           name = c("G", "P", "C"), 
           x = c(1, 2, 2), 
           y = c(2, 2, 1)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

But suppose there is some unobserved variable that is affecting the observed ones, such as the effect of living in a good or bad neighbourhood.

```{r}
dagify(C ~ P + G + U, 
       P ~ G + U, 
       coords = tibble(
           name = c("G", "P", "C", "U"), 
           x = c(1, 2, 2, 2.5), 
           y = c(2, 2, 1, 1.5)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

From the book:

> Now P is a common consequence of G and U, so if we condition on P, it will bias inference about G > C, _even if we never get to measure U_.

Show this with simulation.

```{r}
N <- 200  # number of grandparent-parent-child triads
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U <- 2 #directeffectofUonPandC
set.seed(1)
d <- tibble(
    U = 2 * rbernoulli(N, 0.5) - 1, 
    G = rnorm(N)
) %>% 
    mutate(
        P = rnorm(N, (b_GP * G) + (b_U * U)), 
        C = rnorm(N, (b_PC * P) + (b_GC * G) + (b_U * U))
    )
head(d)
```

To model $C$ we can see that we need to condition on $P$. But we are unaware of $U$, even though it's a common cause of $C$ and $P$. Show this by modelling first without $U$ (which is all that would be possible in the case of real observational data).

```{r}
b6_11 <- brm(
    C ~ 1 + G + P, 
    prior = c(prior(normal(0, 1), class = b), 
              prior(normal(0, 1), class = Intercept), 
              prior(exponential(1), class = sigma)), 
    data = d, 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_11"
)

posterior_summary(b6_11) %>% 
    round(digits = 2)

stanplot(b6_11, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

Easiest to visualise with a scatterplot of the full data.

```{r}
d2 <- d %>% 
    mutate_at(vars(C, G, P), scale) %>% 
    mutate(U = factor(if_else(U == 1, 
                              "good", 
                              "bad")), 
           central = factor(P >= quantile(P, 0.45) & 
                                P <= quantile(P, 0.6)))

d2 %>% 
    ggplot(aes(G, C, colour = U)) + 
    geom_point(data = d2 %>% 
                   filter(central == "TRUE")) + 
    scale_colour_manual(values = c("black", "black", 
                                   "orange", "orange")) + 
    geom_point(data = d2 %>% 
                   filter(central == "FALSE"), 
               shape = 21) + 
    geom_text(data = tibble(
        G = c(0.2, -1.5), 
        C = c(-2, 2), 
        U = c("FALSE", "TRUE"), 
        label = c("bad neighbourhood", 
                  "good neighbourhood")),
        aes(label = label)) + 
    geom_smooth(data = d2 %>% filter(central == TRUE), 
                aes(G, C), 
                method = "lm", 
                inherit.aes = FALSE, 
                fullrange = TRUE, 
                fill = "black", 
                colour = "black", 
                alpha = 0.25, 
                size = 0.2) + 
    labs(x = "grandparent education (G)", 
         y = "grandchild education (C)", 
         colour = NULL, 
         fill = NULL, 
         title = "Grandchild vs. Grandparent Education", 
         subtitle = "Parents in 45th to 60th centiles are filled circles") + 
    theme(legend.position = "none")
```

The filled points are those in the centre of the distribution for $P$: this just makes the effect easier to see. Conditional on knowing $P$, the effect of $G$ is to provide information about $U$. And for a given value of $P$ then the higher $G$ corresponds with a higher probability of living in a bad neighbourhood, and is therefore negatively associated with $C$. The only solution here is to measure $U$ somehow, as below.

```{r}
b6_12 <- brm(
    C ~ 1 + G + P + U, 
    prior = c(prior(normal(0, 1), class = b), 
              prior(normal(0, 1), class = Intercept), 
              prior(exponential(1), class = sigma)), 
    data = d, 
    iter = 2000, 
    warmup = 500, 
    chains = 4, 
    cores = 4, 
    file = "Stan/ch_06/b6_12"
)

posterior_summary(b6_12) %>% 
    round(digits = 2)

stanplot(b6_12, 
         prob_outer = 0.89) + 
    theme(text = element_text(family = "Avenir"))
```

Now the model has generated the correct estimates for the simulated data.

## Confronting Confounding

McElreath discusses the issues with confounding, and how to address it.

Confounding arises when information 'leaks' into the model, such that the association between predictor $X$ and outcome $Y$ is not the same as it would have been in a controlled experiment. Four types and the methods for shutting these backdoors are discussed in the book: in all of these $X$ is the predictor and $Y$ the outcome.

### Shutting Backdoors

#### Fork

```{r}
dagify(X ~ Z, 
       Y ~ Z, 
       coords = tibble(
           name = c("X", "Y", "Z"), 
           x = c(1, 3, 2), 
           y = c(1, 1, 1)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

In this case $X \perp Y  | Z$, so if $Z$ is available it can block the path from $X$ to $Y$.

#### Pipe

```{r}
dagify(Z ~ X, 
       Y ~ Z, 
       coords = tibble(
           name = c("X", "Y", "Z"), 
           x = c(1, 3, 2), 
           y = c(1, 1, 1)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

This was the fungus case: the treatment $X$ influences fungus $Z$, which affects growth $Y$. Again conditioning on $Z$ can block the path, but this may not be desirable (particularly when we want to know about the association between $X$ and $Y$, as in the fungus example).

#### Collider

```{r}
dagify(Z ~ X + Y, 
       coords = tibble(
           name = c("X", "Y", "Z"), 
           x = c(1, 3, 2), 
           y = c(1, 1, 1)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

Same as the fork but with arrows reversed: now $Z$ is caused by both the others. There will be no association between $X$ and $Y$ _unless_ we condition on $Z$. 

#### Descendent

```{r}
dagify(Z ~ X, 
       Y ~ Z, 
       K ~ Z, 
       coords = tibble(
           name = c("X", "Y", "Z", "K"), 
           x = c(1, 3, 2, 2), 
           y = c(1, 1, 1, 0)
       )) %>% 
    tidy_dagitty() %>% 
    ggdag()
```

$K$ is a descendent of $Z$. Conditioning on a variable's descedent is a weaker form of conditioning on the variable itself. In this case there is a pipe $X \rightarrow Z \rightarrow Y$, so ideally we would condition on $Z$. But conditioning on $K$ would offer some approximation to that, which might be necessary (particularly with observational data).

### Two roads

Can analyse DAGs with the `daggity` package. Consider this DAG.

```{r}
dag6_1 <- dagify(
    Y ~ X + C, 
    X ~ U, 
    B ~ U, 
    U ~ A, 
    C ~ A, 
    B ~ U + C, 
    coords = tibble(
        name = c("A", "B", "C", "U", "X", "Y"), 
        x = c(2, 2, 3, 1, 1, 3), 
        y = c(2.5, 1.5, 2, 2, 1, 1)
    )
)
dag6_1 %>% 
    tidy_dagitty() %>% 
    ggdag()
```

In this case want to make some inference about $Y \sim X$. There are two paths available, and for both need to consider whether it is already open (in which case it must be closed), or is closed (and must not be opened).

On this graph the path through $A$ is open, and that with $B$ is closed. Paths with colliders are closed, whereas pipes and forks are open.

`dagitty` will analyse this directly.

```{r}
adjustmentSets(x = dag6_1, 
               exposure = "X", 
               outcome = "Y")
```

If $U$ is unobserved then the choice would be to condition on $A$ and $C$.

### Backdoor Waffles

An example with the Waffle House data. We want the total causal effect of Waffle Houses on divorce rate. A DAG for this might be:

```{r}
waffle_dag <- dagify(
    A ~ S, 
    D ~ A + M + W, 
    M ~ A + S, 
    W ~ S, 
    coords = tibble(
        name = c("A", "D", "M", "S", "W"), 
        x = c(1, 3, 2, 1, 3), 
        y = c(1, 1, 1.5, 2, 2)
    )
)

waffle_dag %>% 
    tidy_dagitty() %>% 
    ggdag() + 
    theme_void() + 
    labs(caption = paste0("S: Southern US; A: median age of marriage; ", 
                          "M: marriage rate\nW: number of Waffle Houses; ", 
                          "D: divorce rate"))
```

Now get `dagitty` to confirm the right variable on which to condition.

```{r}
adjustmentSets(waffle_dag, 
               exposure = "W", 
               outcome = "D")
```

The simplest approach would be to condition on just $S$. 

Can also check for the implied conditional independences in the DAG.

```{r}
impliedConditionalIndependencies(waffle_dag)
```


[BDA_R]: https://github.com/avehtari/BDA_R_demos
[SKurtz_SR_2ed]: https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/blob/master/06.Rmd