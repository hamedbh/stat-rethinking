# Ulysses' Compass

```{r}
#| label: setup
#| output: false
library(rethinking)
library(patchwork)
library(zeallot)
library(dagitty)
library(ggdag)
library(ggrepel)
library(magrittr)
library(tidyverse)
# set up the theme
theme_set(
  theme_light() + 
    theme(panel.grid = element_blank())
)
walk(list.files(here::here("R"), full.names = TRUE), source)
```

At the start of the chapter we get a key point from RM: 

> When we design any particular statistical model, we must decide whether we want to understand causes or rather just predict. These are not the same goal, and different models are needed for each. 

Two main approaches for navigating underfitting and overfitting: 

1. **Regularising priors** tell the method "not to get too excited by the data"; 
2. **Information criteria** or **cross-validation** to estimate predictive accuracy out-of-sample. 

## The problem with parameters

### More parameters (almost) always improve fit

```{r}
sppnames <- c(
  "afarensis", 
  "africanus", 
  "habilis", 
  "boisei", 
  "rudolfensis", 
  "ergaster", 
  "sapiens"
)
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37, 35.5, 34.5, 41.5, 55.5, 61, 53.5)
dbrain <- tibble(species = sppnames, brain = brainvolcc, mass = masskg)
dbrain
```

```{r}
dbrain |> 
  ggplot(aes(mass, brain)) + 
  geom_point(alpha = 0.6, colour = "steelblue") + 
  geom_text_repel(aes(label = species)) + 
  coord_cartesian() +
  labs(x = "body mass (kg)", y = "brain volume (cc)")
```

Now we fit a bunch of models, becoming increasingly complex and useless. Need to rescale the data: `brain` is handled differently than `mass` because we need to preserve 0 as a reference point (i.e. no such thing as negative brain). 

```{r}
dbrain_std <- dbrain |> 
  mutate(mass_std = standardize(mass), brain_std = brain / max(brain))
```

Set up the linear model for brain mass as a function of body mass. 

$$
\begin{align}
b_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta m_i \\
\alpha &\sim \mathcal{N}(0.5, 1) \\ 
\beta &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \text{Lognormal}(0, 1)
\end{align}
$$

This is not a good model for any number of reasons. But the weakness of the priors is part of the lesson anyway ...

```{r}
m7_1 <- quap(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)), 
    mu <- a + (b * mass_std), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), 
  data = dbrain_std
)
```

RM then discussed $R^2$ as a statistical measure, and its weakness. Quite simply it's: 

$$
\begin{align}
R^2 &= \frac{\text{Var}(\text{outcome}) - \text{Var}(\text{residuals})}
  {\text{Var}(\text{outcome})} \\
  
  &= 1 - \frac{\text{Var}(\text{residuals})}{\text{Var}(\text{outcome})}
\end{align}
$$

$R^2$ will always go up when we add more predictors, even if they are just noise. So if we measure it in-sample it tells us little/nothing. 

```{r}
set.seed(12)
# Create the helper function straight away
R2_is_bad <- function(quap_fit) {
  v <- var2(dbrain_std[["brain_std"]])
  r <- quap_fit |> 
    sim(refresh = 0) |> 
    colMeans() |> 
    {\(x) x - dbrain_std[["brain_std"]]}() |> 
    var2()
  
  1 - (r / v)
}
# Compute for first model
R2_is_bad(m7_1)
```

Now we build up those silly models with higher-order polynomial terms. Starting with squares. 

$$
\begin{align}
b_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 m_i + \beta_2 m^2_i\\
\alpha &\sim \mathcal{N}(0.5, 1) \\ 
\beta_j &\sim \mathcal{N}(0, 10) \: \text{for} \: j \in \{1, 2\} \\
\sigma &\sim \text{Lognormal}(0, 1)
\end{align}
$$

```{r}
m7_2 <- quap(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)), 
    mu <- a + (b[1] * mass_std) + (b[2] * mass_std^2), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), 
  data = dbrain_std, 
  start = list(b = rep(0, 2))
)
```

Now we build the remaining models, with increasing levels of nonsense. 

```{r}
m7_3 <- quap(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)), 
    mu <- a + (b[1] * mass_std) + (b[2] * mass_std^2) + (b[3] * mass_std^3), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), 
  data = dbrain_std, 
  start = list(b = rep(0, 3))
)

m7_4 <- quap(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)), 
    mu <- a + (b[1] * mass_std) + (b[2] * mass_std^2) + (b[3] * mass_std^3) + 
      (b[4] * mass_std^4), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), 
  data = dbrain_std, 
  start = list(b = rep(0, 4))
)

m7_5 <- quap(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)), 
    mu <- a + (b[1] * mass_std) + (b[2] * mass_std^2) + (b[3] * mass_std^3) + 
      (b[4] * mass_std^4) + (b[5] * mass_std^5), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), 
  data = dbrain_std, 
  start = list(b = rep(0, 5))
)

# Note that we have to set sigma to a fixed value for this
m7_6 <- quap(
  alist(
    brain_std ~ dnorm(mu, 0.001), 
    mu <- a + (b[1] * mass_std) + (b[2] * mass_std^2) + (b[3] * mass_std^3) + 
      (b[4] * mass_std^4) + (b[5] * mass_std^5) + (b[6] * mass_std^6), 
    a ~ dnorm(0.5, 1), 
    b ~ dnorm(0, 10)
  ), 
  data = dbrain_std, 
  start = list(b = rep(0, 6))
)
```

Now we can generate plots. 

```{r}
plot_R2_is_bad <- function(quap_fit_chr, poly_order) {
  mass_seq <- seq(
    min(dbrain_std[["mass_std"]]), 
    max(dbrain_std[["mass_std"]]), 
    length.out = 100
  )
  quap_fit <- quap_fit_chr |> 
    get()
  R2 <- R2_is_bad(quap_fit) |> 
    round(digits = 2)
  quap_fit |> 
    link(data = list(mass_std = mass_seq)) |> 
    as_tibble(.name_repair = ~ str_c(mass_seq)) |> 
    rowid_to_column(".id") |> 
    pivot_longer(
      -.id, 
      names_to = "mass_std", 
      names_transform = list(mass_std = parse_number)
    ) |> 
    reframe_mean_PI(mass_std) |> 
    pivot_wider() |> 
    ggplot(aes(x = mass_std)) + 
    geom_line(aes(y = mean)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) + 
    geom_point(aes(y = brain_std), data = dbrain_std, colour = "steelblue") + 
    labs(
      title = sprintf("%s: R^2 = %s", quap_fit_chr, R2), 
      x = "body mass (std)", 
      y = "brain volume (std)"
    )
}

plot_R2_is_bad("m7_1") +
  plot_R2_is_bad("m7_2") +
  plot_R2_is_bad("m7_3") +
  plot_R2_is_bad("m7_4") +
  plot_R2_is_bad("m7_5") +
  (
    plot_R2_is_bad("m7_6") +
      geom_hline(yintercept = 0, linetype = 2)
  )
```

The $R^2$ improves to a seemingly-perfect fit on the last model, but it is clearly junk. For example: brain volume goes negative towards the right of the plot. And yet the CI around the MAP line has collapsed to nothing: our model is certain that this is the only possible line compatible with the data. 

This model is simply a recoding of the data. We've projected our data from its original space into a polynomial basis, so that it is being recreated exactly. This doesn't make it at all useful. 

### Too few parameters hurts, too

RM gives a neat description of underfit models as being "insensitive to the sample". We can recreate RM's leave-one-out plots for models 1 and 4 below to illustrate the difference. 

```{r}
plot_brain_loo <- function(quap_fit) {
  model_name <- deparse(match.call()[[2]])
  mass_seq <- seq(
    min(dbrain_std[["mass_std"]]), 
    max(dbrain_std[["mass_std"]]), 
    length.out = 100
  )
  tibble(i = seq_len(nrow(dbrain_std))) |> 
    mutate(d = map(i, ~ dbrain_std[-.x, ])) |> 
    mutate(
      m_tmp = map(
        d, 
        ~ quap(
          quap_fit@formula, 
          data = .x, 
          start = list(b = rep(0, as.integer(str_sub(model_name, 4L, 4L))))
        )
      )
    ) |> 
    mutate(
      l = map(
        m_tmp, 
        ~ link(.x, data = list(mass_std = mass_seq), refresh = 0) |> 
          as_tibble(.name_repair = ~ str_c(mass_seq)) |> 
          pivot_longer(
            everything(), 
            names_to = "mass_std", 
            names_transform = list(mass_std = parse_number)
          ) |> 
          group_by(mass_std) |> 
          summarise(mu = mean(value), .groups = "drop")
      )
    ) |> 
    select(i, l) |> 
    unnest(l) |> 
    ggplot(aes(x = mass_std)) + 
    geom_line(aes(y = mu, group = i), colour = "grey30", alpha = 0.4) + 
    geom_point(
      aes(y = brain_std), 
      data = dbrain_std, 
      colour = "steelblue"
    ) + 
    geom_text_repel(
      aes(y = brain_std, label = species), 
      data = dbrain_std |> 
        filter(species == "sapiens")
    ) + 
    labs(
      title = model_name, 
      x = "body mass (std)", 
      y = "brain volume (std)"
    )
}
plot_brain_loo(m7_1) + 
  plot_brain_loo(m7_4)
```

The lines on the left-hand plot (i.e. with an underfit model) are all very similar: the one that is much lower is the model that drops humans from the sample. The lines in the right-hand plot vary wildly depending on which observation is dropped. 

## Entropy and accuracy

RM outlines the process for measuring the trade-off between underfitting and overfitting in a rigorous way, using information theory. 

1. Set a measurement scale for distance from perfect accuracy; 
2. Approximate distance from perfect prediction with _deviance_; 
3. Focus only on out-of-sample deviance. 

### Firing the weatherperson

Need to set the target, so that we can measure deviance effectively. 

RM uses the example of a weatherperson who simply predicts whether or not it will rain. The established weatherperson predicts rain as follows: 

```{r}
wperson1_preds <- tibble(day = seq_len(10), pred = c(rep(1, 3), rep(0.6, 7)))
wperson1_preds
```

Our observed rain is: 

```{r}
(observed <- tibble(rain = c(rep(1L, 3), rep(0L, 7))))
```

Then a new weatherperson turns up and announces that they perform better based on _hit rate_, which RM defines as the "average chance of a correct prediction". 

```{r}
(wperson2_preds <- tibble(day = seq_len(10), pred = rep(0, 10)))
```

We can calculate the hit rate for each. 

```{r}
map_chr(
  1:2, 
  ~ sprintf(
    "Weatherperson %s hit rate: %s", 
    .x, 
    sprintf("wperson%s_preds", .x) |> 
      get() |> 
      bind_cols(observed) |> 
      summarise(
        hit_rate = sum(((rain == 1L) * pred) + ((rain == 0L) * (1 - pred)))
      ) |> 
      pull(hit_rate)
  )
) |> 
  reduce(~ str_c(.x, .y, sep = "\n")) |> 
  cat()
```

#### Costs and benefits

If we change the standards by which we judge the forecasts then we get very different results. The costs of carrying an umbrella is deemed to be 1, and the cost of being caught without one when it rains is 5. Now recalculate the scores for each. 

```{r}
map_chr(
  1:2, 
  ~ sprintf(
    "Weatherperson %s total cost: %s", 
    .x, 
    sprintf("wperson%s_preds", .x) |> 
      get() |> 
      bind_cols(observed) |> 
      summarise(cost = sum((5 * (1 - pred) * (rain == 1L)) + (1 * pred))) |> 
      pull(cost)
  )
) |> 
  reduce(~ str_c(.x, .y, sep = "\n")) |>
  cat()
```

Now the first weatherperson comes out better: the cost of carrying a brolly is small compared to that of getting wet. We can play with the costs a bit as RM suggests. 

Try setting cost of getting wet as 3, cost of carrying a brolly as 2. 

```{r}
map_chr(
  1:2, 
  ~ sprintf(
    "Weatherperson %s total cost: %s", 
    .x, 
    sprintf("wperson%s_preds", .x) |> 
      get() |> 
      bind_cols(observed) |> 
      summarise(cost = sum((3 * (1 - pred) * (rain == 1L)) + (2 * pred))) |> 
      pull(cost)
  )
) |> 
  reduce(~ str_c(.x, .y, sep = "\n")) |>
  cat()
```

We can try iterating over a number of values to see how it varies. 

```{r}
inner_join(
  wperson1_preds |> rename(pred1 = pred),
  wperson2_preds |> rename(pred2 = pred),
  by = "day"
) |> 
  bind_cols(observed) |> 
  pivot_longer(
    starts_with("pred"),
    names_prefix = "pred",
    names_to = "wperson",
    values_to = "pred"
  ) |> 
  expand(
    nesting(day, rain, wperson, pred),
    W = seq(0, 6, by = 0.2), 
    U = seq(0, 6, by = 0.2)
  ) |> 
  mutate(
    cost = (W * (1 - pred) * (rain == 1L)) + (U * pred)
  ) |> 
  group_by(wperson, W, U) |> 
  summarise(
    cost = sum(cost),
    .groups = "drop"
  ) |> 
  pivot_wider(
    names_from = wperson,
    names_prefix = "wperson",
    values_from = cost
  ) |> 
  mutate(which_better = if_else(wperson1 - wperson2 < 0, "one", "two")) |> 
  ggplot(aes(W, U, fill = which_better)) + 
  geom_raster() + 
  scale_fill_manual(values = c("steelblue", "firebrick")) + 
  labs(
    x = "Cost of getting wet", 
    y = "Cost of carrying an umbrella", 
    fill = NULL
  ) + 
  theme(legend.position = "bottom")
```

We can try something like log loss to see how each fares. 

```{r}
obs_fctr <- observed |> 
  mutate(
    rain = if_else(rain == 1L, "rain", "dry") |> 
      factor(levels = c("rain", "dry"))
  )

map_chr(
  1:2, 
  ~ sprintf(
    "Weatherperson %s log loss: %s", 
    .x, 
    sprintf("wperson%s_preds", .x) |> 
      get() |> 
      bind_cols(obs_fctr) |> 
      yardstick::mn_log_loss(truth = rain, pred) |> 
      pull(.estimate) |> 
      round(digits = 3)
  )
) |> 
  reduce(~ str_c(.x, .y, sep = "\n")) |>
  cat()
```

#### Measuring accuracy

RM then distinguishes between the average probability of being correct (which was the hit rate used above) and the joint probability of getting the sequence correct. We can calculate this for each of the forecasters. 

```{r}
map_chr(
  1:2, 
  ~ sprintf(
    "Weatherperson %s joint probability: %s", 
    .x, 
    sprintf("wperson%s_preds", .x) |> 
      get() |> 
      bind_cols(observed) |> 
      summarise(
        joint_prob = prod(((rain == 1L) * pred) + ((rain == 0L) * (1 - pred)))
      ) |> 
      pull(joint_prob) |> 
      round(digits = 3)
  )
) |> 
  reduce(~ str_c(.x, .y, sep = "\n")) |>
  cat()
```

### Information and uncertainty

RM uses a nice metaphor for the idea of accounting for uncertainty in our distance metric: the problem of hitting a bullseye becomes much harder if the archer also has to hit the bullseye at the right time. Adding an extra dimension makes the problem harder. The same would be true for the weather forecasters in the last problem if they had to forecast sun, rain, or snow. 

So then the basic insight is to ask: 

> How much is our uncertainty reduced by learning an outcome?

That reduction in uncertainty is the **information**, which is a measurable quantity. 

The measurement of uncertainty uses the following function (where there are $n$ possible events, and each event $i$ has probability $p_i$ of occurring): 

$$
\begin{align}
H(p) &= - \mathbb{E}(\log{p_i}) \\
     &= - \sum_{i = 1}^n p_i \log{p_i}
\end{align}
$$

In other words: calculate the mean log-probability and take its negative. Taking the negative makes it a non-negative quantity, so that 0 means no uncertainty. 

We also need to consider the situation when $p_i = 0$, in which case we cannot take its log. Then we need to work with its limit. 

$$
\begin{align}
\lim_{p \to 0} p \log{p} 
  &= \lim_{p \to 0} \frac{\log{p}}{p^{-1}} \\
  &= \lim_{p \to 0} \frac{p^{-1}}{-p^{-2}} \: \text{by L'Hopital's rule} \\
  &= \lim_{p \to 0} - p \\
  &= 0
\end{align}
$$

Now we can think about examples. Sticking with the weather forecasters: if the true probabilities of rain and sunshine are 0.3 and 0.7 then we can calculate the entropy: 

```{r}
- sum(c(0.3, 0.7) * log(c(0.3, 0.7)))
```

If the probabilities were instead 0.01 and 0.99 respectively then the entropy would be: 

```{r}
- sum(c(0.01, 0.99) * log(c(0.01, 0.99)))
```

This makes sense: there is much more uncertainty about the weather in the second example than the first. 

Entropy values are not especially intuitive though: 0.06 entropy means what exactly? We can improve this a little bit by changing to base-2 logs, so that the units are bits. 

```{r}
- sum(c(0.3, 0.7) * log2(c(0.3, 0.7)))
- sum(c(0.01, 0.99) * log2(c(0.01, 0.99)))
```

They are still not that useful on their own. 

### From entropy to accuracy

The next concept is **divergence**: by how far does one distribution diverge from another? Or more formally: 

> **Divergence**: the additional uncertainty induced by using probabilities from one distribution to describe another distribution. 

$$
\begin{align}
D_{KL}(p, q) &= \sum_i p_i (\log(p_i) - \log(q_i)) \\
             &= \sum_i p_i \log \bigg( \frac{p_i}{q_i} \bigg)
\end{align}
$$

If the two distributions are the same then the log term is zero and all summands are zero: no divergence. 

We can show how the divergence changes in a plot: our true target distribution $p = \{0.3, 0.7\}$. Now we can use different $q$ to approximate it, ranging from $q = \{0.01, 0.99\}$ to $q = \{0.99, 0.01\}$. 

```{r}
tibble(q1 = seq(0.01, 0.99, by = 0.01)) |> 
  mutate(q2 = 1 - q1) |> 
  mutate(
    kld = map2_dbl(q1, q2, ~ sum(c(0.3, 0.7) * log(c(0.3, 0.7) / c(.x, .y))))
  ) |> 
  ggplot(aes(q1, kld)) + 
  geom_line(colour = "steelblue") + 
  geom_vline(xintercept = 0.3, linetype = 2) + 
  geom_text(
    aes(label = label), 
    data = tibble(q1 = 0.35, kld = 1.5, label = "q = p")
  ) + 
  labs(
    x = expression(q[1]), 
    y = "Divergence of q from p"
  )
```

The KL divergence is at its minimum when $q = p$. 

RM also explains the concept of **cross-entropy**, which we can use to calculate KL divergence. 

The cross entropy $H(p, q) = -\sum_i p_i \log(q_i)$. In other words: the events arise according to $p$ but are predicted according to $q$. We can expect then that $H(p, q) \geq H(p)$. 

An important point is that KL divergence and cross-entropy are not symmetrical: in general $D_{KL}(p, q) \neq D_{KL}(q, p)$ and $H(p, q) \neq H(q, p)$.

### Estimating divergence

We now hit a problem, which is that in almost any modelling task we will not know the true probability distribution $p$: that is exactly what we are trying to discover!

However we are typically not trying to judge just one model, but a number of them. In the previous examples with under-/overfitting we wanted to compare a model with two parameters to another with three, etc. But we can see that if we are interested in the _difference_ between the divergences of those two models, we don't need to know $p$ at all. Suppose we have two models, $q$ and $r$, and we want to know which is 'better' with respect to KL divergence, i.e. $D_{KL}(p, q) - D_{KL}(p, r)$. Then: 

$$
\begin{align}
D_{KL}(p, q) - D_{KL}(p, r) 
  &= \big[ H(p, q) - H(p) \big] - \big[ H(p, r) - H(p) \big] \\
  &= H(p, q) - H(p, r) \\
  &= - \sum_i p_i \log(q_i) + \sum_i p_i \log(r_i) \\
  &= \sum_i p_i [ \log(r_i) - \log(q_i) ] \\
  &\propto \sum_i \log(r_i) - \log(q_i)
\end{align}
$$

This leads us to the equation for the **score** of a model $q$, $S(q)$: 

$$
S(q) = \sum_i \log (q_i)
$$

Of course for a Bayesian model we must use the entire posterior distribution to calculate the score, which leads to the **log pointwise predictive density**: 

$$
\text{lppd}(y, \Theta) = \sum_i \log \frac{1}{N} \sum_j p(y_i | \Theta_j)
$$

Here $N$ is the number of samples, and $\Theta_j$ is the j-th set of sampled parameter values in the posterior distribution. 

There is a function for this in {rethinking}: 

```{r}
set.seed(1)
lppd(m7_1, n = 1e4)
```

Summing those values gives the total log-probability score for the model. 

In practice we need to be careful when working with small values, as they may not be numerically stable. It is often best to work with logs. We can compute the log-prob score for `m7_1` manually as well. 

```{r}
set.seed(1)
sim(m7_1, ll = TRUE, n = 1e4) |> 
  apply(2, function(x) log_sum_exp(x) - log(1e4))
```

### Scoring the right data

This gives the right ingredients, but pointing `lppd()` at the same data used for training will be useless for the same reason as $R^2$ was: the score will keep going up as we add parameters. 

```{r}
set.seed(1)
str_c("m7_", 1:6) |> 
  map(get) |> 
  map(lppd) |> 
  map_dbl(sum)
```

The procedure needs to be better: 

1. Start with training sample of $N$ observations. 
2. Compute the posterior distribution for a given model on those data, compute the score on the training data, $D_{\text{train}}$. 
3. Take a new sample of $N$ observations, the test sample. 
4. Use the existing posterior to compute the score on those test data, $D_{\text{test}}$. 

RM illustrates this with simulation, but the code takes ages to run. It generates the plots on page 212. 

The true data generating model in this case is: 

$$
\begin{align}
y_i   &\sim \mathcal{N}(\mu_i, 1) \\
\mu_i &=    (0.15)x_{1, i} - (0.4)x_{2, i}
\end{align}
$$

RM's code generates the 'true' predictors as well as extra noise. With the higher values of `k` we try to fit increasing amounts of the noise as well as the signal. Plots tell the story: the performance in-sample keeps getting better, as we are simply encoding more of the original sample in the model. But the out of sample performance peaks at the true model, then gets worse. 

## Golem taming: regularisation

Interesting quote in the Rethinking section: 

> Despite how easy it is to use regularization, most traditional statistical methods use no regularization at all. Statisticians often make fun of machine learning for reinventing statistics under new names. But regularization is one area where machine learning is more mature. Introductory machine learning courses usually describe regularization. Most introductory statistics courses do not. 

RM also creates plots to show the effects of regularising priors, but I'll skip that to save a bit of time. The message is that setting a prior that is more tightly centred at 0 (i.e. no effect) will require stronger evidence from the data in order to yield a posterior distribution away from 0. 

## Predicting predictive accuracy

As outlined at the start of the chapter there are broadly two approaches: cross-validation and information theory. 

### Cross-validation

Great quote that accords with my experience: 

> How many folds should you use? **This is an understudied question**. 

Leave-one-out cross-validation (LOOCV) is common, but quickly becomes impractical if we have many observations. But there are ways to approximate this very closely without actually fitting $N$ models. One way is to weight each observation by its importance: how much will be posterior change if that observation is removed?

RM mentions **Pareto-smoothed Importance Sampling Cross-validation**, or PSIS. The Overthinking box gives more detail about it. 

If we did LOOCV fully we would be fitting $N$ models, leaving out one observation each time. Then our estimate of the out of sample lppd is: 

$$
\text{lppd}_{CV} = 
  \sum_{i = 1}^N 
    \frac{1}{S} \sum_{s = 1}^{S} \log \text{P}(y_i | \theta_{-i, s})
$$

Now instead we want to just take $S$ samples from the full posterior distribution $p(\theta | y)$, but then reweight each of them in a useful way. Specifically we want to reweight each sample $s$ using $p(y_i | \theta_s)$, i.e. the posterior probability of the omitted observation given that particular posterior sample. We actually use the inverse: 

$$
r(\theta_s) = \frac{1}{p(y_i | \theta_s)}
$$
To put that into the calculation of lppd it has to be normalised: 

$$
\text{lppd}_{IS} = 
  \sum_{i = 1}^N 
    \log
    \frac{\sum_{s = 1}^{S} r(\theta_s) p(y_i | \theta_s)}
      {\sum_{s = 1}^{S} r(\theta_s)}
$$

So in the summand the numerator is the sum of all the posterior probability of each observation, weighted by $r(\theta_s)$; the denominator is the sum over all values of $r(\theta_s)$. Take its log, then sum over all observations. 

This is the importance-based sampling part, but now comes the Pareto bit. The weights should follow a Pareto distribution: 

$$
p(r | u, \sigma, k) = \sigma^{-1}(1 + k(r - u) \sigma^{-1})^{-\frac{1}{k}-1}
$$

In this $u$ is the location of the distribution, $\sigma$ its scale, and $k$ the shape. For every $y_i$ we can fit a Pareto distribution, and then use that distribution to smooth the weights. (This is done by replacing a certain number of the largest weights with new values fit to the tail of the Pareto distribution.)

An added bonus is that the method provides its own diagnostic: when $k > 0.7$ that indicates that the PSIS estimate may be unstable. 

### Information criteria

This approach uses information theory to estimate the score out of sample. Or rather, the relative out-of-sample KL divergence (so that two models can be compared to one another). 

The most well-known of these information criteria is the **Akaike Information Criterion**, which estimates the out-of-sample deviance as: 

$$
\text{AIC} = D_{train} + 2p = -2\text{lppd} + 2p
$$

Here $p$ is what RM calls the "number of free parameters in the model", or what is often referred to as degrees of freedom. The factor of 2 in the right-hand side can be factored out without changing the relative deviance of models. 

AIC is only reliable when certain criteria are met (as listed in the book), which means we need something more general. 

RM mentions the **Deviance Information Criterion**, but quickly discounts it as a real alternative. 

The thing we want is the **Widely Applicable Information Criterion** (WAIC). It relies on no assumptions about the shape of the posterior. The subtle point is that it approximates the out-of-sample KL deviance, and that approximation will converge to the cross-validation approximation in a sufficiently large sample. When the sample size is limited the two can disagree, because they are estimating different things. The formula for WAIC is: 

$$
WAIC(y, \Theta) = -2(\text{lppd} - \sum_i \text{var}_{\theta} \log p(y_i|\theta))
$$

The first term is just the log-posterior predictive density. The second term is the penalty (analogous to the $2p$ in AIC): for each observation compute the variance in the log-posterior probabilities, then sum those variances. Those observation-level penalty scores also act as a measure of overfitting risk for each observation. 

RM shows a more detailed calculation of WAIC also. 

```{r}
waic_m <- quap(
  flist = alist(
    dist ~ dnorm(mu, sigma), 
    mu <- a + (b * speed), 
    a ~ dnorm(0, 100), 
    b ~ dnorm(0, 10), 
    sigma ~ dexp(1)
  ), 
  data = cars
)
set.seed(94)

logprob <- extract.samples(waic_m, n = 1000) |>
  as_tibble() |>
  rowid_to_column("sample_id") |>
  cross_join(
    cars |>
      as_tibble() |>
      rowid_to_column("obs_id")
  ) |>
  mutate(logprob = dnorm(dist, a + (b * speed), sigma, log = TRUE))

# Now to get the lppd we average the logprob for each observation. Do this with
# the log sum exp trick, as RM showed earlier. 
example_lppd <- logprob |>
  group_by(obs_id) |> 
  summarise(
    lppd = log_sum_exp(logprob) - log(1000), 
    # Summing the lppd column gives the first term in WAIC, now need to compute
    # the penalty.
    pWAIC = var(logprob), 
    .groups = "drop"
  ) |> 
  mutate(WAIC = -2 * (lppd - pWAIC))

example_lppd |> 
  summarise(across(WAIC, sum))

# Compare that with the result of the WAIC() function. 
WAIC(waic_m)
```

There is some difference because of simulation error, but an amount much smaller than the standard error estimate. It's possible to calculate that standard error directly also, as $\sqrt{N \text{Var}(p_{\text{WAIC}})}$ (where the $p_{\text{WAIC}}$ here is the figure for each observation). 

```{r}
sqrt(nrow(cars) * var(example_lppd$WAIC))
```

### Comparing CV, PSIS, and WAIC

RM creates another set of plots with the five polynomial models, which I won't recreate. 

RM points out that "CV and PSIS have higher variance as estimators of KL divergence, while WAIC has greater bias", and that "Watanabe recommends computing both WAIC and PSIS and contrasting them. If there are large differences, this implies one or both criteria are unreliable." Also reiterates that PSIS has the built-in warning for when it may be unreliable (viz. when $k$ is large). 

## Model comparison

### Model mis-selection

Reminder that causal inference and prediction are different tasks, and that CV and WAIC will favour models that make good predictions. So a good PSIS or WAIC score will not in general mean we have a good causal model. 

To show this we return to the models of plant growth and fungus from the last chapter. We can compute WAIC for all of the models we fit then. 

```{r include=FALSE}
set.seed(71)
dfungus <- tibble(h0 = rnorm(100, 10, 2), treatment = rep(0:1, each = 50)) |> 
  mutate(fungus = rbinom(100, size = 1, prob = 0.5 - (treatment * 0.4))) |> 
  mutate(h1 = h0 + rnorm(100, 5 - (3 * fungus)))

m6_6 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p ~ dlnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)

m6_7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment) + (bF * fungus), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)
m6_8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)
```

```{r}
set.seed(77)
compare(m6_6, m6_7, m6_8, func = WAIC)
```

The estimates from PSIS also: 

```{r}
set.seed(77)
compare(m6_6, m6_7, m6_8, func = PSIS)
```

The values are near-identical. Model `m6_7` gives the best predictions and scores seem substantially better than the others. In fact the 'good' model (i.e. the one that is consistent with the causal model) is only slightly better than the intercept-only model. But are these big differences? To check that we need to consider the standard error, but not of the WAIC itself. Instead we compute the standard error of the difference between WAIC for two models. 

```{r}
set.seed(91)
tibble(
  waic_m6_7 = WAIC(m6_7, pointwise = TRUE) |> pluck("WAIC"), 
  waic_m6_8 = WAIC(m6_8, pointwise = TRUE) |> pluck("WAIC")
) |> 
  mutate(diff_waic = waic_m6_7 - waic_m6_8) |> 
  summarise(dSE = sqrt(nrow(dfungus) * var(diff_waic)))
```

This is roughly what we see in the second row of the WAIC table under `dSE`, to within simulation variance. Since the actual difference (`dWAIC`) is 41, the 99% interval for the difference between the two models is: 

```{r}
41 + c(-1, 1) * 10.4 * 2.6
```

So the best model really is a lot better than the other in terms of prediction. But the right model for causal inference would never get chosen just on the basis of WAIC.

NB. The `weight` column at the end of the `compare()` table is a "traditional way to summarise relative support for each model". For each model $i$ the weight is: 

$$
w_i = \frac{\exp(-0.5\Delta_i)}{\sum_j \exp(-0.5\Delta_j)}
$$

The denominator sums the $\exp(-0.5\Delta_i)$ term across all of the models. In the case above, where `m6_7` has much lower WAIC than the others, the $\Delta_j$ terms for `m6_6` and `m6_8` are very large, which means their weights end up being approximately $\frac{0}{1}$. 

The weights are a useful heuristic but nothing more: to make judgements about the models we must consider the standard error in the WAIC estimates, which are not accounted for in the weights. 

### Outliers and other illusions

We return to the divorce data from chapter 5: Idaho was an outlier, and PSIS/WAIC may have revealed that to us. 

```{r}
data(WaffleDivorce, package = "rethinking")
ddivorce <- WaffleDivorce |> 
  as_tibble() |> 
  mutate(
    D = standardize(Divorce), 
    M = standardize(Marriage), 
    A = standardize(MedianAgeMarriage)
  )

m5_1 <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(mean = 0, sd = 0.2), 
    bA ~ dnorm(mean = 0, sd = 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)

m5_2 <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bM * M), 
    a ~ dnorm(mean = 0, sd = 0.2), 
    bM ~ dnorm(mean = 0, sd = 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)

m5_3 <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bA * A) + (bM * M), 
    a ~ dnorm(mean = 0, sd = 0.2), 
    bA ~ dnorm(mean = 0, sd = 0.5), 
    bM ~ dnorm(mean = 0, sd = 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)
```

Now check the model scores with PSIS. 

```{r}
set.seed(24071847)
compare(m5_1, m5_2, m5_3, func = PSIS)
```

We get a warning about high values of $k$, which is happening because of outliers (like Idaho). We can get the pointwise PSIS and plot things to make it clearer. 

```{r}
set.seed(4)
bind_cols(
  ddivorce |> 
    select(Loc), 
  PSIS(m5_3, pointwise = TRUE) |> 
    as_tibble() |> 
    select(k), 
  WAIC(m5_3, pointwise = TRUE) |> 
    as_tibble() |> 
    select(penalty)
) |> 
  ggplot(aes(k, penalty)) + 
  geom_point(colour = "steelblue") + 
  geom_text_repel(
    aes(label = Loc), 
    data = . %>% 
      filter(Loc %in% c("ME", "ID"))
  ) + 
  geom_vline(xintercept = 0.5, linetype = 2, colour = "grey50") + 
  scale_x_continuous(breaks = c(0, 0.5, 1), minor_breaks = NULL) + 
  scale_y_continuous(breaks = seq(0, 2, by = 0.5), minor_breaks = NULL) + 
  labs(
    title = "Gaussian model (m5_3)", 
    x = "PSIS Pareto k", 
    y = "WAIC penalty"
  )
```

The vertical dashed line is the point at which the variance of the Pareto distribution becomes infinite. Maine is slightly to the right, but still well below the practical limit for $k$ of 0.7. 

Idaho is a problem though: its $k$ is over 1, and its contribution to the WAIC penalty term is over 2. Given that the total penalty term in an ordinary linear regression should roughly equal the number free parameters (in this case 4), this is very high. It pushes the total penalty for this model much higher. 

```{r}
WAIC(m5_3)
```

This equates to additional overfitting risk. 

What to do? RM suggests that a good approach may be **robust regression**: in other words, using something with heavier tails than a Gaussian distribution to build the model. He gives the example of the student-t distribution, which we can use to refit the troublesome `m5_3` and then recalculate the PSIS. 

```{r}
m5_3_t <- quap(
  flist = alist(
    D ~ dstudent(2, mu, sigma), 
    mu <- a + (bA * A) + (bM * M), 
    a ~ dnorm(mean = 0, sd = 0.2), 
    bA ~ dnorm(mean = 0, sd = 0.5), 
    bM ~ dnorm(mean = 0, sd = 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)

PSIS(m5_3_t)
```

No warnings about high values of $k$, because the probability of an outlier under student-t distribution is higher than it was with a Gaussian. 

## Summary

RM points out that this chapter covers a lot! I feel more confident about what PSIS and WAIC are really doing now though. 

## Practice

### Easy

7E1. 

1. Continuity: very small changes in the inputs should only change the entropy by a small amount. 
2. The uncertainty should go up as the number of options increases. 
3. Additivity: when we combine different measures of entropy they should add. 

7E2. 

```{r}
- sum(c(0.7, 0.3) * log(c(0.7, 0.3)))
```

7E3. 

```{r}
- sum(c(0.2, 0.25, 0.25, 0.3) * log(c(0.2, 0.25, 0.25, 0.3)))
```

7E4. 

We must be careful here as $\log(0)$ is undefined. So if we just plug in the numbers to the code above we get: 

```{r}
- sum(c(1/3, 1/3, 1/3, 0) * log(c(1/3, 1/3, 1/3, 0)))
```

Instead we recall that $\lim_{p \to 0} p \log{p} = 0$, which means we can just drop the last term. 

```{r}
- sum(c(1/3, 1/3, 1/3) * log(c(1/3, 1/3, 1/3)))
```

The entropy of this die is less than the one in 7E3: we have effectively reduced the number of outcomes from four to three. 

### Medium

7M1. 

$$
\begin{align}
\text{AIC} &= -2\text{lppd} + 2p \\
           &= -2(\text{lppd} - p) \\
WAIC(y, \Theta) &= -2(\text{lppd} - \sum_i \text{var}_{\theta} \log p(y_i|\theta))
\end{align}
$$

The second form of AIC is more suggestive for this question. WAIC is more general, as its penalty term will take account of the variance of the posterior probabilities, rather than just the number of parameters.

To make the two the same we would need $p \approx \sum_i \text{var}_{\theta} \log p(y_i|\theta)$. 

7M2. 

Model selection is the process of selecting a model based on a score (e.g. PSIS). Model comparison is when we compare the scores across models. 

Model selection (as presented) is a rather mindless process. The information lost includes how much difference there is between the top-scoring model and the rest. 

7M3. 

The information criteria used all estimate at the level of the observation, then aggregate this. So if models were fit to different data we would not be comparing like with like (especially problematic in the case of outliers). 

Having different numbers of observations would also pose problems because deviance is additive, so the information criteria are estimating that sum. Sums over different numbers of observations cannot be compared meaningfully. 

7M4. 

```{r}
m_7m4_wide <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(mean = 0, sd = 1), 
    bA ~ dnorm(mean = 0, sd = 1), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)

m_7m4_med <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(mean = 0, sd = 0.2), 
    bA ~ dnorm(mean = 0, sd = 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)

m_7m4_tight <- quap(
  flist = alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(mean = 0, sd = 0.1), 
    bA ~ dnorm(mean = 0, sd = 0.2), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
)
set.seed(1940)
compare(m_7m4_wide, m_7m4_med, m_7m4_tight)
```

The number of effective parameters gets lower as the priors get tighter. This corresponds to making the model less flexible: the tighter the priors, the fewer parameter values will be compatible with the model. 

7M5. 

The term _sceptical priors_ helps with understanding this: we are telling our model to be more sceptical about what it sees in the data. Overfitting is when our model overreacts to patterns in the training data: making our model sceptical means that the evidence required in the data will have to be stronger to move our posterior estimates away from the prior. 

7M6. 

Continuing from the last question: suppose we made our priors incredibly tight, e.g. $\mathcal{N}(0, 0.00001)$. Then our 99% interval for possible values would be: 

```{r}
qnorm(c(0.005, 0.995), mean = 0, sd = 0.00001)
```

This will make it incredibly hard for any amount of evidence in the data to shift us from our prior: we will be insensitive to the patterns in the data, and tend to underfit. 

### Hard

7H1. 

```{r}
data("Laffer")
dlaffer <- Laffer |> 
  as_tibble() |> 
  mutate(tax_rate_2 = tax_rate^2, tax_rate_3 = tax_rate^3) |> 
  mutate(across(everything(), .fns = standardize))
```

We will try three models: straight-line, quadratic, and cubic. 

```{r}
m_7h1_linear <- quap(
  flist = alist(
    tax_revenue ~ dnorm(mu, sigma), 
    mu <- a + (b1 * tax_rate), 
    a ~ dnorm(0, 0.2), 
    b1 ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)

m_7h1_quad <- quap(
  flist = alist(
    tax_revenue ~ dnorm(mu, sigma), 
    mu <- a + (b1 * tax_rate) + (b2 * tax_rate_2), 
    a ~ dnorm(0, 0.2), 
    c(b1, b2) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)

m_7h1_cubic <- quap(
  flist = alist(
    tax_revenue ~ dnorm(mu, sigma), 
    mu <- a + (b1 * tax_rate) + (b2 * tax_rate_2) + (b3 * tax_rate_3), 
    a ~ dnorm(0, 0.2), 
    c(b1, b2, b3) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)
```

Let's first check the PSIS scores

```{r}
compare(m_7h1_linear, m_7h1_quad, m_7h1_cubic, func = PSIS)
```

All of them are fairly similar (i.e. within two standard errors), but we get warnings about $k > 1$ for all of them. In other words: the estimate isn't too reliable. 

It's hard to draw any conclusions about this. 

7H2. 

```{r}
model_list_7h1 <- list(m_7h1_linear, m_7h1_quad, m_7h1_cubic)
set.seed(629)
total_penalty_7h1 <- map_dbl(model_list_7h1, ~ PSIS(.x)[["penalty"]])
set.seed(629)
max_penalty_7h1 <- map_dbl(
  model_list_7h1, 
  ~ PSIS(.x, pointwise = TRUE) |> 
    as_tibble() |> 
    slice_max(k) |> 
    pull(penalty)
)
tibble(
  name = c("linear", "quadratic", "cubic"), 
  total_penalty = total_penalty_7h1, 
  max_penalty = max_penalty_7h1
) |> 
  mutate(
    penalty_prop = round(max_penalty / total_penalty, digits = 2)
  )
```

That one point is contributing about 80% of the total penalty term. The overall penalty is roughly 8-10, which is far more than the number of actual parameters, suggesting an overfitting risk. Note that the cubic model has a much higher penalty term overall, but the penalty for the linear and quadratic models is essentially identical. 

We can switch to the student-t distribution and repeat the model-fitting. 

```{r}
m_7h2_linear <- quap(
  flist = alist(
    tax_revenue ~ dstudent(2, mu, sigma), 
    mu <- a + (b1 * tax_rate), 
    a ~ dnorm(0, 0.2), 
    b1 ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)

m_7h2_quad <- quap(
  flist = alist(
    tax_revenue ~ dstudent(2, mu, sigma), 
    mu <- a + (b1 * tax_rate) + (b2 * tax_rate_2), 
    a ~ dnorm(0, 0.2), 
    c(b1, b2) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)

m_7h2_cubic <- quap(
  flist = alist(
    tax_revenue ~ dstudent(2, mu, sigma), 
    mu <- a + (b1 * tax_rate) + (b2 * tax_rate_2) + (b3 * tax_rate_3), 
    a ~ dnorm(0, 0.2), 
    c(b1, b2, b3) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dlaffer
)
```

```{r}
set.seed(654)
compare(m_7h2_linear, m_7h2_quad, m_7h2_cubic, func = PSIS)
```

Very little to choose between the models, but it might be worthwhile looking at the parameter estimates in all of the models. 

```{r}
coeftab(
  m_7h1_linear, 
  m_7h1_quad, 
  m_7h1_cubic, 
  m_7h2_linear, 
  m_7h2_quad, 
  m_7h2_cubic
) |> 
  plot(pars = c("a", "b1", "b2", "b3"))
```

Some things to note: 

- The robust regression models have all estimated the intercept below 0, but we know that's not correct because the data were standardised. 
- All of the models have a mean estimate for `b1` (i.e. the linear coefficient) above 0, but with intervals that include 0. Hard to draw firm conclusions. 
- The estimates for the quadratic coefficient are closer to zero. This suggests that the concave parabola shown by the WSJ is probably not a good model for these data. Intervals are very wide. 
- Intervals on the cubic term are huge: many values are consistent with the data. 
- In general the intervals on the parameter estimates in the curved models are much wider: adding those extra terms means that many more curves are plausible given the data. 

We can repeat the exercise to check the highest proportion of the penalty generated by one observation in each model. 

```{r}
model_list_7h2 <- list(m_7h2_linear, m_7h2_quad, m_7h2_cubic)
set.seed(629)
total_penalty_7h2 <- map_dbl(model_list_7h2, ~ PSIS(.x)[["penalty"]])
set.seed(629)
max_penalty_7h2 <- map_dbl(
  model_list_7h2, 
  ~ PSIS(.x, pointwise = TRUE) |> 
    as_tibble() |> 
    slice_max(k) |> 
    pull(penalty)
)
tibble(
  name = c("linear", "quadratic", "cubic"), 
  total_penalty = total_penalty_7h2, 
  max_penalty = max_penalty_7h2
) |> 
  mutate(
    penalty_prop = round(max_penalty / total_penalty, digits = 2)
  )
```

No point is contributing more than 5% of the total penalty: given that there are 29 points this is only a little bit more than a $1/N$ share. The overall penalty has dropped a lot as well. 

7H3. 

```{r}
d_7h3 <- tibble(
  island = 1:3, 
  A = c(0.2, 0.8, 0.05), 
  B = c(0.2, 0.1, 0.15), 
  C = c(0.2, 0.05, 0.7), 
  D = c(0.2, 0.025, 0.05), 
  E = c(0.2, 0.025, 0.05)
)
d_7h3  
```

First we calculate the entropy for each of the islands. 

```{r}
entropy_7h3 <- d_7h3 |> 
  pivot_longer(-island) |> 
  group_by(island) |> 
  summarise(entropy = -sum(value * log2(value)), .groups = "drop")

entropy_7h3
```

Island 2, which has 80% of its population as just one bird has the lowest entropy. We can understand this as quantifying our information about what bird we are likely to see: island 2 is the one where we can be most sure about which bird we will see. Island 3 is a little bit higher, as its highest proportion is 70%. We have a little less information about what bird we will see, and therefore slightly higher entropy. Island 1, where all of the proportions are equal, is the one where we know the least about what to expect. So its entropy is rather higher than either of the others. 

Because I've calculated the entropy in bits (i.e. $\log_2$ rather than $\ln$), the entropy score for island 1 has a clear interpretation: it is simply $\log_2 5$, because that is how many bits it takes to encode five equally possible outcomes. We can exponentiate each of the entropy scores to get this figure for all three islands. 

```{r}
entropy_7h3 |> 
  mutate(exp_2 = round(2^entropy, digits = 2))
```

Island 2 has entropy equivalent to encoding just over two equally likely outcomes, and island 3 a little less than three. 

Now we compute the KL divergence of each island from the others. 

```{r}
island1_7h3 <- d_7h3 |> 
  group_by(island) |> 
  nest() |> 
  ungroup() |> 
  transmute(
    island1 = island, 
    bird_vec1 = map(data, ~ .x |> t() |> as.double())
  )
island2_7h3 <- island1_7h3 |> 
  rename_with(~ str_replace(.x, "1", "2"))
D_KL_7h3 <- cross_join(island1_7h3, island2_7h3) |> 
  filter(island1 != island2) |> 
  mutate(D_KL = map2_dbl(bird_vec1, bird_vec2, ~ sum(.x * (log(.x / .y)))))

D_KL_7h3 |> 
  arrange(D_KL)
```

The lowest pairwise scores are between islands 1 and 3. Which island predicts the others best?

```{r}
D_KL_7h3 |> 
  group_by(island2) |> 
  summarise(total_D_KL = sum(D_KL)) |> 
  arrange(total_D_KL)
```

Island 1 has a much lower overall KL divergence than the others. Why? Because its proportions are spread evenly, that distribution is less surprised than the others when it sees any particular bird. 

7H4. 

```{r}
set.seed(11)
dmarriage <- sim_happiness() |> 
  as_tibble()
dmarriage_adults <- dmarriage |> 
  filter(age > 17L) |> 
  mutate(A = scales::rescale(age)) |> 
  mutate(mid = married + 1L)

m6_9 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a[mid] + (bA * A), 
    a[mid] ~ dnorm(0, 1), 
    bA ~ dnorm(0, 2), 
    sigma ~ dexp(1)
  ), 
  data = dmarriage_adults
)

m6_10 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(0, 1), 
    bA ~ dnorm(0, 2), 
    sigma ~ dexp(1)
  ), 
  data = dmarriage_adults
)
```

Remember that `m6_10` is the correct model (i.e. it agrees with the data generating process that we used for the simulation). 

```{r}
compare(m6_9, m6_10)
```

This highlights the difference between causal inference and prediction. When we condition on a collider in `m6_9` (i.e. whether or not a person is married) we make the wrong inferences about the causal structure. But a person's marriage status is extremely informative about their happiness: we know from the DAG that the causality runs the other way, but marriage status helps us to make better predictions. 

7H5. 

```{r}
data("foxes")
dfox <- foxes |> 
  as_tibble() |> 
  mutate(across(c(avgfood, area, weight, groupsize), standardize))
```

Now we create the models as specified. 

```{r}
m_7h5_1 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bF * avgfood) + (bS * groupsize) + (bA * area), 
    c(a, bF, bS, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)

m_7h5_2 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bF * avgfood) + (bS * groupsize), 
    c(a, bF, bS) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)

m_7h5_3 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bS * groupsize) + (bA * area), 
    c(a, bS, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)

m_7h5_4 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bF * avgfood), 
    c(a, bF) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)

m_7h5_5 <- quap(
  flist = alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bA * area), 
    c(a, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)
```

Now compare them with WAIC. 

```{r}
set.seed(849)
compare(m_7h5_1, m_7h5_2, m_7h5_3, m_7h5_4, m_7h5_5)
```

Let's remind ourselves of the DAG also. 

```{r}
dag_fox <- dagify(
  avgfood ~ area, 
  groupsize ~ avgfood, 
  weight ~ avgfood + groupsize, 
  coords = tibble(
    name = c("avgfood", "area", "weight", "groupsize"), 
    x = c(1, 2, 2, 3), 
    y = c(2, 3, 1, 2)
  )
)

dag_plot_fox <- dag_fox |> 
  tidy_dagitty() |>
  as_tibble() |> 
  inner_join(
    tibble(
      name = c("avgfood", "area", "weight", "groupsize"), 
      label = c("F", "A", "W", "S")
    ), 
    by = "name"
  ) |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(size = 12) +
  geom_dag_text(aes(label = label)) +
  geom_dag_edges() +
  theme_dag()

dag_plot_fox
```

The model with all possible predictors scores the best by WAIC, which isn't too surprising: as we saw in the last question, even confounding variables can help predictions. 

The model that was the most helpful for causal inference, and that was consistent with the DAG, was the second (with food and group size included but not area). It has almost identical WAIC to the first model with all predictors, and the difference is much smaller than the standard error. In other words: based on predictive accuracy these two models are indistinguishable. In fact the first three models all have about the same WAIC, well within one standard error. 

The fourth and fifth models are also interesting: their WAIC scores seem much higher if we look only at the `dWAIC` column, but the overall WAIC scores are higher for these models in general. Looking at the standard error for models four and five, we see that they are each about 1.4 standard errors higher than the best WAIC score: this is suggestive that these models have worse predictive accuracy, but not conclusive. 
