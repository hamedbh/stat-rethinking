# God Spiked the Integers

```{r}
#| label: setup
#| output: false
library(rethinking)
set_ulam_cmdstan(TRUE)
library(patchwork)
library(ggrepel)
library(magrittr)
library(tidyverse)
library(tidybayes)
library(bayesplot)
library(shape)
library(dagitty)
library(ggdag)
# set up the theme
theme_set(
  theme_light() + 
    theme(panel.grid = element_blank())
)
walk(list.files(here::here("R"), full.names = TRUE), source)
```

## Binomial regression

In the globe-tossing example we denoted the binomial distribution as: 

$$
\begin{align*}
y \sim \operatorname{Binomial}(n, p).
\end{align*}
$$

Here $y$ is a count (often called the number of 'successes'), $p$ is the probability that any one trial is a success, and $n$ is the number of trials. RM distinguishes between two varieties of binomial regression, which differ only in how the data are structured. 

1. **Logistic regression** is when the data are organised as single trials, so that the outcome variable is $\in \{0, 1\}$. 
2. **Aggregated binomial regression** is when all trials with the same covariate values are combined together, and the outcome variable will be in $\in \{0, 1, \dots , n\}$.

### Logistic regression: Prosocial chimpanzees

RM outlines the experiment: in short, we want to see if the presence of another chimpanzee affects whether chimpanzees will pick the prosocial option that gives food to both.

```{r}
data(chimpanzees)
dchimp <- chimpanzees |> 
  as_tibble() |> 
  # Create the treatment variable that RM shows a bit later
  mutate(treatment = 1L + prosoc_left + (2L * condition))
dchimp
```

Here `pulled_left` is the outcome variable: an indicator of whether the focal chimp pulled the left lever. Then `prosoc_left` is a predictor: an indicator of whether the left side was the prosocial option. Finally `condition` indicates whether or not a partner was present. 

Now we can set up our model: 

$$
\begin{align*}
  L_i &\sim \operatorname{Bernoulli}(p_i) \\
  \operatorname{logit}(p_i) &= \alpha_{\text{ACTOR}[i]} + 
    \beta_{\text{TREATMENT}[i]} \\
  \alpha_j &\sim \text{TBC} \\
  \beta_k &\sim \text{TBC}
\end{align*}
$$

$L_i$ is the indicator variable for `pulled_left`. In this model the $\alpha$ parameters are for each chimp, but the $\beta$ parameters are shared across all the chimps. RM teases that we will try estimating $\beta$ for each chimp later in the book. 

Now we need to fill in those priors. RM illustrates a poor version of a simplified model to start with using `quap()`: let's do the same for speed. 

$$
\begin{align*}
  L_i &\sim \operatorname{Bernoulli}(p_i) \\
  \operatorname{logit}(p_i) &= \alpha\\
  \alpha_j &\sim \mathcal{N}(0, \omega)
\end{align*}
$$

So we need to pick $\omega$: we can recreate the plot from RM showing the effect of $\omega = 10$ vs. a more sensible value of $\omega = 1.5$

```{r}
set.seed(1999)
fig11_3_1 <- list(
  bad = quap(
    alist(
      pulled_left ~ dbinom(1, p), 
      logit(p) <- a, 
      a ~ dnorm(0, 10)
    ), 
    data = dchimp
  ), 
  good = quap(
    alist(
      pulled_left ~ dbinom(1, p), 
      logit(p) <- a, 
      a ~ dnorm(0, 1.5)
    ), 
    data = dchimp
  )
) |> 
  imap_dfr(
    ~ extract.prior(.x, n = 1e4) |> 
      as_tibble() |> 
      mutate(across(a, inv_logit)), 
    .id = "prior"
  ) |> 
  ggplot(aes(a, colour = prior)) + 
  geom_density(adjust = 0.2, linewidth = 1.1) + 
  geom_label_repel(
    aes(x = x, y = y, label = label, colour = prior), 
    data = tibble(
      x = c(0.1, 0.5), 
      y = c(4, 1.7), 
      label = c("a ~ dnorm(0, 10)", "a ~ dnorm(0, 1.5)"), 
      prior = c("bad", "good")
    ), 
    size = 4
  ) + 
  scale_colour_manual(values = c("black", "steelblue")) + 
  scale_x_continuous("prior prob pulled left", breaks = seq(0, 1, by = 0.2)) + 
  theme(
    legend.position = "none", 
    axis.ticks.y = element_blank(), 
    axis.text.y = element_blank(), 
    axis.title.y = element_blank()
  )
fig11_3_1
```

RM also shows the effect of a wide prior on the $\beta$ parameters. Let's do that, recreate the plot, and put it together to get the whole of figure 11.3. 

```{r}
set.seed(1999)
fig11_3_1 + (
  list(
    bad = quap(
      alist(
        pulled_left ~ dbinom(1, p), 
        logit(p) <- a + b[treatment], 
        a ~ dnorm(0, 1.5), 
        b[treatment] ~ dnorm(0, 10)
      ), 
      data = dchimp
    ), 
    good = quap(
      alist(
        pulled_left ~ dbinom(1, p), 
        logit(p) <- a + b[treatment], 
        a ~ dnorm(0, 1.5), 
        b[treatment] ~ dnorm(0, 0.5)
      ), 
      data = dchimp
    )
  ) |> 
    imap_dfr(
      ~ .x |> 
        extract.prior(n = 1e4) |> 
        map(as_tibble, .name_repair = "universal") |> 
        reduce(bind_cols) |> 
        set_names(c("a", str_c("b", 1:4))) |> 
        mutate(across(starts_with("b"), ~ inv_logit(a + .x))) |> 
        transmute(contrast = abs(b1 - b2)), 
      .id = "prior"
    ) |> 
    ggplot(aes(contrast, colour = prior)) + 
    geom_density(adjust = 0.2, linewidth = 1.1) + 
    geom_label_repel(
      aes(x = x, y = y, label = label, colour = prior), 
      data = tibble(
        x = c(0.9, 0.32), 
        y = c(5, 2), 
        label = c("b ~ dnorm(0, 10)", "b ~ dnorm(0, 0.5)"), 
        prior = c("bad", "good")
      ), 
      size = 4
    ) + 
    scale_colour_manual(values = c("black", "steelblue")) + 
    scale_x_continuous(
      "prior diff between treatments", 
      breaks = seq(0, 1, by = 0.2)
    ) + 
    theme(
      legend.position = "none", 
      axis.ticks.y = element_blank(), 
      axis.text.y = element_blank(), 
      axis.title.y = element_blank()
    )
)
```

In both plots we can see the effect of the wide prior: the prior gives too much plausibility to large values (in absolute terms), and on the log-odds scale that translates to near-certainty of success or failure. The better priors (in blue) are a bit more sceptical about large values: in the left plot this shows in the even spread of prior values across the range $[0, 1]$; in the right plot the model expects that smaller differences are more likely than large ones, but allows for all of them. 

Now that we have the priors we can start fitting models. 

```{r}
chimp_list <- dchimp |> 
  select(pulled_left, actor, treatment) |> 
  compose_data()
```

```{r}
mod_11_4 <- cmdstan_model(here::here("inst/Stan/m11_4.stan"))
m11_4 <- mod_11_4$sample(data = chimp_list, seed = 1144, refresh = 0)
```

```{r}
precis_cmdstan(m11_4, variables = c("a", "b"))
```

```{r}
spread_draws(m11_4, a[i]) |> 
  mutate(a = inv_logit(a)) |> 
  reframe(
    tibble(
      name = c("mean", "lower", "upper"), 
      value = c(mean(a), HPDI(a))
    )
  ) |> 
  pivot_wider() |> 
  mutate(i = str_c("V", i)) |> 
  ggplot(aes(x = mean, y = fct_rev(i), xmin = lower, xmax = upper)) + 
  geom_pointrange() +
  geom_vline(xintercept = 0.5, linetype = 2, colour = "grey50") + 
  coord_cartesian(xlim = c(0, 1)) + 
  labs(x = "Value", y = NULL)
```

Now we can summarise the treatment effects. 

```{r}
spread_draws(m11_4, b[i]) |> 
  reframe(
    tibble(
      name = c("mean", "lower", "upper"), 
      value = c(mean(b), HPDI(b))
    ), 
    .groups = "drop"
  ) |> 
  pivot_wider() |> 
  arrange(i) |> 
  mutate(i = c("R/N", "L/N", "R/P", "L/P")) |> 
  mutate(i = fct_rev(fct_inorder(i))) |> 
  ggplot(aes(x = mean, y = i, xmin = lower, xmax = upper)) + 
  geom_pointrange() +
  geom_vline(xintercept = 0, linetype = 2, colour = "grey50") +
  labs(x = "Value", y = NULL)
```

The way RM has structured the treatment variable means we need to compare 1 with 3 and 2 with 4. Better to compute the contrasts explicitly. 

```{r}
spread_draws(m11_4, b[i]) |> 
  ungroup() |> 
  pivot_wider(names_from = "i", names_prefix = "b", values_from = "b") |> 
  transmute(db13 = b1 - b3, db24 = b2 - b4) |> 
  pivot_longer(everything(), names_to = "contrast") |> 
  group_by(contrast) |> 
  reframe(
    tibble(
      name = c("mean", "lower", "upper"), 
      value = c(mean(value), HPDI(value))
    )
  ) |> 
  pivot_wider() |> 
  mutate(contrast = factor(contrast)) |> 
  ggplot(aes(x = mean, y = fct_rev(contrast), xmin = lower, xmax = upper)) + 
  geom_pointrange() +
  geom_vline(xintercept = 0, linetype = 2, colour = "grey50") +
  labs(x = "Value", y = NULL)
```

RM decodes this in the book: the contrasts are between no-partner and partner treatments. For `db13` this is where the prosocial option is on the right, so a positive value (i.e. a bigger difference) is evidence of prosocial behaviour: we want to see the chimps pulling the right lever more often with a partner present. For `db24` a negative value would be evidence of prosocial behaviour. We see the first but not the second, so there's nothing conclusive here at all. 

Now we can do some posterior predictive checks. 

```{r}
list(
  first = dchimp |> 
    group_by(actor, treatment) |> 
    summarise(across(pulled_left, mean), .groups = "drop") |> 
    mutate(
      grp = as.integer(treatment %% 2 == 0) + 1L, 
      treatment_chr = case_when(
        treatment == 1L ~ "R/N", 
        treatment == 2L ~ "L/N", 
        treatment == 3L ~ "R/P", 
        treatment == 4L ~ "L/P", 
        TRUE ~ "error"
      )
    ) |> 
    {
      \(x) {
        assertthat::assert_that(!any(x[["treatment_chr"]] == "error"))
        x
      }
    }() |> 
    separate(
      treatment_chr, 
      into = c("side", "partner"), 
      sep = "/", 
      remove = FALSE
    ) |> 
    ggplot(aes(x = treatment, y = pulled_left, shape = partner)) + 
    geom_point(colour = "steelblue") + 
    geom_line(aes(group = grp), colour = "steelblue", alpha = 0.4) + 
    geom_text_repel(
      aes(label = treatment_chr), 
      data = . %>% 
        filter(actor == 1L)
    ) + 
    geom_hline(yintercept = 0.5, linetype = 2, alpha = 0.5) + 
    facet_wrap(~ actor, nrow = 1) + 
    scale_shape_manual(values = c(21, 19)) + 
    scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(0, 1)) + 
    labs(
      subtitle = "observed proportions by actor", 
      x = NULL, 
      y = "proportion left lever"
    ) + 
    theme(
      plot.subtitle = element_text(hjust = 0.5), 
      axis.ticks.x = element_blank(), 
      axis.text.x = element_blank(), 
      legend.position = "none"
    ), 
  second = spread_draws(m11_4, a[i], b[j]) |> 
    mutate(p = inv_logit(a + b)) |> 
    reframe(
      tibble(
        name = c("pulled_left", "lower", "upper"), 
        value = c(mean(p), HPDI(p))
      )
    ) |> 
    pivot_wider() |> 
    rename(actor = i, treatment = j) |> 
    mutate(
      grp = as.integer(treatment %% 2 == 0) + 1L, 
      treatment_chr = case_when(
        treatment == 1L ~ "R/N", 
        treatment == 2L ~ "L/N", 
        treatment == 3L ~ "R/P", 
        treatment == 4L ~ "L/P", 
        TRUE ~ "error"
      )
    ) |> 
    {
      \(x) {
        assertthat::assert_that(!any(x[["treatment_chr"]] == "error"))
        x
      }
    }() |> 
    separate(
      treatment_chr, 
      into = c("side", "partner"), 
      sep = "/", 
      remove = FALSE
    ) |> 
    ggplot(aes(x = treatment, y = pulled_left, shape = partner)) + 
    geom_linerange(aes(ymin = lower, ymax = upper)) + 
    geom_point(colour = "black") + 
    geom_line(aes(group = grp), colour = "grey30", alpha = 0.4) + 
    geom_text_repel(
      aes(label = treatment_chr), 
      data = . %>% 
        filter(actor == 1L)
    ) + 
    geom_hline(yintercept = 0.5, linetype = 2, alpha = 0.5) + 
    facet_wrap(~ actor, nrow = 1) + 
    scale_shape_manual(values = c(21, 19)) + 
    scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(0, 1)) + 
    labs(
      subtitle = "posterior predictions by actor", 
      x = NULL, 
      y = "proportion left lever"
    ) + 
    theme(
      plot.subtitle = element_text(hjust = 0.5), 
      axis.ticks.x = element_blank(), 
      axis.text.x = element_blank(), 
      legend.position = "none"
    )
) |> 
  wrap_plots(nrow = 2)
```

Our current model has varying intercepts by actor and then varying coefficients for the `treatment`, and this latter part is essentially an interaction between the `prosoc_left` (i.e. where is the prosocial option) and `condition` (i.e. whether or not a partner is present). So instead RM suggests another model: where we allow for each of those variables to act independently with no interaction. 

```{r}
mod_11_5 <- cmdstan_model(here::here("inst/Stan/m11_5.stan"))
m11_5 <- mod_11_5$sample(
  data = dchimp |> 
    mutate(side = prosoc_left + 1L, cond = condition + 1L) |> 
    select(pulled_left, side, cond, actor) |> 
    compose_data(),
  seed = 1805,
  refresh = 0
)
```

We can compare these models with PSIS: 

```{r}
compare_cmdstan(m11_4, m11_5)
```

They have almost identical predictive accuracy. RM notes that this model comparison is not for use in selecting a model: that choice (`m11_4`) flows from the experiment and hypothesis. 

### Relative shark and absolute deer

The chimp problem focussed on **absolute effects**: how much different does the treatment make in the outcome (i.e. probability of pulling a lever)? Instead we can consider **relative effects**, or proportional changes in the odds of an outcome. 

### Aggregated binomial: Chimpanzees again, condensed.

In the chimps example above we had one row per experiment. So in `n` experiments each was modelled as being $\operatorname{Binomial}(1, p)$ (or, equivalently, $\operatorname{Bernoulli}(p)$). However we can compress the data and model each unique combination of predictors with the outcome as $\operatorname{Binomial}(n, p)$. 

```{r}
dchimp_agg <- chimpanzees |> 
  as_tibble() |> 
  group_by(
    treatment = 1L + prosoc_left + (2L * condition), 
    actor, 
    side = prosoc_left + 1L, 
    cond = condition + 1L
  ) |> 
  summarise(
    n_trials = n(), 
    left_pulls = sum(pulled_left), 
    .groups = "drop"
  )
dchimp_agg
```

We've compressed the data frame from `r nrow(dchimp)` rows to `r nrow(dchimp_agg)`. Now we can build the model again. 

```{r}
mod_11_6 <- cmdstan_model(here::here("inst/Stan/m11_6.stan"))
m11_6 <- mod_11_6$sample(
  data = compose_data(dchimp_agg),
  seed = 804,
  refresh = 0
)
```

**This is a bad comparison!!!** 

```{r}
compare_cmdstan(m11_6, m11_4)
```

**Again: this is a bad comparison!!!** RM walks through the output carefully.  

First the PSIS scores are very different. This is because the calculations are quite different for each: in the binomial regression case (i.e. 11_6) there is an extra factor at the front of the likelihood to account for different orderings of the data. This increases the probability, which in turn lowers the PSIS. 

RM illustrates this with a simple example: deviance calculated for probability 0.2, 6 successes, 9 trials. 

```{r}
# aggregated, binomial regression
-2 * dbinom(6, 9, prob = 0.2, log = TRUE)
# disaggregated, logistic regression
-2 * sum(dbinom(rep(c(1, 0), c(6, 3)), size = 1, prob = 0.2, log = TRUE))
```

RM says that this difference is "entirely meaningless", but then moves on to cover the warnings. However his discussion of the warnings makes me think this difference _isn't_ meaningless. In short: the organisation of the data makes a statement about how we should consider those trials. Are they a completely homogeneous block, which can be left out of the model (i.e. in cross-validation) en masse? If so then binomial regression is a reasonable approach, but otherwise (as RM says) logistic regression is preferable. 

The different number of observations in each model is to be expected, since this was the main change we made to the model. We should never do this, because it changes the computations of the PSIS. 

Then there is the warning about high $k$ values, which didn't come up in logistic regression. This is because of the data structure also: by compressing the data we change the nature of the LOO cross-validation. Instead of one experiment being left out each time, 18 are left out. So compared with the logistic regression setting each observation can affect the results more dramatically. This is easy to see in a plot. 

```{r}
tibble(mod_name = c("m11_4", "m11_6")) |> 
  mutate(mod = map(mod_name, get)) |> 
  mutate(
    penalty = map(
      mod, 
      ~ suppressWarnings(
        loo::waic(.x$draws("log_lik"))[["pointwise"]][, "p_waic"]
      )
    ),
    k = map(
      mod, 
      ~ suppressWarnings(
        loo::psis(.x$draws("log_lik"))[["diagnostics"]][["pareto_k"]]
      )
    )
  ) |> 
  select(-mod) |> 
  unnest(c(penalty, k)) |> 
  ggplot(aes(k, penalty)) + 
  geom_vline(xintercept = 0.5, colour = "grey50", linetype = 2) + 
  geom_point(colour = "steelblue", alpha = 0.4) + 
  facet_wrap(~ mod_name)
```

As RM puts it: if you use the aggregated, binomial regression approach you are "implicitly assuming that only large chunks of the data are separable". 

### Aggregated binomial: Graduate school admissions.

Now comes a very famous example: the Berkeley admissions data. 

```{r}
data(UCBadmit)
UCBadmit |> 
  as_tibble() |> 
  print(n = 12)
```

For six departments we have the numbers of male and female applicants who were accepted and rejected. So there are 12 rows in the data frame but these represent `r format(sum(UCBadmit[["applications"]]), big.mark = ",")` applications. We could easily split these data out with `tidyr::uncount()` and a modest amount of reshaping. 

```{r}
UCBadmit |> 
  as_tibble() |> 
  pivot_longer(cols = c(admit, reject), names_to = "result", values_to = "n") |> 
  mutate(result = as.integer(result == "admit")) |> 
  uncount(n)
```

In this case though we will work with the aggregated data to evaluate whether there is evidence of gender bias in the admissions. 

$$
\begin{align*}
  A_i &\sim \operatorname{Binomial}(N_i, p_i) \\
  \operatorname{logit}(p) &= \alpha_{\text{GID}[i]} \\
  \alpha_j &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$

Here the $\text{GID}[i]$ is an index for the gender of the applicant. So let's fit the model. 

```{r}
dadmit <- UCBadmit |>
  as_tibble() |> 
  transmute(
    admit, 
    applications, 
    gid = if_else(applicant.gender == "male", 1L, 2L)
  )
ladmit <- compose_data(dadmit)
```


```{r}
mod_11_7 <- cmdstan_model(here::here("inst/Stan/m11_7.stan"))
m11_7 <- mod_11_7$sample(
  data = ladmit,
  seed = 909,
  refresh = 0
)
```

```{r}
precis_cmdstan(m11_7, variables = "a")
```

The estimate for male applicants is higher, but by how much? We need to compute the contrasts, which gets us back to the shark and deer: compute both absolute differences (shark, logit scale) and the relative differences (deer, outcome scale). 

```{r}
spread_draws(m11_7, a[gid]) |> 
  mutate(gid = c("male", "female")[gid]) |> 
  pivot_wider(names_from = "gid", values_from = "a") |> 
  transmute(
    diff_a = male - female, 
    diff_p = inv_logit(male) - inv_logit(female)
  ) |> 
  precis()
```

So the absolute difference is positive (i.e. males have a higher log-odds of being admitted than females). The difference in probabilities is somewhere between 12% and 16%. 

We can also plot posterior predictive checks. 

```{r}
m11_7 |> 
  spread_draws(p[n]) |> 
  mean_hdi(.width = 0.89) |> 
  inner_join(
    UCBadmit |> 
      as_tibble() |> 
      rowid_to_column("n") |> 
      group_by(n, dept, gender = applicant.gender) |> 
      summarise(true_prop = admit / applications, .groups = "drop"), 
    by = "n"
  ) |> 
  select(n, dept, gender, p, .lower, .upper, true_prop) |> 
  ggplot(aes(n)) + 
  geom_point(aes(y = true_prop), col = "steelblue") + 
  geom_line(aes(y = true_prop, group = dept), col = "steelblue") + 
  geom_label(
    aes(y = y, label = dept), 
    data = . %>%
      group_by(dept = as.character(dept)) %>%
      summarise(
        y = max(true_prop), 
        n = mean(n)
      ), 
    colour = "steelblue"
  ) + 
  geom_pointrange(aes(y = p, ymin = .lower, ymax = .upper), fatten = 1) + 
  scale_x_continuous("case", breaks = seq_len(12)) + 
  scale_y_continuous(
    "proportion admitted", 
    breaks = seq(0, 1, by = 0.2), 
    limits = c(0, 1)
  ) + 
  labs(
    title = "Posterior validation check", 
    subtitle = "Posterior prediction intervals in black, observed in blue"
  )
```

For each department the men are on the left and women on the right. The slope of the blue line tells us about which group has a higher proportion admitted. Obviously the predictions are terrible. There are only two departments where the proportion of men admitted is higher than that of women, yet our model expects higher proportions of men in every department. Of course that is only because that's the question we asked of the model: there is a global mean for each gender, and the model uses that for all predictions. Which is why the prediction intervals are identical in each department (to within simulation error): our model cannot account for the differences between departments. 

So we can change the question to ask "what is the average difference in probability of admission for men and women within the departments?"

$$
\begin{align*}
  A_i &\sim \operatorname{Binomial}(N_i, p_i) \\
  \operatorname{logit}(p_i) &= \alpha_{\text{GID}[i]} + \delta_{\text{DEPT}[i]} \\
  \alpha_j &\sim \mathcal{N}(0, 1.5), j \in \{1, 2\} \\
  \delta_k &\sim \mathcal{N}(0, 1.5), k \in \{1, \dots, 6\}
\end{align*}
$$

```{r}
ladmit_dept <- dadmit |> 
  mutate(dept = rep(seq_len(6), each = 2)) |> 
  compose_data()
```

```{r}
mod_11_8 <- cmdstan_model(here::here("inst/Stan/m11_8.stan"))
m11_8 <- mod_11_8$sample(
  data = ladmit_dept,
  seed = 1002,
  refresh = 0,
  iter_warmup = 2000L,
  iter_sampling = 2000L
)
```

Now we do the same thing as we did before: calculate the contrasts on both relative and absolute scales. 

```{r}
spread_draws(m11_8, a[gid]) |> 
  mutate(gid = c("male", "female")[gid]) |> 
  pivot_wider(names_from = "gid", values_from = "a") |> 
  transmute(
    diff_a = male - female, 
    diff_p = inv_logit(male) - inv_logit(female)
  ) |> 
  precis()
```

Now we see that there is little to no difference between the groups: the mean estimate for the contrasts are both slightly negative, but not conclusively so. 

We can summarise this as RM does in the book: 

```{r}
UCBadmit |> 
  as_tibble() |> 
  group_by(dept) |> 
  mutate(value = applications / sum(applications)) |> 
  ungroup() |> 
  select(dept, name = applicant.gender, value) |> 
  mutate(across(value, \(x) round(x, digits = 2))) |> 
  pivot_wider() |> 
  inner_join(
    UCBadmit |> 
      as_tibble() |> 
      group_by(dept) |> 
      summarise(across(c(admit, applications), sum)) |> 
      transmute(dept, admit_rate = round(admit / applications, digits = 2)), 
    by = "dept"
  )
```

The two departments with the highest admission rates, A and B, are overwhelmingly male. That inflates their admission rates overall, and led to the inferences in model `m11_7`. 

RM then discusses the DAG, which I'll skip. Suffice to say that even if department is a confound we get a better model with it than without. 

RM also touched on the fact that `m11_8` is over-parameterised, in the sense that there is some redundancy. This is a problem in frequentist statistics that needs to be handled (e.g. with ridge regression), but here it is fine. It leads to inflated standard deviation on our parameter estimates, but the posterior predictions are good. The reason to do it this way is that it makes setting priors much easier: whereas if we set one gender as the baseline then setting a prior on the difference would be difficult and harder to interpret. 

## Poisson regression

RM gives his set-up to the Poisson as a sort of extension to the binomial when the number of trials, $N$, is either unknown or uncountably large. The mean of a $\operatorname{Binomial}(N, p)$ random variable is $Np$, and the variance is $Np(1 - p)$. If $N$ is fairly large and $p$ is very small then these will be about the same. 

RM uses an example of a monastery writing out manuscripts. There are 1,000 monks and on any given day about 1 of them will complete a manuscript. So a reasonable question is: what is the distribution of the number of manuscripts completed each day?

$$
\begin{align*}
  y &\sim \operatorname{Binomial}(N, p) \\
  N &= 1000 \\
  p &= 1/1000 \\
  \mathbb{E}(y) &= 1 \\
  \operatorname{Var}(y) &= 1000 \times 1/1000 \times (999/1000) \approx 1
\end{align*}
$$

As always, we can check with a quick simulation. 

```{r}
set.seed(1110)
dmonk <- tibble(y = rbinom(1e5, 1e3, 1 / 1e3))
head(dmonk)
dmonk |> 
  summarise(mean = mean(y), variance = var_pop(y))
```

To set up a model then we need only one parameter, $\lambda$, which serves as mean and variance. 

$$
\begin{align*}
  y_i &\sim \operatorname{Poisson}(\lambda_i) \\
  \ln(\lambda_i) &= \alpha + \beta(x_i - \bar{x})
\end{align*}
$$

The link function is the natural log, which ensures that $\lambda$ is always positive. This implies though that $\lambda_i = e^{\alpha + \beta(x_i - \bar{x})}$, i.e. an exponential relationship between the predictors and $\lambda$. This may be reasonable at times, but can quickly lead to unreasonable inferences. This makes prior and posterior predictive checks even more important. 

### Example: Oceanic tool complexity

This example uses data on tools in Oceania societies. There are three hypotheses that we want to test: 

1. Larger populations will develop and sustain more complex sets of tools. 
2. More contact with other populations will have a similar effect to 1, essentially creating a larger population in which innovation happens. 
3. The impact of population is moderated by contact. In other words, we expect the interaction effect between these to be positive. 

```{r}
data(Kline)
Kline |> 
  as_tibble()
```

So the model might be: 

$$
\begin{align*}
  Y_i &\sim \operatorname{Poisson}(\lambda_i) \\
  \ln(\lambda_i) &= \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]}\ln(P_i)
\end{align*}
$$

Here $P_i$ is the population. So now we must set priors on $\alpha_j$ and $\beta_j$. The parameters pass through the link function, which means the relationship with the outcome will not be linear. 

```{r}
tibble(grp = c("a", "b"), meanlog = c(0, 3), sdlog = c(10, 0.5)) |> 
  expand(
    nesting(grp, meanlog, sdlog), 
    x = seq(0, 100, by = 0.5)
  ) |> 
  mutate(y = dlnorm(x, meanlog, sdlog)) |> 
  ggplot(aes(x, y, colour = grp)) + 
  geom_path(linewidth = 1) + 
  geom_text(
    aes(label = label), 
    data = tibble(
      grp = c("a", "b"), 
      x = c(14, 35), 
      y = c(0.06, 0.035), 
      label = c(
        "a ~ dnorm(0, 10)", 
        "a ~ dnorm(3, 0.5)"
      )
    )
  ) + 
  scale_x_continuous(
    "mean number of tools", 
    breaks = seq(0, 100, by = 20)
  ) + 
  scale_y_continuous("Density") + 
  scale_colour_manual(values = c("black", "steelblue")) + 
  theme(legend.position = "none")
```

What's wrong here? The black curve (corresponding to a wide, flat prior on $\alpha$) has a peak near zero and then a long tail to the right. It's hard to tell how much longer from this plot, but we can quickly simulate the mean of such a lognormal variable. 

```{r}
set.seed(1236)
mean(exp(rnorm(1e4, 0, 10))) |> 
  format(big.mark = ",")
```

The exact value of the mean of a lognormal variable is $e^{\mu + {\sigma^2}/2}$, which is $e^{50}$. This is extremely silly. 

The blue line is the better prior: it re-centres the distribution on $\alpha$ at 3, but the more important change is in the reduction of the scale parameter. This drastically reduces the spread of possible values once it passes through the link function, since the mean of the lognormal variable increases at $O(e^{\sigma^2})$.

```{r}
set.seed(1236)
mean(exp(rnorm(1e4, 3, 0.5))) |> 
  round(digits = 2)
```

This is much more sensible. The maths behind this make sense: inverting the log link maps the interval $(-\infty, 0]$ to the interval $(0, 1]$, and the interval $(0, \infty)$ to the interval $(1, \infty)$. So if a prior is centred at zero then half the probability (on the outcome scale) will be on $(0, 1]$. This makes little sense here, so shifting the location to the right and reducing the scale gives us better answers.

Our updated model is: 

$$
\begin{align*}
  Y_i &\sim \operatorname{Poisson}(\lambda_i) \\
  \ln(\lambda_i) &= \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]}\ln(P_i) \\
  \alpha_j &\sim \mathcal{N}(3, 0.5)
\end{align*}
$$

We can repeat this for the $\beta$ parameter, i.e. the coefficient on log-population, using the prior on $\alpha$ chosen. We'll plot the bad prior and the good in one go. 

```{r}
set.seed(1251)
fig11_8_pt1 <- tibble(
  grp = seq_len(100), 
  a = rnorm(100, 3, 0.5), 
  `beta%~%Normal(0*', '*10)` = rnorm(100, 0, 10), 
  `beta%~%Normal(0*', '*0.2)` = rnorm(100, 0, 0.2)
) |> 
  pivot_longer(contains("beta"), names_to = "prior", values_to = "b") |> 
  expand(
    nesting(grp, a, b, prior), 
    x = seq(-2, 2, length.out = 101)
  ) |> 
  mutate(y = exp(a + b*x)) |> 
  ggplot(aes(x = x, y = y, group = grp)) + 
  geom_path(alpha = 0.5) + 
  coord_cartesian(ylim = c(0, 100)) + 
  facet_wrap(~ fct_rev(prior), labeller = label_parsed) + 
  labs(
    x = "log population (std)", 
    y = "total tools"
  )
fig11_8_pt1
```

The left-hand plot shows some crazy relationships between the predictor and the outcome. In contrast the right-hand plot is more restrained. 

We can do something similar but on the unstandardised predictor scale. Everything above on the x axis refers to a standardised variable, but that's hard to think about. We do this for the better prior only: log and natural population scales. 

```{r}
set.seed(1320)
tibble(
  grp = seq_len(100), 
  a = rnorm(100, 3, 0.5), 
  b = rnorm(100, 0, 0.2)
) |> 
  expand(
    nesting(grp, a, b), 
    x = seq(log(100), log(200000), length.out = 101)
  ) |> 
  mutate(y = exp(a + b*x), pop = exp(x)) |> 
  transmute(
    grp, 
    log_pop = x, 
    pop, 
    y
  ) |> 
  pivot_longer(contains("pop"), names_to = "scale", values_to = "x") |> 
  mutate(
    scale = factor(if_else(scale == "log_pop", "log population", "population"))
  ) |> 
  ggplot(aes(x, y, group = grp)) + 
  geom_path(alpha = 0.5) + 
  coord_cartesian(ylim = c(0, 500)) + 
  scale_x_continuous(NULL, labels = scales::label_comma()) + 
  facet_wrap(~ scale, scales = "free_x")
```

The right-hand plot shows why using the log of a predictor can be useful: it implies 'diminishing returns' for the predictor as it grows, so that all of the lines flatten. This constrains our model and prevents us getting results that are too extreme. 

Finally we have a full model: 

$$
\begin{align*}
  Y_i &\sim \operatorname{Poisson}(\lambda_i) \\
  \ln(\lambda_i) &= \alpha_{\text{CID}[i]} + \beta_{\text{CID}[i]}\ln(P_i) \\
  \alpha_j &\sim \mathcal{N}(3, 0.5) \\
  \beta_j &\sim \mathcal{N}(0, 0.2)
\end{align*}
$$

Now we produce an intercept-only model and one with an interaction. 

```{r}
dkline <- Kline |>
  as_tibble()
lkline <- dkline |> 
  transmute(
    Y = total_tools, 
    P = as.double(standardize(log(population))), 
    cid = if_else(contact == "high", 2L, 1L)
  ) |> 
  compose_data()
```

```{r}
mod_11_9 <- cmdstan_model(here::here("inst/Stan/m11_9.stan"))
m11_9 <- mod_11_9$sample(
  data = lkline,
  seed = 1344,
  refresh = 0
)
```

```{r}
mod_11_10 <- cmdstan_model(here::here("inst/Stan/m11_10.stan"))
m11_10 <- mod_11_10$sample(
  data = lkline,
  seed = 1350,
  refresh = 0
)
```

```{r}
set.seed(1518)
compare_cmdstan(m11_9, m11_10)
```

There are two things to note: 

1. The model with the interaction performs better, which is as expected. 
2. The model with fewer parameters (i.e. the intercept-only model) has a higher `pPSIS` score (i.e. effective number of parameters). This may seem counterintuitive, but only if we are thinking about it in the most basic frequentist setup (i.e. simple linear regression with flat priors). In that case there would be a direct relationship between model complexity and the number of parameters. However in this case the distribution is bounded, so parameter values near the boundary produce less overfitting than those far from it. This also applies to the distribution of the data: counts near zero are harder to overfit. So `pPSIS` as a measure of model complexity/overfitting risk depends on the structure of the model and the data. 

```{r}
set.seed(1908)
P_seq <- seq(-1.4, 3, length.out = 100)
full_kline <- bind_cols(
  dkline, 
  lkline |> 
    as_tibble() |> 
    select(-n) |> 
    mutate(across(everything(), as.double))
) |> 
  mutate(
    k = psis_with_relative_eff(m11_10), 
    P_natural = exp(P * 1.53 + 9)
  )
d_fig_11_9 <- m11_10 |> 
  spread_draws(a[i], b[i]) |> 
  select(.iteration, a, b, cid = i) |> 
  expand(nesting(.iteration, a, b), P = P_seq) |> 
  mutate(value = exp(a + b * P)) |> 
  group_by(P, cid) |> 
  reframe_mean_PI() |> 
  pivot_wider() |> 
  rename(lambda = mean) |> 
  mutate(
    cid = if_else(cid == 1L, "low", "high") |> 
      factor(levels = c("low", "high")), 
    P_natural = exp(P * 1.53 + 9)
  )

list(
  left = d_fig_11_9 |> 
    ggplot(aes(P)) + 
    geom_line(aes(y = lambda, linetype = cid)) + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper, group = cid), 
      colour = "grey60", 
      alpha = 0.4
    ) + 
    geom_point(
      aes(y = total_tools, shape = contact, size = k), 
      data = full_kline, 
      colour = "midnightblue"
    ) + 
    geom_text_repel(
      aes(x = P, y = total_tools, label = label), 
      data = full_kline |> 
        filter(culture %in% c("Yap", "Tonga", "Trobriand", "Hawaii")) |> 
        transmute(
          label = sprintf("%s (%.2f)", as.character(culture), k), 
          P, 
          total_tools
        ), 
      nudge_y = 4
    ) + 
    scale_linetype_manual(values = c(2, 1)) + 
    scale_shape_manual(values = c(16, 1)) + 
    scale_x_continuous(
      "log population (std)", 
      breaks = seq(-1, 2)
    ) + 
    scale_y_continuous(
      "total tools", 
      breaks = seq(0, 60, by = 20)
    ) + 
    coord_cartesian(
      xlim = c(-1.4, 2.5), 
      ylim = c(0, 75)
    ) + 
    labs(size = "Pareto k"),
  right = d_fig_11_9 |> 
    ggplot(aes(P_natural)) + 
    geom_line(aes(y = lambda, linetype = cid)) + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper, group = cid), 
      colour = "grey60", 
      alpha = 0.4
    ) + 
    geom_point(
      aes(y = total_tools, shape = contact, size = k), 
      data = full_kline, 
      colour = "midnightblue"
    ) + 
    scale_linetype_manual(values = c(2, 1)) + 
    scale_shape_manual(values = c(16, 1)) + 
    scale_x_continuous(
      "population", 
      breaks = c(0, 50000, 150000, 250000), 
      labels = scales::label_comma()
    ) + 
    scale_y_continuous(
      "total tools", 
      breaks = seq(0, 60, by = 20)
    ) + 
    coord_cartesian(
      xlim = c(0, 290000),
      ylim = c(0, 75)
    ) + 
    labs(size = "Pareto k")
) |> 
  wrap_plots(guides = "collect")
```

Hawaii is moving the needle: it is far larger than any other and has far more tools. So we see that the low-contact line has to bend towards Hawaii: at the left of the distribution it is below the high-contact line, but then it flips. Also the posterior interval for the high-contact cultures is enormous, because we have no data points with a population larger than Tonga ($P = 0.5$). 

Then RM covers an alternative model. Why bother with this? Let's look at the previous model again and ask a question about a counterfactual Hawaii with high contact. Such an island should have more tools: does our model predict this?

```{r}
m11_10 |> 
  spread_draws(a[i], b[i]) |> 
  select(.iteration, .chain, a, b, cid = i) |> 
  mutate(
    P = full_kline |> 
      filter(culture == "Hawaii") |> 
      pull(P)
  ) |> 
  mutate(
    lambda = exp(a + b * P), 
    cid = if_else(cid == 1L, "low", "high") |> 
      factor(levels = c("low", "high"))
  ) |> 
  ungroup() |> 
  select(.iteration, .chain, name = cid, value = lambda) |>
  pivot_wider() |> 
  transmute(lambda = high - low) |> 
  ggplot(aes(x = lambda)) + 
  geom_density(fill = "steelblue", colour = "steelblue", alpha = 0.5) + 
  geom_vline(xintercept = 0, colour = "grey40") + 
  geom_vline(
    aes(xintercept = x), 
    data = . %>%
      reframe(tibble(x = c(mean(lambda), HPDI(lambda)))), 
    linetype = 2, 
    colour = "grey50"
  ) + 
  labs(
    title = "Contrast between high- and low-contact cultures", 
    subtitle = "Population held constant at that of Hawaii", 
    caption = "Dotted lines are mean and 89% HPDI of contrast", 
    x = NULL, 
    y = NULL
  ) + 
  theme(
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank()
  )
```

Our model is quite uncertain, with a huge 89% HPDI for the difference. But more often than not it thinks that the high-contact cultures will have fewer tools. 

So we try to improve on that: call `m11_10` the _geocentric model_, and the next one a _scientific model_. This is based on a scientific model of how population size related to tool kit complexity. We start by saying that the change in tools at each step in time is: 

$$
\begin{align*}
  \Delta Y &= \alpha P^{\beta} - \gamma Y
\end{align*}
$$

In other words: the change in the numbers of tools is a function of $P$, where we subtract a multiple of the current number of tools ($\gamma Y$) to reflect the fact that tool changes should decrease over time. We want the equilibrium state, so set $\Delta Y$ to zero and solve for $Y$. That gives: 

$$
\begin{align*}
  \hat{Y} & = \frac{\alpha P^{\beta}}{\gamma}
\end{align*}
$$

Now we embed that in a Poisson model. 

$$
\begin{align*}
  Y_i &\sim \operatorname{Poisson}(\lambda_i) \\
  \lambda_i &= \frac{\alpha_{\text{[CID]}} P_i^{\beta_{\text{[CID]}}}}{\gamma} \\
  \log(\alpha_j) &\sim \mathcal{N}(1, 1) \\
  \beta_k &\sim \operatorname{Exponential}(1) \\
  \gamma &\sim \operatorname{Exponential}(1)
\end{align*}
$$

We don't use the log link, but we have to constrain the right-hand side of the equation to be positive. We do that by setting priors on $\alpha$ and $\gamma$ that are strictly positive, as above. 

Now we can fit the model. 

```{r}
lkline_sci <- dkline |> 
  transmute(
    Y = total_tools, 
    P = population, 
    cid = 3L - as.integer(contact)
  ) |> 
  compose_data()
```

```{r}
mod_11_11 <- cmdstan_model(here::here("inst/Stan/m11_11.stan"))
m11_11 <- mod_11_11$sample(
  data = lkline_sci,
  seed = 1219,
  refresh = 0
)
```

We can compare the models as RM suggests. 

```{r}
compare_cmdstan(m11_10, m11_11)
```

And now we can recreate the plot from the book. 

```{r}
set.seed(1908)
full_kline_sci <- bind_cols(
  dkline, 
  lkline_sci |> 
    as_tibble() |> 
    select(-n) |> 
    mutate(across(everything(), as.double))
) |> 
  mutate(k = psis_with_relative_eff(m11_11))

d_fig_11_10 <- m11_11 |> 
  spread_draws(a[i], b[i], g) |> 
  ungroup() |>
  select(.iteration, a, b, g, cid = i) |> 
  expand(
    nesting(.iteration, a, b, g, cid), 
    P = seq(0, 280000, length.out = 100)
  ) |> 
  mutate(value = (exp(a) * P^b) / g) |> 
  reframe_mean_PI(.by = c(P, cid)) |> 
  pivot_wider() |> 
  rename(lambda = mean) |> 
  mutate(cid = factor(c("low", "high")[cid], levels = c("low", "high")))

d_fig_11_10 |> 
  ggplot(aes(P)) + 
  geom_line(aes(y = lambda, linetype = cid)) + 
  geom_ribbon(
    aes(ymin = lower, ymax = upper, group = cid), 
    colour = "grey60", 
    alpha = 0.4
  ) + 
  geom_point(
    aes(y = total_tools, shape = contact, size = k), 
    data = full_kline_sci, 
    colour = "midnightblue"
  ) + 
  scale_linetype_manual(values = c(2, 1)) + 
  scale_shape_manual(values = c(16, 1)) + 
  scale_x_continuous(
    "population", 
    breaks = c(0, 50000, 150000, 250000), 
    labels = scales::label_comma()
  ) + 
  scale_y_continuous(
    "total tools", 
    breaks = seq(0, 70, by = 10)
  ) + 
  coord_cartesian(
    xlim = c(0, 300000),
    ylim = c(0, 75)
  ) +
  theme(legend.position = "none")
```

### Negative binomial (gamma-Poisson) models

The Poisson model assumes a constant $\lambda$, but this can underestimate the variation. So instead we can account for _rate heterogeneity_ with a negative binomial or gamma-Poisson model. 

### Example: Exposure and the offset

RM extends the monastery and manuscripts example: now we have one monastery that counts the manuscripts each day and another per week. By thinking of $\lambda$ as the rate per unit time, i.e. the expected number of events $\mu$ per unit time $\tau$, so that $\lambda = \mu / \tau$, we get: 

$$
\begin{align*}
   y_i &\sim \operatorname{Poisson}(\lambda_i) \\
   \log \lambda_i &= \log \frac{\mu_i}{\tau_i} \\
                  &= \log \mu_i - \log \tau_i \\
                  &= \alpha + \beta x_i \\
  \log \mu_i &= \log \tau_i + \alpha + \beta x_i
\end{align*}
$$

If $\tau_i = 1$ then $\log \tau_i = 0$ and we get back to the Poisson model. This $\tau_i$ is fixed for each observation, and is referred to as the offset.

So let's simulate data for our current monastery, for which we have daily data and the true $\lambda = 1.5$; and the new monastery, for which we have weekly data and the true $\lambda = 0.5$. 

```{r}
set.seed(926)
dmonk <- bind_rows(
  tibble(
    y = rpois(30, 1.5), 
    log_tau = log(1L), 
    monastery = 0L
  ), 
  tibble(
    y = rpois(4, 0.5 * 7), 
    log_tau = log(7L), 
    monastery = 1L
  )
)
```

```{r}
mod_11_12 <- cmdstan_model(here::here("inst/Stan/m11_12.stan"))
m11_12 <- mod_11_12$sample(
  data = compose_data(dmonk),
  seed = 909,
  refresh = 0
)
```

```{r}
m11_12 |> 
  spread_draws(a, b) |> 
  transmute(
    old = exp(a), 
    new = exp(a + b)
  ) |> 
  precis()
```

I tried this originally with data created from random seed 900 and got very different results, posterior means for old and new were about 0.85 and 0.45 respectively. So given the sample sizes here the posterior mean estimates aren't telling the whole story. So instead of doing what RM does in the text, let's actually compute the contrast and plot that. 

```{r}
contrast_11_12 <- m11_12 |> 
  spread_draws(a, b) |> 
  transmute(contrast = exp(a) - exp(a + b))

summ_11_12 <- contrast_11_12 |>
  reframe(x = c(mean(contrast), HPDI(contrast)))

contrast_11_12 |> 
  ggplot(aes(contrast)) + 
  geom_density(fill = "steelblue", alpha = 0.5) + 
  geom_vline(
    aes(xintercept = x), 
    data = summ_11_12, 
    linetype = 2, 
    colour = "grey50"
  ) + 
  labs(
    title = "Contrast: old monstery minus new", 
    x = NULL, 
    y = NULL
  ) + 
  theme(
    axis.ticks.y = element_blank(), 
    axis.text.y = element_blank()
  )
```

The contrast is reliably positive, but with a fair bit of variation.

## Multinomial and categorical models

Generalise from the binomial to multiple possible outcomes. RM mentions that there are numerous approaches. 

The conventional choice for the link function is the **multinomial logit**, or **softmax**. 

$$
\operatorname{P}(k | s_1, s_2, \dots, s_K) = 
  \frac{e^{s_k}}{\sum_{i = 1}^K e^{s_i}}
$$

RM says we need $K - 1$ models for $K$ event types, with one of the types acting as a 'pivot'. Importantly the predictors and parameters don't have to be the same in each one. 

There are two basic cases then: 

1. predictors have different values for different values of the outcome; and 
2. parameters are distinct for each value of the outcome. 

### Predictors matched to outcomes

We are modelling choice of career for young people, for which one predictor is the expected income. So we expect the same parameter $\beta_\text{INCOME}$ to appear in each model, multiplying the different values of expected income. 

Start by simulating some data, then building the model. 

```{r}
set.seed(34302)
career <- sample(
  3, 
  size = 500, 
  replace = TRUE, 
  prob = softmax(c(1, 2, 5) * 0.5)
)
career
```

```{r}
income <- c(1, 2, 5)
dcareer <- list(
  n = 500,
  k = 3,
  career = career,
  career_income = income
)
```

```{r}
mod_11_13 <- cmdstan_model(here::here("inst/Stan/m11_13.stan"))
m11_13 <- mod_11_13$sample(
  data = dcareer,
  seed = 1121,
  refresh = 0
)
```

```{r}
precis_cmdstan(m11_13, variables = c("a", "b"))
```

This is quite a different precis than the one that RM gets in the book, which is more like:

```{r}
#| echo: false
tribble(
  ~variable, ~ mean, ~sd,  ~lower, ~upper,
  "a[1]",     0.06,  0.21, -0.31,  0.37,
  "a[2]",    -0.49,  0.38, -1.19,  0.04,
  "b",        0.27,  0.19,  0.02,  0.61
)
```
Our model is now estimating a smaller effect from career (i.e. `b`), and is more confident about it (lower standard deviation on the estimate). But the intercepts for the two non-pivot careers are _much_ lower. Since their incomes are lower we expect them to be chosen less often: this is baked into the simulation, so our model should identify this.

I couldn't replicate RM's results at all. I tried writing out the code exactly as RM did in the book, and even forcing the model to compile with RStan rather than CmdStan. Still I got identical results to mine. So let's try manually sampling from the `precis()` output from the book to approximate the posterior that RM got, simulate the career choices, then compare that to mine.

```{r}
set.seed(1200)
tibble(
  .draw = seq_len(4000),
  a1 = rnorm(4000, 0.06, 0.21),
  a2 = rnorm(4000, -0.49, 0.38),
  b = rnorm(4000, 0.27, 0.19)
) |> 
  pivot_longer(
    starts_with("a"),
    names_to = "i",
    names_prefix = "a",
    names_transform = list(i = as.integer),
    values_to = "a"
  ) |> 
  arrange(.draw, i) |> 
  mutate(s = a + (b * i)) |> 
  group_by(.draw) |> 
  summarise(career_RM = sample(3, size = 1, prob = softmax(c(s, 0)))) |> 
  inner_join(
    m11_13 |> 
      spread_draws(a[i], b) |> 
      mutate(s = a + (b * i)) |> 
      ungroup() |> 
      arrange(.draw, i) |> 
      group_by(.draw) |> 
      summarise(career_mine = sample(3, size = 1, prob = softmax(c(s, 0)))),
    by = join_by(.draw)
  ) |> 
  pivot_longer(
    starts_with("career"),
    names_prefix = "career_",
    values_to = "career"
  ) |> 
  count(name, career) |>
  group_by(name) |>
  mutate(pct = n / sum(n)) |> 
  mutate(label = scales::label_percent(accuracy = 1)(pct)) |> 
  ggplot(aes(x = career, y = pct, fill = name)) + 
  geom_col(position = position_dodge()) + 
  geom_label_repel(
    aes(x = career, y = pct, label = label),
    inherit.aes = FALSE
  ) +
  scale_fill_viridis_d(option = "E") + 
  scale_y_continuous(NULL, labels = scales::label_percent()) + 
  theme(legend.position = "bottom") +
  labs(title = "Comparing posterior samples for career choice model")
```

Something seems very off here. Sampling using RM's `precis()` results gives us exactly the opposite results to those we'd expect: career 1 most popular, followed by 2, and finally 3. We can see this from the precis: a mean estimate of 0.06 for the intercept plus a positive effect for income on choice (i.e. the positive estimate on b) means that the score, $s$, for career 1 is greater than zero. But career 3 was chosen as the pivot and has $s = 0$ hard-coded into the model. Compare this with the proportions of each career in the original data:

```{r}
table(career) / length(career)
```

This is something much closer to what I get from my model.

[Solomon Kurz goes into depth](https://bookdown.org/content/4857/god-spiked-the-integers.html#predictors-matched-to-outcomes.) on this, although in his case it's with {brms}. The results are similar though, and he dives into the guts of these models nicely. I'll skip that level of detail here, and move on to the counterfactual that RM poses: how much does the probability of choosing career 2 change if we double its income?

```{r}
map(
  seq_len(2),
  ~ m11_13 |> 
    spread_draws(a[i], b) |> 
    mutate(s = a + (b * income[i] * .x)) |> 
    ungroup() |> 
    select(i, .chain, .iteration, .draw, s) |> 
    pivot_wider(
      names_from = i,
      names_prefix = "career_",
      values_from = s
    ) |> 
    mutate(career_3 = 0) |> 
    select(starts_with("career")) |> 
    as.matrix() |> 
    apply(1, softmax) |> 
    t() |> 
    as_tibble(.name_repair = ~ str_c("p", 1:3)) |> 
    rowid_to_column(".draw") |> 
    add_column(i = .x, .before = 1)
) |> 
  list_rbind() |> 
  pivot_longer(starts_with("p")) |> 
  pivot_wider(
    names_from = i,
    names_prefix = "i"
  ) |> 
  transmute(.draw, name, value = i2 - i1) |> 
  pivot_wider() |> 
  select(-.draw) |> 
  precis()
```

We get an average 4% increase in the probability of choosing career 2 after doubling its income, with a roughly equal decrease for career 3.

### Predictors matched to observations

Now we change things so that each observed outcome has unique predictor values. Sticking with career choices: now we want to model career choice based on the person's family income. So the predictor variable has the same value in every linear model

```{r}
set.seed(1423)
dcareer_family <- tibble(family_income = runif(500)) |> 
  mutate(
    career = map_int(
      family_income,
      function(x) {
        p <- softmax((0.5 * seq_len(3) + c(-2, 0, 2) * x))
        sample(3, size = 1, prob = p)
      }
    )
  ) |> 
  compose_data() |> 
  c(k = 3)
```

```{r}
mod_11_14 <- cmdstan_model(here::here("inst/Stan/m11_14.stan"))
m11_14 <- mod_11_14$sample(
  data = dcareer_family,
  seed = 1430,
  refresh = 0
)
```

```{r}
precis_cmdstan(m11_14, variables = c("a", "b"))
```

These are again different results to those RM gets in the book, which are more like:

```{r}
#| echo: false
tribble(
  ~variable, ~ mean, ~sd, ~lower, ~upper,
  "a[1]", -1.41, 0.28, -1.88, -0.97,
  "a[2]", -0.64, 0.20, -0.96, -0.33,
  "b[1]", -2.72, 0.60, -3.69, -1.79,
  "b[2]", -1.72, 0.39, -2.32, -1.10
)
```

The differences aren't quite so stark this time, e.g. all of the mean estimates are negative, and the estimates for intercept and coefficient for career 1 are quite similar. RM says that "computing implied predictions is the safest way to interpret these models". So let's do that.

```{r}
set.seed(1200)
tibble(
  i = c(1, 2, 1, 2),
  name = c("a", "a", "b", "b"),
  mean = c(-1.41, -0.64, -2.72, -1.72),
  sd = c(0.28, 0.2, 0.6, 0.39)
) |> 
  transmute(
    i,
    name,
    draws = map2(
      mean,
      sd,
      ~ tibble(.draw = seq_len(4000), value = rnorm(4000, .x, .y))
    )
  ) |> 
  unnest(draws) |> 
  pivot_wider(names_from = c(name, i)) |> 
  mutate(a_3 = 0, b_3 = 0) |> 
  mutate(family_income = runif(4000)) |> 
  pivot_longer(
    matches("^a|^b"),
    names_to = c("name", "i"),
    names_sep = "_",
    names_transform = list(i = as.integer)
  ) |> 
  pivot_wider() |> 
  transmute(.draw, i, s = a + (b * family_income)) |> 
  arrange(.draw, i) |> 
  group_by(.draw) |> 
  mutate(p = softmax(s)) |> 
  ungroup() |> 
  select(.draw, name = i, value = p) |> 
  pivot_wider(names_prefix = "p") |> 
  select(-.draw) |> 
  precis()
```

```{r}
m11_14 |> 
  gather_draws(a[i], b[i]) |> 
  ungroup() |> 
  pivot_wider(names_from = c(.variable, i), values_from = .value) |> 
  select(-c(.chain, .iteration)) |> 
  mutate(a_3 = 0, b_3 = 0) |> 
  mutate(family_income = runif(4000)) |> 
  pivot_longer(
    matches("^a|^b"),
    names_to = c("name", "i"),
    names_sep = "_",
    names_transform = list(i = as.integer)
  ) |> 
  pivot_wider() |> 
  transmute(.draw, i, s = a + (b * family_income)) |> 
  arrange(.draw, i) |> 
  group_by(.draw) |> 
  mutate(p = softmax(s)) |> 
  ungroup() |> 
  select(.draw, name = i, value = p) |> 
  pivot_wider(names_prefix = "p") |> 
  select(-.draw) |> 
  precis()
```

We get almost identical posterior distributions on the probability scale.

### Multinomial in disguise as Poisson

Now we factor apart the multinomial model into a series of Poisson likelihoods. RM does the implementation first (albeit with a binary outcome) and then delves into the maths, so I'll do the same.

We return to the Berkeley admissions data.

```{r}
data(UCBadmit)
UCBadmit
```

```{r}
mod_11_15_binom <- cmdstan_model(here::here("inst/Stan/m11_15_binom.stan"))
m11_15_binom <- mod_11_15_binom$sample(
  data = compose_data(UCBadmit |> select(admit, applications)),
  seed = 1504,
  refresh = 0
)
```

```{r}
mod_11_15_pois <- cmdstan_model(here::here("inst/Stan/m11_15_pois.stan"))
m11_15_pois <- mod_11_15_pois$sample(
  data = compose_data(UCBadmit |> select(admit, rej = reject)),
  seed = 1505,
  refresh = 0
)
```

Let's compare the posterior predictions for probability of admission from each of these.

```{r}
set.seed(835)
m11_15_binom |>
  spread_draws(p) |> 
  select(.draw, p_admit_binom = p) |> 
  inner_join(
    m11_15_pois |> 
      spread_draws(lambda[i]) |> 
      ungroup() |> 
      select(.draw, name = i, value = lambda) |> 
      pivot_wider(names_prefix = "lambda") |> 
      transmute(.draw, p_admit_pois = lambda1 / (lambda1 + lambda2)),
    by = ".draw"
  ) |> 
  pivot_longer(
    matches("admit"),
    names_to = "model",
    names_prefix = "p_admit_",
    values_to = "p"
  ) |> 
  ggplot(aes(x = p, fill = model)) + 
  geom_density(alpha = 0.3) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "bottom"
  ) +
  labs(y = NULL)
```

We get near-identical inferences, as we should.

## Summary

RM wraps things up including a reminder that:

> It is important to never convert counts to proportions before analysis, because doing so destroys information about sample size.

Wise words. This was quite a big chapter, covered a lot of ground.

## Practice

### Easy

11E1.

$$
\begin{align*}
p = 0.35 \implies \operatorname{log-odds} &= \log\bigg(\frac{0.35}{0.65}\bigg) \\
 &\approx -0.62
\end{align*}
$$

11E2.

$$
\begin{align*}
\operatorname{log-odds} = 3.2 \implies p &= \frac{1}{1 + e^{-3.2}} \\
 &\approx 0.96
\end{align*}
$$

11E3.

The coefficient of 1.7 implies that for every unit increase in the predictor the log-odds will increase by 1.7.

11E4.

In Poisson regression we may have different time periods for different measurements. We keep $\lambda$ as our rate parameter, but now for each observation $\lambda = \mu / \tau$ ($\mu$ is the expected number of events per unit of time, $\tau$ is the length of time for the observation). When using the log link function the right hand side is just a difference of logs, so that $\log\tau$ is just a constant: the offset.

We can cook up a simple example. Suppose we are counting how many cars are passing over a bridge. In the morning our first observer counts over periods of 15 minutes, but in the afternoon the second observer counts every hour. Then for all of the morning observations the offset would be $\log(15)$, and in the afternoon $\log(60)$.

### Medium

11M1.

The likelihood changes because the binomial coefficient disappears in the disaggregated case. The binomial likelihood of seeing $y$ successes in $n$ trials with probability $p$ is:

$$
\operatorname{P}(y | n, p) = {n \choose y} p^y (1 - p)^{n - y}
$$

When $n = 1$, i.e. the disaggregated format, then we have the Bernoulli likelihood (which is just the binomial likelihood with $n = 1$).

$$
\begin{align*}
\operatorname{P}(y | n = 1, p) &= {1 \choose y} p^y (1 - p)^{n - y} \\
&= p^y (1 - p)^{n - y}
\end{align*}
$$

This doesn't affect the inference because that coefficient represents the number of ways to rearrange the successes within the trials. As mentioned in the notes earlier: the aggregated case implicitly treats all of the observations in each aggregation as a single unit. This may make sense sometimes, e.g. in the Berkeley admissions data. But in general RM advises using the disaggregated version.

11M2.

In a Poisson regression a coefficient of 1.7 on a predictor means that for every unit increase on the predictor the log of the rate, $\lambda$, increases by 1.7. In turn that means $\lambda$ increases by $e^{1.7} \approx 5.5$.

11M3.

The logit link function has two important benefits for binomial GLMs.

1. It allows us to map the domain of the predictors, $\mathbb{R}$, to the interval $[0, 1]$, which is appropriate for probabilities.
2. The centre of each domain is preserved: $\operatorname{logit}(0.5) = 0$.

11M4.

The log link constrains $\lambda$ to be positive, which is required for a Poisson model.

11M5.

The simplest such model would be:

$$
\begin{align*}
y &\sim \operatorname{Poisson}(\lambda) \\
\operatorname{logit}(\lambda) &= \alpha \\
\log\frac{\lambda}{1 - \lambda} &= \alpha \\
\end{align*}
$$

I was struggling to think of a situation where this would be useful, since this will only be defined for $\lambda \in (0, 1)$. That implies that $\lambda$ is a probability, but in that case why wouldn't you just use Bernoulli instead of Poisson? Then I read [Wake Thompson's solutions](https://sr2-solutions.wjakethompson.com/generalized-linear-models.html#chapter-11) where he points out that the 1 in the denominator could just be a maximum count, $M$. So the model becomes:

$$
\begin{align*}
y &\sim \operatorname{Poisson}(\mu) \\
\operatorname{logit}(\mu) &= \alpha \\
\log\frac{\mu}{M - \mu} &= \alpha \\
\end{align*}
$$

This could be used in a situation where $M$ is very large, so the Poisson regression would be a simpler model, a bit like Joe Blitzstein's course using the Poisson to approximate the birthday problem (and then generalise it to more than two people sharing a birthday).

11M6.

The [maximum entropy constraints](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution) for each are very similar:

- First Binomial constraint is that $\operatorname{E}(X) = \mu$, i.e. a constant (although in Bayesian modelling we typically model a distribution of plausible values for $\mu$). First Poisson constraint is basically identical, except we would typically use $\lambda$ for the parameter.
- The second constraint is different: for the binomial it must be an n-generalised binomial distribution, for the Poisson a $\infty\text{-generalised binomial distribution}$.

This makes sense given what RM showed in the chapter: the Poisson is the limiting case of the binomial as $n \to \infty$. So while the two are similar they aren't identical.

11M7.

```{r}
quap11_4 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = chimp_list
)
```

```{r}
precis(quap11_4, depth = 2) |> 
  as_tibble(rownames = "variable") |> 
  select(variable, mean_quap = mean) |> 
  inner_join(
    precis_cmdstan(m11_4, variables = c("a", "b")) |> 
      select(variable, mean_HMC = mean),
    by = "variable"
  ) |> 
  pivot_longer(
    matches("mean"),
    names_to = "model",
    names_prefix = "mean_"
  ) |> 
  ggplot(aes(x = value, y = variable, colour = model)) + 
  geom_point(alpha = 0.6)
```

The posterior means are very similar except for a[2]. Let's plot the posterior distribution for each model for that parameter.

```{r}
set.seed(1213)
m11_4 |> 
  spread_draws(a[i]) |> 
  filter(i == 2) |> 
  ungroup() |> 
  select(.draw, a2_HMC = a) |> 
  mutate(a2_quap = extract.samples(quap11_4, n = 4000)[["a"]][, 2]) |> 
  pivot_longer(
    matches("a2"),
    names_to = "model",
    names_prefix = "a2_",
    values_to = "a2"
  ) |> 
  ggplot(aes(x = a2, fill = model)) +
  geom_density(alpha = 0.4)
```

The quap model is constrained to be a normal distribution, i.e. symmetrical. However the HMC model has a longer tail to the right, and therefore gives higher estimates on average.

Now let's repeat with the wider priors on the actor intercepts.

```{r}
quap11_4_wide <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 10),
    b[treatment] ~ dnorm(0, 0.5)
  ),
  data = chimp_list
)
```

```{r}
mod_11_4_wide <- cmdstan_model(here::here("inst/Stan/m11_4_wide.stan"))
m11_4_wide <- mod_11_4_wide$sample(
  data = chimp_list,
  seed = 1223,
  refresh = 0
)
```

First just the mean estimates.

```{r}
precis(quap11_4_wide, depth = 2) |> 
  as_tibble(rownames = "variable") |> 
  select(variable, mean_quap = mean) |> 
  inner_join(
    precis_cmdstan(m11_4_wide, variables = c("a", "b")) |> 
      select(variable, mean_HMC = mean),
    by = "variable"
  ) |> 
  pivot_longer(
    matches("mean"),
    names_to = "model",
    names_prefix = "mean_"
  ) |> 
  ggplot(aes(x = value, y = variable, colour = model)) + 
  geom_point(alpha = 0.6)
```

Now the difference on a[2] is enormous, while the rest are still very similar. Let's compare just that parameter.

```{r}
set.seed(1232)
m11_4_wide |> 
  spread_draws(a[i]) |> 
  filter(i == 2) |> 
  ungroup() |> 
  select(.draw, a2_HMC = a) |> 
  mutate(a2_quap = extract.samples(quap11_4_wide, n = 4000)[["a"]][, 2]) |> 
  pivot_longer(
    matches("a2"),
    names_to = "model",
    names_prefix = "a2_",
    values_to = "a2"
  ) |> 
  ggplot(aes(x = a2, fill = model)) +
  geom_density(alpha = 0.4)
```

The HMC model has an even stronger right-skew than it did previously, although the locations of both distributions have moved a long way to the right. It's worth reminding ourselves what chimp number 2's results actually look like.

```{r}
dchimp |> 
  filter(actor == 2L) |> 
  count(pulled_left)
```

Chimp no. 2 pulled left every single time. Since a[2] is the actor-level intercept, we can interpret it as "conditional on knowing the treatment, how much of an increase in the log-odds can be explained by the experiment subject being actor 2?" The data will support about as high a number for a[2] as our priors will allow, which is why the HMC model (having no constraints on the distribution being symmetrical) gives a higher mean estimate.

11M8.

The models are already compiled, we just need to resample with new data. We can skip the intercept-only model and just do the geocentric model with the interaction.

```{r}
dkline_nohawaii <- dkline |> 
  filter(culture != "Hawaii")
lkline_nohawaii <- dkline_nohawaii |> 
  transmute(
    Y = total_tools, 
    P = as.double(standardize(log(population))), 
    cid = if_else(contact == "high", 2L, 1L)
  ) |> 
  compose_data()
```

```{r}
m11_10_nohawaii <- mod_11_10$sample(
  data = lkline_nohawaii,
  seed = 1311,
  refresh = 0
)
```

And finally the scientific model.

```{r}
lkline_sci_nohawaii <- dkline_nohawaii |> 
  transmute(
    Y = total_tools, 
    P = population, 
    cid = 3L - as.integer(contact)
  ) |> 
  compose_data()
```

```{r}
m11_11_nohawaii <- mod_11_11$sample(
  data = lkline_sci_nohawaii,
  seed = 1312,
  refresh = 0
)
```

First let's compare the models using PSIS.

```{r}
compare_cmdstan(m11_10_nohawaii, m11_11_nohawaii)
```

The scientific model is still the best, and still only about one standard error better than the interaction model. Let's plot both of those.

```{r}
set.seed(1908)
P_seq <- seq(-1.4, 3, length.out = 100)
full_kline_nohawaii <- bind_cols(
  dkline_nohawaii, 
  lkline_nohawaii |> 
    as_tibble() |> 
    select(-n) |> 
    mutate(across(everything(), as.double))
) |> 
  mutate(
    k = psis_with_relative_eff(m11_10_nohawaii), 
    P_natural = exp(P * 1.53 + 9)
  )
d_fig_11_9_nohawaii <- m11_10_nohawaii |> 
  spread_draws(a[i], b[i]) |> 
  select(.iteration, a, b, cid = i) |> 
  expand(nesting(.iteration, a, b), P = P_seq) |> 
  mutate(value = exp(a + b * P)) |> 
  group_by(P, cid) |> 
  reframe_mean_PI() |> 
  pivot_wider() |> 
  rename(lambda = mean) |> 
  mutate(
    cid = if_else(cid == 1L, "low", "high") |> 
      factor(levels = c("low", "high")), 
    P_natural = exp(P * 1.53 + 9)
  )

full_kline_sci_nohawaii <- bind_cols(
  dkline_nohawaii, 
  lkline_sci_nohawaii |> 
    as_tibble() |> 
    select(-n) |> 
    mutate(across(everything(), as.double))
) |> 
  mutate(k = psis_with_relative_eff(m11_11_nohawaii))

d_fig_11_10_nohawaii <- m11_11_nohawaii |> 
  spread_draws(a[i], b[i], g) |> 
  ungroup() |>
  select(.iteration, a, b, g, cid = i) |> 
  expand(
    nesting(.iteration, a, b, g, cid), 
    P = seq(0, 280000, length.out = 100)
  ) |> 
  mutate(value = (exp(a) * P^b) / g) |> 
  reframe_mean_PI(.by = c(P, cid)) |> 
  pivot_wider() |> 
  rename(lambda = mean) |> 
  mutate(cid = factor(c("low", "high")[cid], levels = c("low", "high")))

list(
  geo = d_fig_11_9_nohawaii |> 
    ggplot(aes(P_natural)) + 
    geom_line(aes(y = lambda, linetype = cid)) + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper, group = cid), 
      colour = "grey60", 
      alpha = 0.4
    ) + 
    geom_point(
      aes(y = total_tools, shape = contact, size = k), 
      data = full_kline_nohawaii, 
      colour = "midnightblue"
    ) + 
    scale_linetype_manual(values = c(2, 1)) + 
    scale_shape_manual(values = c(16, 1)) + 
    scale_x_continuous(
      "population", 
      breaks = c(0, 50000, 150000, 250000), 
      labels = scales::label_comma()
    ) + 
    scale_y_continuous(
      "total tools", 
      breaks = seq(0, 60, by = 20)
    ) + 
    coord_cartesian(
      xlim = c(0, 290000),
      ylim = c(0, 75)
    ) + 
    theme(legend.position = "none") +
    labs(subtitle = "Geocentric"),
  sci = d_fig_11_10_nohawaii |> 
    ggplot(aes(P)) + 
    geom_line(aes(y = lambda, linetype = cid)) + 
    geom_ribbon(
      aes(ymin = lower, ymax = upper, group = cid), 
      colour = "grey60", 
      alpha = 0.4
    ) + 
    geom_point(
      aes(y = total_tools, shape = contact, size = k), 
      data = full_kline_sci_nohawaii, 
      colour = "midnightblue"
    ) + 
    scale_linetype_manual(values = c(2, 1)) + 
    scale_shape_manual(values = c(16, 1)) + 
    scale_x_continuous(
      "population", 
      breaks = c(0, 50000, 150000, 250000), 
      labels = scales::label_comma()
    ) + 
    scale_y_continuous(
      "total tools", 
      breaks = seq(0, 70, by = 10)
    ) + 
    coord_cartesian(
      xlim = c(0, 300000),
      ylim = c(0, 75)
    ) + 
    theme(legend.position = "none") +
    labs(subtitle = "Scientific")
) |> 
  wrap_plots() +
  plot_annotation(
    title = "Comparing models without Hawaii",
    caption = "Solid line and filled points are high contact, dotted line and empty are low contact.",
    theme = theme(plot.title = element_text(hjust = 0.5))
  )
```

Our models no longer have to bend towards Hawaii, so we get different mean lines and credible intervals. Now the geocentric model is a much better fit: the low-contact mean line is below that for high-contact throughout. Since that was one of the motivations for building the scientific model, this seems an important improvement.

### Hard

11H1.

I didn't actually fit the very simple models, so let's do that now. We can skip the models with the silly priors and just fit the one with a single coefficient for all chimps (but still with one coefficient per treatment).

```{r}
mod_11_3 <- cmdstan_model(here::here("inst/Stan/m11_3.stan"))
m11_3 <- mod_11_3$sample(
  data = chimp_list,
  seed = 1820,
  refresh = 0
)
```

```{r}
hwk_h1_compare <- compare_cmdstan(m11_3, m11_4)
hwk_h1_compare
```

The difference is enormous: the difference in PSIS is about `r round(max(hwk_h1_compare$dPSIS) / hwk_h1_compare$dSE[[2]])` times the size of the standard error. We got a clue about why this might be in question 11M7: chimp number 2 is so unlike the others that having a single `a` coefficient for all chimps gives a terrible fit.

11H2.

```{r}
data(eagles, package = "MASS")
deagles <- eagles |> 
  transmute(
    y,
    sample_size = n,
    P = as.integer(P == "L"),
    V = as.integer(V == "L"),
    A = as.integer(A == "A")
  )
leagles <- compose_data(deagles)
```

```{r}
eagles_quap <- quap(
  alist(
    y ~ dbinom(sample_size, p),
    logit(p) <- a + (bP * P) + (bV * V) + (bA * A),
    a ~ dnorm(0, 1.5),
    c(bP, bV, bA) ~ dnorm(0, 0.5)
  ),
  data = leagles
)
eagles_quap
```

```{r}
mod_11_hwh2 <- cmdstan_model(here::here("inst/Stan/m11_hwh2.stan"))
m11_hwh2 <- mod_11_hwh2$sample(
  data = leagles,
  seed = 1842,
  refresh = 0
)
```

Let's compare the posterior means and 90% interval.

```{r}
(
  (
    eagles_quap |> 
      precis(prob = 0.9) |> 
      as_tibble(rownames = "variable") |>
      rename(q5 = `5%`, q95 = `95%`) |> 
      arrange(variable) |> 
      select(-c(sd, variable)) |> 
      as.matrix()
  ) - (
    m11_hwh2 |> 
      precis_cmdstan() |> 
      filter(variable %in% c("a", "bA", "bP", "bV")) |> 
      arrange(variable) |> 
      select(mean, q5, q95) |> 
      as.matrix()
  )
) |> 
  `rownames<-`(c("a", "bA", "bP", "bV"))
```

These are pretty close to each other, so we have RM's permission to use the quap fit. But let's stick with the HMC fit to get more practice.

Let's check the parameter estimates more closely.

```{r}
eagles_precis <- m11_hwh2 |> 
  precis_cmdstanfit(pars = c("a", "bA", "bP", "bV"), prob = 0.9)
eagles_precis
```

We've used dummy variable encoding here, which gives a nice interpretation for the mean estimate of `r round(eagles_precis["a", "mean", drop = TRUE], 2)` for the intercept: it corresponds to `r round(plogis(eagles_precis["a", "mean", drop = TRUE]), 2)` mean probability of success in salmon pirating when the pirate and victim both have small body size, and the pirate was not an adult.

If we 'switch on' any of the dummy variables we get corresponding changes in the log-odds of success, as summarised in the precis above.

Now let's generate some posterior predictions. First compute the predicted probabilities and then the counts for each row of the original data.

```{r}
post11_hwh2 <- m11_hwh2 |> 
  spread_draws(a, bA, bP, bV) |> 
  expand(
    nesting(.draw, a, bA, bP, bV),
    transmute(deagles, i = row_number(), n = sample_size, A, P, V)
  )
set.seed(1524)
(
  post11_hwh2 |> 
    transmute(i, A, P, V, value = plogis(a + (bP * P) + (bV * V) + (bA * A))) |> 
    reframe_mean_PI(.by = c(i, A, P, V)) |> 
    pivot_wider() |> 
    pivot_longer(c(A, P, V)) |> 
    unite("label", name, value, sep = " = ") |> 
    group_by(i, mean, lower, upper) |> 
    summarise(across(label, ~ str_c(.x, collapse = ", ")), .groups = "drop") |>
    inner_join(
      deagles |> 
        transmute(i = row_number(), observed = y / sample_size),
      by = "i"
    ) |> 
    unite(label, i, label, sep = ": ") |> 
    mutate(label = fct_inorder(label)) |> 
    ggplot(aes(y = fct_rev(label))) +
    geom_pointrange(aes(x = mean, xmin = lower, xmax = upper)) +
    geom_point(aes(x = observed), colour = "steelblue") +
    scale_x_continuous(
      breaks = seq(0.25, 1, by = 0.25),
      labels = c("0.25", "0.5", "0.75", "1")
    ) +
    labs(x = NULL, y = NULL) +
    theme(axis.text.y = element_blank())
) + (
  post11_hwh2 |> 
    transmute(i, n, A, P, V, p = plogis(a + (bP * P) + (bV * V) + (bA * A))) |> 
    {
      \(x) mutate(x, value = rbinom(n = nrow(x), size = n, prob = p))
    }() |> 
    reframe_mean_PI(.by = c(i, n, A, P, V)) |> 
    pivot_wider() |> 
    pivot_longer(c(n, A, P, V)) |> 
    unite("label", name, value, sep = " = ") |> 
    group_by(i, mean, lower, upper) |> 
    summarise(across(label, ~ str_c(.x, collapse = ", ")), .groups = "drop") |> 
    inner_join(
      deagles |> 
        transmute(i = row_number(), observed = y),
      by = "i"
    ) |> 
    unite(label, i, label, sep = ": ") |> 
    mutate(label = fct_inorder(label)) |> 
    ggplot(aes(y = fct_rev(label))) +
    geom_pointrange(aes(x = mean, xmin = lower, xmax = upper)) +
    geom_point(aes(x = observed), colour = "steelblue") +
    labs(x = NULL, y = NULL)
) +
  plot_annotation(
    title = "Posterior success probabilities and counts for salmon pirates",
    subtitle = "Blue points are observed values"
  )
```

The plot of probabilities ignores sample size. Row 8 of the dataset, where the pirate and victim are both small and the pirate is non-adult, shows a big difference across the two plots. The posterior predicted probability is much higher than the observed (25%, one success from four attempts). Because the sample size is small the posterior estimate for $p$ is fairly wide, meaning that many values are plausible.

Now we try to improve the model. We can introduce an interaction term for the pirate's size and age as RM suggests.

```{r}
mod_11_hwh2_interact <- cmdstan_model(
  here::here("inst/Stan/m11_hwh2_interact.stan")
)
m11_hwh2_interact <- mod_11_hwh2_interact$sample(
  data = leagles,
  seed = 905,
  refresh = 0
)
```

```{r}
compare_cmdstan(m11_hwh2, m11_hwh2_interact)
```

The interaction term made the model slightly worse. Let's inspect the parameter estimates for the new model and the previous one.

```{r}
precis_cmdstan(m11_hwh2_interact, variables = c("a", "bA", "bP", "bV", "bPA")) |> 
  mutate(interaction = "yes") |> 
  bind_rows(
    precis_cmdstan(m11_hwh2, variables = c("a", "bA", "bP", "bV")) |> 
      mutate(interaction = "no")
  ) |> 
  ggplot(aes(y = fct_rev(variable), colour = interaction)) +
  geom_pointrange(
    aes(x = mean, xmin = q5, xmax = q95),
    position = position_dodge(width = 0.25)
  ) +
  scale_colour_manual(values = c("grey20", "steelblue")) +
  labs(
    title = "Parameter estimates for models with and without interaction",
    x = NULL,
    y = NULL
  ) +
  theme(legend.position = "bottom")
```

There's really very little difference between them. Adding the interaction reduces the estimates for A and P a little. The coefficient on the interaction term is mostly positive but with a wider interval. What should we make of this? Let's return to the original data.

```{r}
deagles |> 
  as_tibble() |> 
  mutate(PA = P * A, p = round(y / sample_size, 2))
```

The column `p` is the proportion of pirating attempts that succeeded. 

There are only two rows in the dataset where $PA = 1$. Both have a fairly high success rate, but one has $V = 1$ (i.e. victim was large) the other $V = 0$. Both models agree that the effect of $V$ is strongly negative though. So in some sense the interaction is only adding confusion, which is why it reduces the PSIS.

11H3.

```{r}
data(salamanders)
dsalaman <- salamanders |> 
  as_tibble() |> 
  mutate(
    cover = as.double(scale(PCTCOVER)),
    tree_age = as.double(scale(FORESTAGE))
  )
lsalaman <- dsalaman |> 
  select(C = cover, A = tree_age, S = SALAMAN) |> 
  compose_data()
dsalaman
```

For part (a) our model will be:

$$
\begin{align*}
S_i &\sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) & = \alpha + \beta_C C_i
\end{align*}
$$

Now need to set some reasonable priors. RM suggests "weakly informative", and recalling the Poisson example earlier the log-link implies that the parameters have an exponential relationship with the outcome variable. If we use normal priors then we are modelling them as lognormal, and the mean of the lognormal variable increases at $O(e^{\sigma^2})$. Therefore it's the scale parameters on our priors that we need to control.

Let's try simulating from the priors we used earlier.

```{r}
set.seed(1050)
tibble(
  a = rnorm(1000, 3, 0.5),
  b = rnorm(1000, 0, 0.5)
) |> 
  mutate(S = rpois(1000, lambda = exp(a + b))) |> 
  pivot_longer(everything()) |> 
  ggplot(aes(value)) +
  geom_histogram(bins = 25) + 
  facet_wrap(~ name, scales = "free", ncol = 1)
```

This seems pretty reasonable. For the intercept we have a peak at 3; for the coefficient we centre at zero but with some spread to $\pm 1$; and on the prediction scale our peak is somewhere just below 20, most values under 100, but with a few up to 150. 150 would be two salamanders per metre squared across 49$m^2$, which seems like a lot of salamanders. So our model now looks like:

$$
\begin{align*}
S_i &\sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) & = \alpha + \beta_C C_i \\
\alpha &\sim \mathcal{N}(3, 0.5) \\
\beta_C &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$

```{r}
mod_11_hwh3 <- cmdstan_model(here::here("inst/Stan/m11_hwh3.stan"))
m11_hwh3 <- mod_11_hwh3$sample(
  data = lsalaman,
  seed = 835,
  refresh = 0
)
```

```{r}
precis_cmdstan(m11_hwh3, variables = c("a", "bC"))
```
Let's compare this to the quadratic approximation.

```{r}
q11_hwh3 <- quap(
  alist(
    S ~ dpois(lambda),
    log(lambda) <- a + (bC * C),
    a ~ dnorm(3, 0.5),
    bC ~ dnorm(0, 0.5)
  ),
  data = lsalaman
)
precis(q11_hwh3)
```

We get very similar estimates for the parameters, so we perhaps could use the `quap()` model, but we'll stick with HMC.

Now we want the expected counts and 89% interval against cover.

```{r}
C_seq <- seq(-2, 2, length.out = 101)
m11_hwh3 |> 
  spread_draws(a, bC) |> 
  select(.draw, a, bC) |> 
  group_by(.draw) |> 
  reframe(
    tibble(
      C = C_seq,
      value = exp(a + (bC * C))
    )
  ) |> 
  reframe_mean_PI(.by = C) |> 
  pivot_wider() |> 
  ggplot(aes(x = C)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) + 
  geom_line(aes(y = mean)) +
  geom_point(
    aes(cover, SALAMAN),
    colour = "steelblue",
    alpha = 0.7,
    data = dsalaman,
    inherit.aes = FALSE
  ) +
  labs(
    title = "Expected counts vs. actual",
    subtitle = expression(paste("Showing mean and 89% interval for ", lambda)),
    x = "Tree cover (standardised)",
    y = NULL
  )
```

This fit is not great, but the line with just the mean is perhaps underestimating the variance. Let's plot the posterior prediction intervals vs. the actual counts.

```{r}
m11_hwh3 |> 
  spread_draws(a, bC) |> 
  select(.draw, a, bC) |> 
  expand(
    nesting(.draw, a, bC),
    C = C_seq
  ) |> {
    \(x) mutate(x, value = rpois(nrow(x), lambda = exp(a + (bC * C))))
  }() |> 
  reframe_mean_PI(.by = C) |> 
  pivot_wider() |> 
  ggplot(aes(x = C)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) + 
  geom_line(aes(y = mean)) +
  geom_point(
    aes(cover, SALAMAN),
    colour = "steelblue",
    alpha = 0.7,
    data = dsalaman,
    inherit.aes = FALSE
  ) +
  labs(
    title = "Posterior predicted counts vs. actual",
    subtitle = "Showing mean and 89% interval",
    x = "Tree cover (standardised)",
    y = NULL
  )
```

This is better but still not exactly good: in particular at the higher levels of cover we have many observed counts outside the 89% interval.

Let's move on to considering the age of the forest. We add it to the model and then can consider whether it improves things.

```{r}
mod_11_hwh3b <- cmdstan_model(here::here("inst/Stan/m11_hwh3b.stan"))
m11_hwh3b <- mod_11_hwh3b$sample(
  data = lsalaman,
  seed = 908,
  refresh = 0
)
```

Let's compare the models with PSIS.

```{r}
compare_cmdstan(m11_hwh3, m11_hwh3b)
```
It made the model worse. Why? Before we get into that let's also inspect the parameter estimates for each model.

```{r}
bind_rows(
  m11_hwh3 |> 
    gather_draws(a, bC) |> 
    rename(value = .value) |> 
    reframe_mean_PI() |> 
    add_column(model = "without age", .before = 1),
  m11_hwh3b |> 
    gather_draws(a, bC, bA) |> 
    rename(value = .value) |> 
    reframe_mean_PI() |> 
    add_column(model = "with age", .before = 1)
) |> 
  pivot_wider() |> 
  mutate(model = fct_rev(model)) |> 
  ggplot(aes(y = .variable, colour = model)) +
  geom_pointrange(
    aes(x = mean, xmin = lower, xmax = upper),
    position = position_dodge(width = 0.25)
  ) +
  scale_colour_manual(values = c("grey20", "steelblue")) + 
  labs(
    colour = NULL,
    y = NULL
  ) +
  theme(legend.position = "bottom")
```

The second model estimates roughly zero effect for forest age: the other parameter estimates are unchanged. Let's consider the DAG. We've already assumed a causal arrow from C to S, so let's leave that as-is. Then we need to consider the four possibilities for edges with forest age.

1. Edge from S to A
2. Edge from A to S
3. Edge from C to A
4. Edge from A to C

We can rule out 1 and 3 straight away: such causal relationships are nonsense.

That leaves three possible DAGs.

```{r}
salamander_coords <- tibble(
  name = c("S", "A", "C"),
  x = c(1, 0, 0),
  y = c(1, 0, 1)
)
# First: only edge from A to S
salamander_dag_1 <- dagify(
  S ~ C + A,
  outcome = "S",
  coords = salamander_coords
  )
salamander_dag_1 |> 
  ggdag() + 
  theme_dag()
```

```{r}
# Second: only edge from A to C
salamander_dag_2 <- dagify(
  S ~ C,
  C ~ A,
  outcome = "S",
  coords = salamander_coords
  )
salamander_dag_2 |> 
  ggdag() + 
  theme_dag()
```

```{r}
# Third: both edges out of A
salamander_dag_3 <- dagify(
  S ~ C + A,
  C ~ A,
  outcome = "S",
  coords = salamander_coords
  )
salamander_dag_3 |> 
  ggdag() + 
  theme_dag()
```

What does each of these DAGs imply in terms of testable conditional independencies?

```{r}
list(
  salamander_dag_1,
  salamander_dag_2,
  salamander_dag_3
) |> 
  map(~ impliedConditionalIndependencies(.x, type = "all"))
```

The first DAG, with no edge between A and C, implies that the two variables would be independent. We can model that quickly with `quap()`.

```{r}
quap(
  alist(
    C ~ dnorm(mu, sigma),
    mu <- a + (bA * A),
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dsalaman |> 
    select(C = PCTCOVER, A = FORESTAGE) |> 
    mutate(across(everything(), ~ as.double(standardize(.x))))
) |> 
  precis()
```

We get quite a strong positive association between forest age and tree cover, which suggests that DAG 1 isn't that reasonable. And intuitively this makes sense: as trees get older we would expect that the tree cover would increase.

DAG 2 implies that S is independent of A given C: that's the model we just fit, and there's quite good evidence for it. DAG 2 seems plausible. This would explain why the predictions got worse when we added A to the model: we were conditioning on a confounder.
