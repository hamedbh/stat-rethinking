---
title: "Small Worlds and Large Worlds"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

We must distinguish between: 

- **Small World**: the self-contained world of the model.
- **Large World**: the 'real' world where you deploy the model.

## The garden of forking data

Bayesian inference considers all the possible ways that the data _could_ have been generated, and weighs the relative probability of each.

Start with simple counts, then add prior information, finally start working with probabilities instead of counts.

The example starts by counting the blue and white marbles, then representing these with symbols. So $p$ is the proportion of the marbles that are blue (i.e. the probability of any randomly chosen marble being blue), and $D_{new}$ is the sequence of marbles drawn from the bag. Then: 

plausibility of $p$ after getting $D_{new} \propto$ number of ways $p$ can produce $D_{new} \times$ prior for $p$

Which leads to standardising that result via Bayes rule:

$$
\text{P}(p | D_{new}) = \frac{\text{P}(D_{new} | p) \times \text{P}(p)}{\text{P}(D_{new})}
$$

Method for computing the plausibilities is simply to rescale the counts.

```{r}
# these are the numbers of ways D_new could have been generated for each 
# possible composition of marbles, each having 0:4 blue marbles
ways <- c(0, 3, 8, 9, 0)
# normalise them to sum to 1
(ways/sum(ways)) %>% 
    set_names(0:4)
```

The names for the components of Bayes rule are:

- $p$ is the **parameter**, conjecture for proportion of blue marbles;
- relative number of ways that $p$ can produce the data is the **likelihood**;
- prior plausibility of any specific $p$ is the **prior probability**;
- updated plausibility of any specific $p$ is the **posterior probability**.

## Building a Model

The next example shown is a binomial, using repeated tests of a toy globe to estimate the proportion of water on its surface. The data generated are:

_W L W W W L W L W_

Then work through three steps for designing the Bayesian model: 

1. Set up a _data story_; 
2. _Update_ the model with data; 
3. _Evaluate_ the model (possibly looping back to revise the model).

### Data story

In this case the story is simply that each toss of the globe has probability $p$ of landing on water, where $p$ is the true proportion of water on the globe, and that each toss is independent of the others. 

### Updating

We start out (in this case) knowing nothing about the value of $p$, so it could be any value on $[0, 1]$. At each step the result changes which values of $p$ are most plausible. 

```{r}
# 1 denotes water
tosses <- c(1, 0, 1, 1, 1, 0, 1, 0, 1)

tibble(
    n_trials = seq_along(tosses), 
    result = tosses
) %>% 
    mutate(
        water = cumsum(result)
    ) %>% 
    expand(
        nesting(n_trials, result, water), 
        p = seq(0, 1, length.out = 100)
    ) %>% 
    group_by(p) %>% 
    mutate(
        lagged_n_trials = lag(n_trials), 
        lagged_water = lag(water)
    ) %>% 
    ungroup() %>% 
    mutate(prior = if_else(n_trials == 1, 
                           0.5, 
                           dbinom(x = lagged_water,
                                  size = lagged_n_trials, 
                                  prob = p)), 
           likelihood = dbinom(x = water, 
                               size = n_trials, 
                               prob = p), 
           label = str_c("n = ", n_trials)) %>% 
    group_by(n_trials) %>% 
    mutate(prior = prior/sum(prior), 
           likelihood = likelihood/sum(likelihood)) %>% 
    ungroup() %>% 
    ggplot(aes(x = p)) + 
    geom_line(aes(y = likelihood)) + 
    geom_line(aes(y = prior), lty = 2) + 
    facet_wrap(~ label, ncol = 3, scales = "free_y") + 
    labs(
        x = "proportion of water", 
        y = "plausibility"
    ) + 
    theme_bw() + 
    theme(
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        panel.grid = element_blank()
    )
```

### Evaluate

Conditional on the model assumptions being correct this is flawless, but that's quite a condition! For example, shuffling the order of the tosses leads to the same final curve, and therefore estimate of plausibility for $p$, because of the assumption that the tosses are independent. If that assumption is wrong then dependence between the tosses would have to be accounted for in the model. 

## Components of the model

### Variables

Unobserved variables, such as $p$ in this model, are normally called **parameters**. We infer $p$ based on observations. 

### Definitions

#### Observed variables

In this case the results of the tosses, _W_ or _L_, are the observed variables. The distribution of the plausibility for any combination of these is the **Likelihood**. In this case the assumptions of independence between the tosses and a constant probability of getting a _W_ mean that the likelihood follows the binomial distribution. 

$$
\text{P}(W, L | p) = {W + L \choose W}p^{W}(1-p)^{L}
$$
Where the binomial coefficient at the front is given by:

$$
{W + L \choose W} =  \frac{(W + L)!}{W!L!}
$$

Calculating this for any given values of $p$, $W$, and $L$ is trivial: 

```{r}
dbinom(
    x = 6, # this is the value of W
    size = 9, # W + L
    prob = 0.5 # p
)
```

#### Unobserved variables

In this model $p$ is the unobserved variable, or **parameter**. It is inferred from the observed variables and the statistical structure we build in the model. For any parameter we also need a **prior**. Choices for priors are contentious, but this will be covered fully later in the book. 

### A model is born

McElreath shows a particular grammar for specifying models: 

$$
W \sim \text{Binomial}(N, p) \\
$$

where 

$$
N = W + L
$$

and 

$$
p \sim \text{Uniform}(0, 1)
$$
## Making the model go

Once the model is specified then we can generate the posterior distribution, or $\text{P}(p|W, L)$: how likely is a given value of $p$, conditional on seeing the data?

### Bayes' theorem

Can derive the theorem from the definitions of joint and conditional probability. The joint probability for all variables is: 

$$
\text{P}(W, L, p) = \text{P}(W, L|p)\text{P}(p)
$$

We could equally separate out the unconditional probability of $W$ and $L$: 

$$
\text{P}(W, L, p) = \text{P}(p|W, L)\text{P}(W, L)
$$
The two right-hand sides are equal, and a bit of arithmetic gives: 

$$
\text{P}(p|W, L) = \frac{\text{P}(W, L|p)\text{P}(p)}{\text{P}(W, L)}
$$

To generalise: 

$$
Posterior = \frac{Likelihood \times Prior}{Average \; Likelihood}
$$

The denominator is often the trickiest part. It's called the average likelihood, but it's essentially the unconditional probability of $W, L$. Since we normally only see the conditional distribution that means taking its expectation: 

$$
\text{P}(W, L) = 
\text{E}(\text{P}(W, L|p)) = 
\int \text{P}(W, L|p) \, \text{P}(p) \, dp
$$

To compare different posteriors it can be enough to get the answer only up to a constant, which means we can ditch the tricky denominator (sometimes) and compute only the rest. 

$$
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
$$
```{r}
d <- tibble(
    probability = seq(0, 1, length.out = 1e3)
) %>% 
    expand(probability, row = c("flat", "stepped", "Laplace")) %>% 
    arrange(row, probability) %>% 
    mutate(prior = ifelse(row == "flat", 1,
                          ifelse(row == "stepped", 
                                 rep(0:1, each = 1e3/2),
                                 exp(-abs(probability - .5) / .25) / ( 2 * .25))),
           likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
    group_by(row) %>% 
    mutate(posterior = (prior * likelihood)/(sum(prior * likelihood))) %>% 
    gather(key, value, -probability, -row) %>% 
    ungroup() %>% 
    mutate(key = factor(key, levels = c("prior", "likelihood", "posterior")),
           row = factor(row, levels = c("flat", "stepped", "Laplace")))

plots <- purrr::map(levels(d$key), 
                    ~ return(d %>%
                                 filter(key == .x) %>% 
                                 ggplot(aes(x = probability, y = value)) +
                                 geom_line() +
                                 scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
                                 scale_y_continuous(NULL, breaks = NULL) +
                                 labs(subtitle = .x) +
                                 theme_light() + 
                                 theme(panel.grid       = element_blank(),
                                       strip.background = element_blank(),
                                       strip.text       = element_blank()) +
                                 facet_wrap(row ~ ., scales = "free_y", ncol = 1)))

gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], ncol = 3)
```

### Motors

Sometimes there is an analytical solution to Bayes theorem, but most often we need some sort of numerical solution. McElreath will cover three methods (engines) for this: 

1. Grid approximation; 
2. Quadratic approximation; 
3. Markov chain Monte Carlo (MCMC). 

### Grid approximation

Start with grid approximation: essentially this changes a continuous problem to a discrete one by picking grid points at which to estimate the parameters.

```{r}
globe_grid <- function(grid_size) {
    d <- tibble(
        p_grid = seq(from = 0,
                     to = 1,
                     length.out = grid_size), 
        prior = 1) %>% 
        mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%
        mutate(unstd_posterior = prior * likelihood) %>%
        mutate(posterior = unstd_posterior/sum(unstd_posterior))
    
    max_posterior <- d %>% 
        top_n(n = 1, posterior) %>% 
        pull(p_grid)
    
    g <- d %>%
        ggplot(aes(x = p_grid, y = posterior)) +
        geom_point() +
        geom_line() +
        geom_vline(xintercept = max_posterior, 
                   colour = "blue") +
        ggtitle(paste(grid_size, "Points")) +
        labs(x = "probability of water", 
             y = "posterior probability") +
        scale_x_continuous(breaks = seq(0, 1, by = 0.1),
                           minor_breaks = NULL) + 
        theme_light()
    plot(g)
}
globe_grid(20)
```

Try again with different grid sizes.

```{r}
walk(c(5, 20, 100, 1000), globe_grid)
```

### Quadratic approximation

Grid approximation is OK with a small number of parameters, but the complexity explodes (for a grid of $x$ values of $n$ parameters the computation increases $O(x^n)$). Instead can take advantage of the log of a Gaussian distribution being a quadratic, so we can approximate the Gaussian distribution curve that way. 

This has two steps: 

1. Find the posterior mode (peak); 
2. Estimate the curve at that point. 

```{r}
library(rethinking)
```

```{r}
globe_qa_9 <- quap(
    flist = alist(
        W ~ dbinom(W + L, p), # binomial likelihood
        p ~ dunif(0, 1)       # uniform prior
    ),
    data = list(W = 6, L = 3)
)

precis(globe_qa_9)
```

Because the structure of this model is simple can compare directly to the analytical solution.

```{r}
# analytical calculation
w <- 6
n <- 9
g_9 <- tibble(x = seq(0, 1, length.out = 100)) %>% 
    ggplot(aes(x, ..y..)) + 
    stat_function(fun = dbeta, 
                  n = 100, 
                  args = list(shape1 = w + 1, 
                              shape2 = n - w + 1)) + 
    stat_function(fun = dnorm, 
                  n = 100, 
                  colour = "blue", 
                  args = list(mean = 0.67, 
                              sd = 0.16)) + 
    labs(x = "probability of water", y = "posterior") + 
    theme_light()
g_9
```

Try again with $n = 18$ and $n = 36$.

```{r}
globe_qa_18 <-
    quap(
        alist(
            w ~ dbinom(9 * 2, p),
            p ~ dunif(0, 1)
        ), data = list(w = 6 * 2))

globe_qa_36 <-
    quap(
        alist(
            w ~ dbinom(9 * 4, p),
            p ~ dunif(0, 1)
        ), data = list(w = 6 * 4))

precis(globe_qa_18)
precis(globe_qa_36)
```

Now plot them altogether to see that the approximation improves. 

```{r}
g <- purrr::map(
    c(9, 18, 36), 
    function(num_tosses) {
        quap_obj <- get(paste0("globe_qa_", num_tosses))
        w <- 2/3 * (num_tosses)
        n <- num_tosses
        post_mode <- precis(quap_obj)@.Data[[1]]
        post_sd   <- precis(quap_obj)@.Data[[2]]
        tibble(x = seq(0, 1, length.out = 100)) %>% 
            ggplot(aes(x, ..y..)) + 
            stat_function(fun = dbeta, 
                          n = 100, 
                          args = list(shape1 = w + 1, 
                                      shape2 = n - w + 1)) + 
            stat_function(fun = dnorm, 
                          n = 100, 
                          colour = "blue", 
                          args = list(mean = post_mode, 
                                      sd = post_sd)) + 
            labs(x = "probability of water", y = "posterior") + 
            theme_light()
    }
)
gridExtra::grid.arrange(g[[1]], g[[2]], g[[3]], nrow = 1)
```

### Markov chain Monte Carlo

Quadratic approximation won't suffice for some model types, such as multilevel models. (Anything with multimodal distributions will be hard in this way.)

Markov chain Monte Carlo (MCMC) is the most popular technique for model fitting. It's the combination of MCMC and affordable computing power that enables much of modern Bayesian statistics. 

We can fit a simple model for the globe tossing example using MCMC. 

```{r}
n_samples <- 1000
p <- double(n_samples)
p[1] <- 0.5
W <- 6
L <- 3
for (i in 2:n_samples) {
    p_new <- rnorm(1 , p[i - 1] , 0.1)
    if (p_new < 0)
        p_new <- abs(p_new)
    if (p_new > 1)
        p_new <- 2 - p_new
    q0 <- dbinom(W , W + L , p[i - 1])
    q1 <- dbinom(W , W + L , p_new)
    p[i] <- ifelse(runif(1) < q1 / q0 , p_new , p[i - 1])
}
tibble(p) %>% 
    ggplot(aes(p)) + 
    geom_density() + 
    stat_function(fun = dbeta, 
                  n = 100, 
                  args = list(shape1 = w + 1, 
                              shape2 = n - w + 1), 
                  lty = 2) + 
    theme_light()
```

## Summary

Everything is set up now for the rest of the book. I'll be using tidyverse packages for data manipulation and plotting, but will stick with `{rethinking}` for the model fitting. 

## Practice

### Easy 

2E1. Which of the expressions below correspond to the statement: the probability of rain on Monday?

$Pr(rain|Monday)$

2E2. Which of the following statements corresponds to the expression: $Pr(Monday|rain)$?

The probability that it is Monday, given that it is raining. 

2E3. Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?

$Pr(Monday|rain)$

2E4. … What does it mean to say "the probability of water is 0.7"?

This depends on our perspective. Using a Bayesian framework this statement means "based on our prior beliefs and evidence gathered the most plausible value for the proportion of water on the globe is 0.7". 

### Medium

2M1.1

```{r}
# set up a function to make the computation simpler for the rest of the example
globe_grid_unif <- function(grid_size = 100, N, W) {
    tibble(
        p = seq(from = 0, 
                to = 1, 
                length.out = grid_size), 
        prior = 1
    ) %>% 
        mutate(likelihood = dbinom(x = W, 
                                   size = N, 
                                   prob = p)) %>% 
        mutate(posterior = (likelihood * prior)/sum(likelihood * prior))
}

plot_globe_grid_unif <- function(grid_size = 100, N, W) {
    d <- globe_grid_unif(grid_size, N, W)
    
    max_post <- d %>% 
        top_n(n = 1, posterior) %>% 
        pull(p)
    g <- d %>%
        ggplot(aes(x = p, y = posterior)) +
        geom_point() +
        geom_line() +
        geom_vline(xintercept = max_post, 
                   colour = "blue") +
        labs(x = "probability of water", 
             y = "posterior probability", 
             title = sprintf("Grid approximation for N = %s, W = %s", N, W), 
             subtitle = sprintf("%s points", grid_size)) +
        scale_x_continuous(breaks = seq(0, 1, by = 0.1),
                           minor_breaks = NULL) + 
        theme_light()
    plot(g)
}
plot_globe_grid_unif(N = 3, W = 3)
```

2M1.2

```{r}
plot_globe_grid_unif(N = 4, W = 3)
```

2M1.3

```{r}
plot_globe_grid_unif(N = 7, W = 5)
```

2M2.1

```{r}
# change the function for the new prior
globe_grid_step <- function(grid_size = 100, N, W) {
    d <- tibble(
        p = seq(from = 0, 
                to = 1, 
                length.out = grid_size)
    ) %>% 
        mutate(prior = if_else(p < 0.5, 0, 1)) %>% 
        mutate(likelihood = dbinom(x = W, 
                                   size = N, 
                                   prob = p)) %>% 
        mutate(posterior = (likelihood * prior)/sum(likelihood * prior))
}

plot_globe_grid_step <- function(grid_size = 100, N, W) {
    d <- globe_grid_step(grid_size, N, W)
    max_post <- d %>% 
        top_n(n = 1, posterior) %>% 
        pull(p)
    g <- d %>%
        ggplot(aes(x = p, y = posterior)) +
        geom_point() +
        geom_line() +
        geom_vline(xintercept = max_post, 
                   colour = "blue") +
        labs(x = "probability of water", 
             y = "posterior probability", 
             title = sprintf("Grid approximation for N = %s, W = %s", N, W), 
             subtitle = sprintf("%s points", grid_size)) +
        scale_x_continuous(breaks = seq(0, 1, by = 0.1),
                           minor_breaks = NULL) + 
        theme_light()
    plot(g)
}

plot_globe_grid_step(N = 3, W = 3)
```

2M2.2

```{r}
plot_globe_grid_step(N = 4, W = 3)
```

2M2.3

```{r}
plot_globe_grid_step(N = 7, W = 5)
```

2M3. 

Set up the computation in Bayes' theorem: 

$$
\text{P}(\text{Earth}|\text{land}) = 
    \frac{\text{P}(\text{land}|\text{Earth}) \text{P}(\text{Earth})}
    {\text{P}(\text{land})}
$$

We get from the question that $\text{P}(\text{land}|\text{Earth}) = 0.3$. Absent any other information about which globe was tossed we can assume that $\text{P}(\text{Earth}) = 0.5$. By the law of total probability: 

$$
\text{P}(\text{land}) = 
    \text{P}(\text{land}|\text{Earth})\text{P}(\text{Earth}) + 
    \text{P}(\text{land}|\text{Mars})\text{P}(\text{Mars}) = 
    \big(0.3 \times 0.5 \big) + \big(1 \times 0.5 \big)
$$

```{r}
round((0.3 * 0.5)/((0.3 * 0.5) + (1 * 0.5)), 2)
```
2M4. 

Call the cards by how many black sides they have: 0B, 1B, 2B. We want $\text{P}(2B)$.

| Conjecture | Ways to show black side up |
| :--------: | :------------------------: |
|     0B     |              0             |
|     1B     |              1             |
|     2B     |              2             |

Can discount the 0B row and just consider the others: there are three ways that I can have a black side facing up, of which two have a black side down, which implies the probability of $2/3$.

2M5. 

Use a similar structure but add a prior with the appropriate weight (whereas there was an implicit uniform prior in 2M4). 

By Bayes rule:

$$
\text{P}(2B \; | \; 1B) = 
    \frac {\text{P}(1B \; | \; 2B) \; \text{P}(2B)}
    {\text{P}(1B)}
$$

```{r}
# set up a function for the next couple of questions
bayes_rule <- function(likelihood, 
                       prior, 
                       avg_likelihood) {
    (likelihood * prior) / avg_likelihood
}

bayes_rule(likelihood = 1, 
           prior = 1/2, 
           avg_likelihood = 5/8)
```

2M6. 

Use same as before, but multiply each by the new prior (i.e. probability of drawing the card, or $\text{P}(2B)$).

| Conjecture | Ways to show black side up | Ways to draw the card |
| :--------: | :------------------------: | :-------------------: |
|     0B     |              0             |           3           |
|     1B     |              1             |           2           |
|     2B     |              2             |           1           |

This reweights the 1B row, so that the 2 ways to have a black side down in the 2B row are now 2/4 of the ways to have a black side up.

2M7. 

We want to get $\text{P}(2B | 1B, 1W)$. Use the counting method as suggested.

| Conjecture | Ways to show black, first card | Ways to show white, second card |
| :--------: | :----------------------------: | :-----------------------------: |
|     0B     |                  0             |                  0              |
|     1B     |                  1             |                  2              |
|     2B     |                  2             |                  3              |

Multiply the ways to show black on the first card by the ways to show white on the second card to give 2 for $1B$, and 6 for $2B$, so $\text{P}(2B | 1B, 1W) = 0.75$.

### Hard 

2H1. 

We want $\text{P}(2T|T)$. We get some info from the question: 

$$
\begin{align}
\text{P}(T|A)& = 0.1 \\
\text{P}(T|B)& = 0.2 \\
\text{P}(A)& = \text{P}(A) = 0.5
\end{align}
$$

We can use that to calculate $\text{P}(T)$. 

$$
\begin{align}
\text{P}(T)& = \text{P}(T|A)\text{P}(A) = \text{P}(T|B)\text{P}(B) \\
& = (0.1 \times 0.5) \; + \; (0.2 \times 0.5) \\
& = 0.15
\end{align}
$$
Now use this to update $\text{P}(A)$ and $\text{P}(B)$ to $\text{P}(A|T)$ and $\text{P}(B|T)$. 

$$
\begin{align}
\text{P}(A|T)& = \frac{\text{P}(T|A)\text{P}(A)}{\text{P}(T)} \\
& = \frac{0.1 \times 0.5}{0.15} \\
& = \frac{1}{3} \\
\text{P}(B|T)& = \frac{\text{P}(T|B)\text{P}(B)}{\text{P}(T)} \\
& = \frac{0.2 \times 0.5}{0.15} \\
& = \frac{2}{3} \\
\end{align}
$$

Since we know that $T$ occurred we can use those values as the updated values for $\text{P}(A)$ and $\text{P}(B)$, and then update $\text{P}(T)$ to $\text{P}(2T|T)$. Everything is conditional on $T$ so leave that out of the notation for simplicity.

$$
\begin{align}
\text{P}(2T)& = \text{P}(2T|A)\text{P}(A) + \text{P}(2T|B)\text{P}(B) \\
& = (0.1 \times 1/3) \; + \; (0.2 \times 2/3) \\
& = \frac{1}{6} \\
& \approx 0.17
\end{align}
$$

2H2. 

We want $\text{P}(A|T)$. Already computed that as part of the last question: 

$$
\begin{align}
\text{P}(A|T)& = \frac{\text{P}(T|A)\text{P}(A)}{\text{P}(T)} \\
& = \frac{0.1 \times 0.5}{0.15} \\
& = \frac{1}{3}
\end{align}
$$

2H3. 

Everything in this question is conditional on the first birth being twins, so we can leave that out of the equations for simplicity. 

$$
\begin{align}
\text{P}(A|Singleton)& = \frac{\text{P}(Singleton|A)\text{P}(A)}
    {\text{P}(Singleton)} \\

& = \frac{9/10 \times 1/3}{\text{P}(Singleton)}
\end{align}
$$

The missing ingredient is $\text{P}(Singleton)$. 

$$
\begin{align}
\text{P}(Singleton)& = \text{P}(Singleton|A)\text{A} + 
    \text{P}(Singleton|B)\text{B} \\
& = (9/10 \times 1/3) \; + \; (8/10 \times 2/3) \\
& = 5/6
\end{align}
$$

Now compute the answer: 

$$
\begin{align}
\text{P}(A|Singleton)& = \frac{\text{P}(Singleton|A)\text{P}(A)}
{\text{P}(Singleton)} \\

& = \frac{9/10 \times 1/3}{5/6} \\
& = 9/25 \\
& = 0.36
\end{align}
$$

2H4. 

Call the event that the test asserts that panda is species $A$ $Test_A$. First compute $\text{P}(A|Test_A)$ without the birth data, then again with it. 

Can state the new info given in this question a bit more clearly (including the complements, which we'll need): 

$$
\begin{align}
\text{P}(Test_A|A)& = 0.8 \\
\text{P}(Test_B|A)& = 0.2 \\
\\
\text{P}(Test_B|B)& = 0.65 \\
\text{P}(Test_A|B)& = 0.35
\end{align}
$$

Without the birth data we go back and use the probabilities given at the start. That gives us: 

$$
\begin{align}
\text{P}(A|Test_A)& = \frac{\text{P}(Test_A|A)\text{P}(A)}{\text{P}(Test_A)} \\
& = \frac{0.8 \times 0.5}{\text{P}(Test_A)} \\
& = \frac{0.4}{\text{P}(Test_A|A)\text{P}(A) + \text{P}(Test_A|B)\text{P}(B)} \\
& = \frac{0.4}{(0.8 \times 0.5) + (0.35 \times 0.5)} \\
& = \frac{0.4}{0.575} \\
& \approx 0.7
\end{align}
$$

To do this with the birth data we need only replace $\text{P}(A)$ with the posterior from the births, or 0.36. 

$$
\begin{align}
\text{P}(A|Test_A)& = \frac{\text{P}(Test_A|A)\text{P}(A)}{\text{P}(Test_A)} \\

& = \frac{\text{P}(Test_A|A)\text{P}(A)}
    {\text{P}(Test_A|A)\text{P}(A) + \text{P}(Test_A|B)\text{P}(B)} \\


& = \frac{0.8 \times 0.36}
    {(0.8 \times 0.36) + (0.35 \times 0.64)} \\

& = 0.5625
\end{align}
$$
