---
title: "The Many Variables & The Spurious Waffles"
output: html_document
---

```{r setup}
library(rethinking)
library(patchwork)
library(zeallot)
library(dagitty)
library(ggdag)
library(ggrepel)
library(magrittr)
library(corrr)
library(tidyverse)
# set up the theme
theme_set(theme_light())
```

Let's first do the simulation of _Berkson's Paradox_ that RM shows. 

```{r}
set.seed(1941)
berkson_ex <- tibble(nw = rnorm(200), tw = rnorm(200)) %>% 
  mutate(selected = ntile(nw + tw, 10) == 10)

berkson_ex %>% 
  ggplot(aes(nw, tw, colour = selected, fill = selected)) + 
  geom_point(alpha = 0.4) + 
  stat_smooth(
    data = . %>% filter(selected), 
    formula = "y ~ x", 
    method = "lm", 
    se = FALSE
  ) + 
  scale_colour_manual(
    values = c("black", "steelblue"), 
    aesthetics = c("fill", "colour")
  ) + 
  labs(x = "Newsworthiness", y = "Trustworthiness") + 
  theme(
    legend.position = "none", 
    panel.grid = element_blank()
  )
```

We can compute the correlation as RM does also. 

```{r}
berkson_ex %>% 
  filter(selected) %>% 
  select(-selected) %>% 
  correlate()
```

A strong negative correlation, even though we know from the simulation that trustworthiness and newsworthiness are entirely independent. But they are **not** independent conditional on selection for publication. Or we can say: 

$$
X \perp\!\!\!\perp Y \\
X \not\!\perp\!\!\!\perp Y | S
$$
Where $X$ is trustworthiness, $Y$ is newsworthiness, and $S$ is an indicator of whether the paper was selected. 

## Multicollinearity

RM describes this as "the least of your worries". Nonetheless it's a problem. 

### Multicollinear legs

First show this with some simulated data: we want to predictor height using leg length. 

```{r}
set.seed(909)
dheight <- tibble(
  # heights are Gaussian around 10
  height = rnorm(100, 10, 2), 
  # What proportion of height is the leg?
  prop = runif(100, 0.4, 0.5)) %>% 
  mutate(
    # Each leg is prop * height plus some error
    left = prop * height + rnorm(100, 0, 0.02), 
    right = prop * height + rnorm(100, 0, 0.02)
  )
```

We can then fit a model. As RM says: 

> On average an individual's legs are 45% of their height ... should expect the beta coefficient that measures association of a leg with height to end up around ... 2.2.

```{r}
m6_1 <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (bl * left) + (br * right), 
    a ~ dnorm(10, 100), 
    bl ~ dnorm(2, 10), 
    br ~ dnorm(2, 10), 
    sigma ~ dexp(1)
  ),
  data = dheight
)
precis(m6_1)
```

The posterior means are strange, but the picture becomes even worse in a plot. 

```{r}
plot(precis(m6_1))
```

These are the correct answers to the question our model poses: _what is the value of knowing the length of one leg when we already know the length of the other_? Once you already know the length of the left leg, what information is there in the length of the right? The answer is, it depends a lot! Let's plot the joint posterior distribution for the parameters. 

```{r}
set.seed(1220)
post6_1 <- extract.samples(m6_1) %>% 
  as_tibble()
(
  post6_1 %>% 
  ggplot(aes(br, bl)) + 
  geom_point(alpha = 0.3, colour = "steelblue") + 
  theme(panel.grid = element_blank())
) + (
  post6_1 %>% 
  ggplot(aes(x = bl + br)) + 
  geom_density(colour = "steelblue", bw = 0.004) + 
  labs(x = "sum of bl and br", y = "Density") + 
  theme(panel.grid = element_blank())
)
```

By including both legs, for any given height there are a huge number of combinations of left and right leg lengths that can generate the same predictions. So we get out nonsense. RM illustrates this by letting $y$ be the height and $x$ be the leg length (we know that there is basically just one, which of course assumes that everyone in our sample has two legs). Then: 

$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 x_i + \beta_2 x_i \\
      &= \alpha + (\beta_1 + \beta_2)x_i
\end{align}
$$

Now the problem is clear: there will be huge numbers of plausible combinations of $\beta_1$ and $\beta_2$ that give plausible outcomes, so our model cannot distinguish between them. 

The second plot above give a clue as to how we could improve this model: by using the sum of the two legs as the predictor. 

```{r}
m6_1b <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + b * (left + right), 
    a ~ dnorm(10, 100), 
    b ~ dnorm(2, 10), 
    sigma ~ dexp(1)
  ),
  data = dheight
)
precis(m6_1b)
```

Now we get the right inferences. Could also do this with just one of the leg lengths as RM does. 

```{r}
m6_2 <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + (bl * left), 
    a ~ dnorm(10, 100), 
    bl ~ dnorm(2, 10), 
    sigma ~ dexp(1)
  ),
  data = dheight
)
precis(m6_2)
```

### Multicollinear milk

Now a more challengaing problem with real data, the primate milk data. 

```{r}
data(milk)
dmilk <- milk %>% 
  as_tibble() %>% 
  mutate(
    K = standardize(kcal.per.g), 
    F = standardize(perc.fat), 
    L = standardize(perc.lactose)
  )
```

We are modelling $K \sim F + L$. Start off my modelling each separately. 

```{r}
m6_3 <- quap(
  alist(
    K ~ dnorm(mu, sigma), 
    mu <- a + (bF * F), 
    a ~ dnorm(0, 0.2), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dmilk
)

m6_4 <- quap(
  alist(
    K ~ dnorm(mu, sigma), 
    mu <- a + (bL * L), 
    a ~ dnorm(0, 0.2), 
    bL ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dmilk
)

precis(m6_3)
precis(m6_4)
```

The picture seems clear enough: each predictor has strong association but in opposite directions. Now we build a model with both. 

```{r}
m6_5 <- quap(
  alist(
    K ~ dnorm(mu, sigma), 
    mu <- a + (bL * L) + (bF * F), 
    a ~ dnorm(0, 0.2), 
    bL ~ dnorm(0, 0.5), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dmilk
)
precis(m6_5)
```

The pairs plot is informative. 

```{r}
dmilk %>% 
  select(K, F, L) %>% 
  GGally::ggpairs()
```

The two predictors have a correlation of -0.94: they encode almost the same information, much like the leg lengths earlier. (Their correlation was about 1.) 

RM points out that this doesn't mean the model is wrong: it is answering correctly the question that was asked. He also advises against the procedure of inspecting the data for pairwise collinear predictors and removing some. He say that the pairwise correlations are not the issue, rather it's a problem with the conditional associations, and what they imply. 

With the milk example: those species who nurse more frequently will tend to have more sugary milk; those who nurse less frequently will have fattier milk. These work in opposite directions. So the implied DAG might be: 

```{r}
dagify(
  K ~ L + F, 
  L ~ D, 
  F ~ D, 
  coords = tibble(
    name = c("L", "D", "F", "K"), 
    x = c(1, 2, 3, 2), 
    y = c(1, 1, 1, 0)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

In this DAG $D$ is some unobserved variable for how dense the milk must be. So in this case the real challenge is to figure out a means of measuring or quantifying $D$, rather than just fitting regressions. 

## Post-treatment bias

RM points out that bias from including variables is just as much a problem as from excluding them, although the latter gets much more attention. One example of included bias is post-treatment bias. 

```{r}
set.seed(71)
dfungus <- tibble(h0 = rnorm(100, 10, 2), treatment = rep(0:1, each = 50)) %>% 
  mutate(fungus = rbinom(100, size = 1, prob = 0.5 - (treatment * 0.4))) %>% 
  mutate(h1 = h0 + rnorm(100, 5 - (3 * fungus)))
dfungus %>% 
  precis()
```

### A prior is born

We know set up the model as though we don't already know the data generating structure, but using some scientific knowledge and physical constraints. 

To start with we can consider the ratio $p = h_1 / h_0$. Then we set up a linear model: 

$$
\begin{align}
h_{1, i} &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= h_{0, i} \times p
\end{align}
$$

In other words: the height at time $t = 1$ is normally distributed with mean equal to the height at time $t = 0$ times the proportion parameter $p$. We know then that $p$ must be a positive real number, so we can use the lognormal distribution for its prior. We can simulate from that prior to get a feel for the plausible values: 

```{r}
set.seed(1419)
tibble(sim_p = rlnorm(1e4, 0, 0.25)) %>% 
  precis()
```

The range of plausible values goes from shrinkage of about $1/3$ to growth of about 50%. Now we can fit a model. 

```{r}
m6_6 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p ~ dlnorm(0, 0.25), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)
precis(m6_6)
```

The MAP estimate for $p$ is 1.43, i.e. 43% growth. Now we can add predictors. 

$$
\begin{align}
h_{1, i} &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i    &= h_{0, i} \times p \\
p        &= \alpha + \beta_T T_i + \beta_F F_i \\
\alpha   &\sim \text{Lognormal}(0, 0.25) \\
\beta_T  &\sim \mathcal{N}(0, 0.5) \\
\beta_F  &\sim \mathcal{N}(0, 0.5) \\
\sigma   &\sim \text{Expo(1)}
\end{align}
$$

Now we fit the model. 

```{r}
m6_7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment) + (bF * fungus), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)
```

Before checking the model fit let's take a look at those priors and see if they make sense. 

```{r}
set.seed(1504)
prior_summary_6_7 <- extract.prior(m6_7) %>% 
  enframe(name = "var") %>% 
  unnest(value) %>% 
  group_by(var) %>% 
  summarise(
    tibble(
      name = c("mean", "lower", "upper"), 
      value = c(mean(value), PI(value))
    ), 
    .groups = "drop"
  ) %>% 
  pivot_wider()

prior_summary_6_7 %>% 
  ggplot(aes(y = var, colour = var)) + 
  geom_pointrange(aes(x = mean, xmin = lower, xmax = upper)) + 
  geom_vline(xintercept = 0, linetype = 2, colour = "grey50") +
  labs(x = NULL, y = NULL) + 
  theme(legend.position = "none")
```

The $\beta$ priors are centred at 0 and have 89% of their mass roughly in $\pm 0.8$. The $\alpha$ parameter is as we saw above. And $\sigma$ looks fine. 

Now we inspect the model results. 

```{r}
precis(m6_7)
```

The model is telling us that treatment has no effect, which we know to be wrong because of how we simulated it. 

### Blocked by consequence

We set it up so that `treatment` causes `fungus`. So our estimate for the coefficient on treatment is asking "once we know that a plant developed fungus, how much information do we get from knowing whether or not it had the treatment?" Since so much of the information from `treatment` is encoded in `fungus`, the answer is 'not much'. 

A more interesting question is "what effect does the treatment have on growth?" So we can ask it directly. 

```{r}
m6_8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfungus
)
precis(m6_8)
```

Now the estimate for the treatment effect is clearly positive. 

RM points out that we might want to include _pre_-treatment variables, such as the original height, in order to improve the model. But including post-treatment variables will cause problems. 

### Fungus and d-separation

```{r warning=FALSE}
dagify(
  H_1 ~ H_0 + F, 
  F ~ T, 
  coords = tibble(
    name = c("H_0", "H_1", "F", "T"), 
    x = c(0, 2, 3, 4), 
    y = rep(0, 4)
  )
) %>% 
  tidy_dagitty() %>%
  arrange(name) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text(
    parse = TRUE, 
    label = c("F", expression(H[0]), expression(H[1]), "T")
  ) +
  theme_dag()
```

Including $F$ in the model 'blocks' the path from $T$. We can check the implied conditional independencies. 

```{r}
dagitty::impliedConditionalIndependencies(
  dagify(
    H_1 ~ H_0 + F, 
    F ~ T, 
    coords = tibble(
      name = c("H_0", "H_1", "F", "T"), 
      x = c(0, 2, 3, 4), 
      y = rep(0, 4)
    )
  )
)
```

We can consider each of these. The third one is already done as that was the post-treatment effect problem highlighted above. But what happens if we regress $F$ on $H_0$ or $H_0$ on $T$? We know from the simulation that they are unconnected, so the implied conditional independencies are correct. 

This post-treatment effect has led to an incorrect inference (from `m6_7`) in this case that the treatment is not effective. It could also fool us in an opposite direction. Suppose the fungus didn't affect plant growth, but moisture affected both plant and fungus growth. The DAG then might be: 

```{r warning=FALSE}
dagify(
  H_1 ~ H_0 + M, 
  F ~ T + M, 
  coords = tibble(
    name = c("H_0", "H_1", "M", "F", "T"), 
    x = c(0, 1, 2, 3, 4), 
    y = c(1, 1, 0.8, 1, 1)
  )
) %>% 
  tidy_dagitty() %>%
  arrange(name) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text(
    parse = TRUE, 
    label = c("F", expression(H[0]), expression(H[1]), "M", "T")
  ) +
  theme_dag()
```

What happens to our inference now? First set up the data, then run each of the models from before. 

```{r}
set.seed(71)
dmoisture <- tibble(
  h0 = rnorm(1000, 10, 2), 
  treatment = rep(0:1, each = 500), 
  M = rbern(1000)
) %>% 
  mutate(fungus = rbinom(1000, 1, 0.5 - (treatment * 0.4) + (M * 0.4))) %>% 
  mutate(h1 = h0 + rnorm(1000, 5 + (3 * M)))

quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment) + (bF * fungus), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dmoisture
) %>% 
  precis()
```

```{r}
quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + (bT * treatment), 
    a ~ dlnorm(0, 0.25), 
    bT ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dmoisture
) %>% 
  precis()
```

Including fungus in the model gives the wrong inference. 

RM mentions in the Rethinking section that model selection (i.e. using error metrics or information criteria) wouldn't help: including fungus will improve the model's predictions but the causal inference will be wrong. 

## Collider bias

We return to the published scientific studies example from earlier. Its DAG: 

```{r}
dagify(
  S ~ T, 
  S ~ N, 
  coords = tibble(
    name = c("T", "S", "N"), 
    x = c(0, 1, 2), 
    y = c(0, 0, 0)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

RM gives a simple means for identifying colliders: 

> The fact that two arrows enter $S$ means it is a **collider**. 

### Collider of false sorrow

We want to study the relationship between ageing and happiness. Our DAG is: 

```{r}
dagify(
  M ~ H, 
  M ~ A, 
  coords = tibble(
    name = c("H", "M", "A"), 
    x = c(0, 1, 2), 
    y = c(0, 0, 0)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

So both ageing and happiness affect marriage. Two arrows going into $M \implies$ that we have a collider. 

Some simulation to illustrate the issue. 

```{r}
set.seed(11)
dmarriage <- sim_happiness() %>% 
  as_tibble()
precis(dmarriage)
```

```{r}
dmarriage %>% 
  mutate(
    married = factor(married) %>% 
      fct_relabel(~ c("unmarried", "married"))
  ) %>% 
  ggplot(aes(age, happiness, fill = married)) + 
  geom_point(shape = 21) + 
  scale_fill_manual(values = c("white", "steelblue")) + 
  scale_x_continuous(breaks = seq(0, 6, by = 10)) + 
  labs(fill = NULL) + 
  theme(legend.position = "top", panel.grid = element_blank())
```

There's a clear pattern to the distribution of blue and white dots: happier people are more likely to be married at every age, but overall older people are more likely to be married. 

Now if we want to understand whether age is related to happiness we can build a model. The linear moel part is: 

$$
\begin{align}
\mu_i &= \alpha_{MID[i]} + \beta_A A_i
\end{align}
$$

There is a different $\alpha$ for each group (married and unmarried), then a single coefficient for age. 

Now we need to work on the priors. It helps to rescale age on $[0, 1]$, such that we can interpret it as "what proportion of their adult life has this person lived?"

We will also add a variable for the marriage index ($MID$ in the linear model). 

```{r}
dmarriage_adults <- dmarriage %>% 
  filter(age > 17L) %>% 
  mutate(A = scales::rescale(age)) %>% 
  mutate(mid = married + 1L)

```

Now fit the model. 

```{r}
m6_9 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a[mid] + (bA * A), 
    a[mid] ~ dnorm(0, 1), 
    bA ~ dnorm(0, 2), 
    sigma ~ dexp(1)
  ), 
  data = dmarriage_adults
)
precis(m6_9, depth = 2)
```

The model shows a strong negative association between age and happiness. What happens if we drop marriage from the model?

```{r}
m6_10 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a + (bA * A), 
    a ~ dnorm(0, 1), 
    bA ~ dnorm(0, 2), 
    sigma ~ dexp(1)
  ), 
  data = dmarriage_adults
)
precis(m6_10)
```

We get the correct inference, that age and happiness have no association. Conditioning on the collider created a spurious association between its common causes. 

### The haunted DAG

We now consider the question of grandparents' and parents' effect on their grandkids' education. We can presume that there will be influence both directly and indirectly. 

```{r}
dagify(
  C ~ P + G, 
  P ~ G, 
  coords = tibble(
    name = c("G", "P", "C"), 
    x = c(1, 2, 2), 
    y = c(0, 0, -1)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

But we might also expect that there are other factors affecting the parents and children: where they live for example. 

```{r}
dagify(
  C ~ P + G + U, 
  P ~ G + U, 
  coords = tibble(
    name = c("G", "P", "C", "U"), 
    x = c(1, 2, 2, 3), 
    y = c(0, 0, -1, -0.5)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

The new variable $U$ is unobserved. 

Now we can try to model this. The issue here is that conditioning on $P$ will bias our inference about $\beta_G$, because it is a collider for $G$ and $U$. This happens even though we never measure $U$. 

Some simulation to illustrate. 

```{r}
# First set up the strengths of association between the variables
bGP <- 1 # effect of G on P
bGC <- 0 # effect of G on C
bPC <- 1 # direct effect of P on C
bU <- 2 # direct effect of U on P and C

set.seed(1)
dhaunted <- tibble(
  U = (2 * rbern(200, 0.5)) - 1L, 
  G = rnorm(200)
) %>% 
  mutate(P = rnorm(200, (bGP * G) + (bU * U))) %>% 
  mutate(C = rnorm(200, (bPC * P) + (bGC * G) + (bU * U)))
```

Now we fit the model (with the vague priors that RM uses). 

```{r}
m6_11 <- quap(
  alist(
    C ~ dnorm(mu, sigma), 
    mu <- a + (bPC * P) + (bGC * G), 
    a ~ dnorm(0, 1), 
    c(bPC, bGC) ~ dnorm(0, 1), 
    sigma ~ dexp(1)
  ), 
  data = dhaunted
)
precis(m6_11)
```

We know from our setup that `bPC` should be about 1, but we get something much larger because there are effects from the unmeasured variable $U$ that we cannot include in our model. 

We get a more surprising result also: apparently grandparents have a _negative_ effect on educational outcomes. But this arises because of the neighbourhood effects. We can recreate RM's plot to illustrate.

```{r}
dhaunted %>% 
  mutate(across(c(G, C), standardize)) %>% 
  mutate(
    filled = between(ntile(P, 100), 45, 60), 
    hood = if_else(U < 0, "bad", "good") %>% 
      factor()
  ) %>% 
  ggplot(aes(G, C)) + 
  geom_point(aes(colour = hood, shape = filled)) + 
  stat_smooth(
    data = . %>% filter(filled), 
    method = "lm", 
    formula = "y ~ x", 
    se = FALSE, 
    colour = "grey30", 
    size = 0.5
  ) + 
  geom_text(
    aes(label = label, colour = hood), 
    data = tibble(
      G = c(-2, -0.1), 
      C = c(2, -2.1), 
      label = c("good neighbourhood", "bad neighbourhood"), 
      hood = factor(c("good", "bad"), levels = c("bad", "good"))
    )
  ) +
  scale_colour_manual(values = c("black", "steelblue")) + 
  scale_shape_manual(values = c(1, 19)) + 
  labs(
    title = "Parents in 45th to 60th centiles", 
    x = "grandparent education (G)", 
    y = "grandchild education (C)"
  ) + 
  theme(
    panel.grid = element_blank(), 
    legend.position = "none"
  )
```

The regression line is for those parents between the 45th and 60th centiles. Once we know that a parent is in that group, the only thing left that can vary is $G$, which ends up telling us indirectly about the neighbourhood effects. (NB. I tried running this again with `bU` set to 1, and the effect was still there albeit less pronounced.)

Could we try regressing each of our predictor variables on $C$ separately?

```{r}
# first just parents
m6_11b <- quap(
  alist(
    C ~ dnorm(mu, sigma), 
    mu <- a + (bPC * P), 
    a ~ dnorm(0, 1), 
    bPC ~ dnorm(0, 1), 
    sigma ~ dexp(1)
  ), 
  data = dhaunted
)
m6_11b %>% 
  precis()
```

The estimate for `bPC` is still far too high. Note also that `sigma` has increased a bit. 

```{r}
# now just grandparents
m6_11c <- quap(
  alist(
    C ~ dnorm(mu, sigma), 
    mu <- a + (bGC * G), 
    a ~ dnorm(0, 1), 
    bGC ~ dnorm(0, 1), 
    sigma ~ dexp(1)
  ), 
  data = dhaunted
)
m6_11c %>% 
  precis()
```

Now `bGC` is lower than it should be, the interval for `a` is huge, and `sigma` has exploded. Easier to see in a plot. 

```{r}
coeftab(
  m6_11, 
  m6_11b, 
  m6_11c
) %>% 
  plot()
```

We get similar inferences for the effect of parents in `m6_11` and `b`, but for everything else the results are markedly different across the models. 

The only solution is to measure $U$. We can fit a model to it and check the results. 

```{r}
m6_12 <- quap(
  alist(
    C ~ dnorm(mu, sigma), 
    mu <- a + (bPC * P) + (bGC * G) + (bU * U), 
    a ~ dnorm(0, 1), 
    c(bPC, bGC, bU) ~ dnorm(0, 1), 
    sigma ~ dexp(1)
  ), 
  data = dhaunted
)
precis(m6_12)
```

We recover the same values for our parameters that we used for simulation. It's helpful to see these in a plot with the rest. 

```{r}
coeftab(
  m6_11, 
  m6_11b, 
  m6_11c, 
  m6_12
) %>% 
  plot()
```

## Confronting confounding

RM's definition of confounding is worth repeating.

> Let's define **confounding** as any context in which the association ebtween an outcome $Y$ and a predictor of interest $X$ is not the same as it would be if we had experimentally determined the values of $X$.

This is almost inevitable in any sort of observational study/modelling: there will be other variables (measured or unmeasured) that affect $X$. 

Being **confounded** means that there are _multiple paths connecting two variables_. Suppose we want to regress $W$ on $E$ in this DAG: 

```{r}
dagify(
  W ~ U + E, 
  E ~ U, 
  coords = tibble(
    name = c("E", "U", "W"), 
    x = c(1, 2, 3), 
    y = c(1, 2, 1)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

There are two paths from $E$ to $W$, and if we fail to consider both then we will get the wrong answers. 

The direction of the arrows doesn't matter when considering whether confounders exist, as the statistical associations can move 'against' the arrows. The arrows _do_ matter when considering causality. 

### Shutting the backdoor

RM lists the four elemental confounds. 

```{r}
(
  dagify(
    X ~ Z, 
    Y ~ Z, 
    coords = tibble(
      name = c("X", "Y", "Z"), 
      x = c(1, 3, 2), 
      y = c(1, 1, 0)
    )
  ) %>% 
    ggdag() + 
    theme_dag() + 
    labs(title = "The Fork") + 
    theme(plot.title = element_text(hjust = 0.49))
) + (
  dagify(
    Z ~ X, 
    Y ~ Z, 
    coords = tibble(
      name = c("X", "Y", "Z"), 
      x = c(1, 3, 2), 
      y = c(2, 0, 1)
    )
  ) %>% 
    ggdag() + 
    theme_dag() + 
    labs(title = "The Pipe") + 
    theme(plot.title = element_text(hjust = 0.49))
) + (
  dagify(
    Z ~ X + Y, 
    coords = tibble(
      name = c("X", "Y", "Z"), 
      x = c(1, 3, 2), 
      y = c(1, 1, 2)
    )
  ) %>% 
    ggdag() + 
    theme_dag() + 
    labs(title = "The Collider") + 
    theme(plot.title = element_text(hjust = 0.49))
) + (
  dagify(
    Z ~ X + Y, 
    D ~ Z, 
    coords = tibble(
      name = c("X", "Y", "Z", "D"), 
      x = c(1, 3, 2, 2), 
      y = c(1, 1, 2, 1.2)
    )
  ) %>% 
    ggdag() + 
    theme_dag() + 
    labs(title = "The Descendent") + 
    theme(plot.title = element_text(hjust = 0.49))
)
```

From these four we can build any DAG. 

1. The fork: the confounder is a common cause of $X$ and $Y$. 
2. The pipe: the effect of $X$ is mediated through $Z$ to $Y$. 
3. The collider: $X$ and $Y$ both affect $Z$. 
4. The descendent: $Z$ influences $D$. 

The key takeaways are: 

- We handle both pipes and forks the same way: we must condition on the confounder in order to block the flow of information from $X$ to $Y$. 
- For the collider we **must not** condition on the confounder: doing so will open the path through which information can flow between $X$ and $Y$. 
- The approach for handling a descendant depends on its parent: in the DAGs above the right step would be to leave both $Z$ and $D$ out of the model, but that would not be the case if $Z$ were a fork or pipe. 

### Two roads

Our first example. We have this DAG: 

```{r}
dagify(
  X ~ U, 
  U ~ A, 
  B ~ U + C, 
  C ~ A, 
  Y ~ X + C, 
  coords = tibble(
    name = c("X", "U", "B", "A", "Y", "C"), 
    x = c(1, 1, 2, 2, 3, 3), 
    y = c(0, 2, 1, 3, 0, 2)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

I worked through the steps: 

1. Check all paths: we have the obvious one $XY$, plus $XUACY$ and $XUBCY$. 
2. Classify each path by whether it's open or closed: only $XUBCY$ contains a collider, so it is the only closed path. 
3. Classify each path by whether it's a backdoor path: of the open paths only $XUACY$ has a path entering $X$. 
4. Decide which variables to condition on to close the backdoor: can use $A$ or $C$. 

RM suggests that $C$ will be better in this particular case because it will tell us about the precision of our estimate of $X \to Y$. Maybe this will make more sense later in the book when we cover measurement?

Can also get this info from {dagitty}: 

```{r}
dagitty::adjustmentSets(
  dag(
    "dag {
    U [unobserved]
    X -> Y
    X <- U <- A -> C -> Y
    U -> B <- C
    }"
  ), 
  exposure = "X", 
  outcome = "Y"
)
```

### Backdoor waffles

Now we go back to the Waffle House divorce data. We could set up the DAG as: 

```{r}
dagify(
  A ~ S, 
  M ~ A + S, 
  W ~ S, 
  D ~ W + M + A, 
  coords = tibble(
    name = c("A", "S", "M", "D", "W"), 
    x = c(1, 1, 2, 3, 3), 
    y = c(1, 3, 2, 1, 3)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

We are interested in the effect of $W$ on $D$. There are three other open paths in this DAG: $WSAD$, $WSAMD$, and $WSMD$. All of them are open. We can close all of those backdoor paths by conditioning on $S$. {dagitty} will confirm as much: 

```{r}
dagitty::adjustmentSets(
  dag(
    "dag {
    A <- S -> W
    A -> M <- S
    A -> D
    M -> D <- W
    }"
  ), 
  exposure = "W", 
  outcome = "D"
)
```

Conditioning on $A$ and $M$ would do the same. We can test how well this agrees with the data. 

```{r}
data("WaffleDivorce")
ddivorce <- WaffleDivorce %>% 
  as_tibble() %>% 
  transmute(
    W = standardize(WaffleHouses / Population), 
    sid = South + 1L, 
    D = standardize(Divorce), 
    M = standardize(Marriage), 
    A = standardize(MedianAgeMarriage)
  )

quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bW * W), 
    a[sid] ~ dnorm(0, 0.5), 
    bW ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
) %>% 
  precis(depth = 2)
```

The estimate for `bW` is fairly uncertain, but most of the posterior range is positive. 

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bW * W) + (bM * M) + (bA * A), 
    a ~ dnorm(0, 0.5), 
    c(bW, bM, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = ddivorce
) %>% 
  precis(depth = 2)
```

Similar estimate for `bW`, although we are now fairly certain it's positive. In this case there are likely to be unobserved variables not accounted for in the DAG above. Or Waffle Houses really do cause divorce. 

## Practice

### Easy

6E1. 

Multicollinearity, post-treatment effect, and collider bias. 

6E2. 

An example of multicollinearity: 

Suppose there was a population of taxpayers, and we want to predict risk of non-compliance. There are two predictors that are almost exactly the same. If we use both predictors we will get wide estimates for the posterior distributions, as many combinations of them would be compatible with the data. We should instead use just one of them, or find a sensible way to combine them (e.g. summing them as we did in the leg example). 

6E3. 

1. The fork: in this case $X \perp\!\!\!\perp Y | Z$. If we condition on $Z$ we block the path. 

```{r}
dagitty("dag {X <- Z -> Y}") %>% 
  impliedConditionalIndependencies()
```

2. The pipe: same as the fork, $X \perp\!\!\!\perp Y | Z$. Same as the fork, conditioning on $Z$ blocks the pipe. 

```{r}
dagitty("dag {X -> Z -> Y}") %>% 
  impliedConditionalIndependencies()
```

3. The collider: now $X \perp\!\!\!\perp Y$ without conditioning on $Z$. So we just model $X$ and $Y$ directly. 

```{r}
dagitty("dag {X -> Z <- Y}") %>% 
  impliedConditionalIndependencies()
```

4. The descendent: in this case the conditional dependencies will change based on the rest of the DAG. In RM's example he added a descendent to a collider. In that case we would have the same $X \perp\!\!\!\perp Y$ that we had with the collider, but would also have $D \perp\!\!\!\perp X | Z$ and $D \perp\!\!\!\perp Y | Z$. 

```{r}
dagitty("dag {X -> Z <- Y; Z -> D}") %>% 
  dagitty::impliedConditionalIndependencies()
```

What if we change $Z$ from a collider to a fork?

```{r}
dagitty("dag {X <- Z -> Y; Z -> D}") %>% 
  dagitty::impliedConditionalIndependencies()
```

Now we get the $X \perp\!\!\!\perp Y | Z$ that we expect from the fork example above. Also we see $D \perp\!\!\!\perp X | Z$ and $D \perp\!\!\!\perp Y | Z$ as before. So the relationship of $D$ to $X$ and $Y$ doesn't change. 

6E4. 

We can consider that there is some variable, $B$, that governs the bias of the sample: how likely is a given observation to be present in the sample? If our quantities of interest ($X$ and $Y$) both affect $B$, then $B$ is a collider. 

Using a sample that is biased by $B$ is the same as conditioning on it, which will harm our inferences. 

```{r}
set.seed(1855)
tibble(X = rnorm(200), Y = rnorm(200)) %>% 
  mutate(B = rnorm(200, (X + Y) / 2)) %>% 
  mutate(sampled = ntile(B, 10) == 10) %>% 
  ggplot(aes(X, Y, colour = sampled, fill = sampled)) + 
  geom_point(alpha = 0.4) + 
  stat_smooth(
    data = . %>% filter(sampled), 
    formula = "y ~ x", 
    method = "lm", 
    se = FALSE
  ) + 
  scale_colour_manual(
    values = c("black", "steelblue"), 
    aesthetics = c("fill", "colour")
  ) + 
  theme(
    legend.position = "none", 
    panel.grid = element_blank()
  )
```

This is Berkson's Paradox, as we saw at the beginning of the chapter. 

### Medium 

6M1. 

We can create the DAG. 

```{r}
dagify(
  X ~ U, 
  U ~ A, 
  B ~ U + C, 
  C ~ A + V, 
  Y ~ X + C + V, 
  coords = tibble(
    name = c("X", "U", "B", "A", "Y", "C", "V"), 
    x = c(1, 1, 2, 2, 3, 3, 4), 
    y = c(0, 2, 1, 3, 0, 2, 1)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

Earlier we found $XUACY$ and $XUBCY$ (excluding the direct path $XY$): of these only $XUACY$ was open and it is a backdoor path. We now have to consider two more paths: $XUACVY$ and $XUBCVY$. Adding $V$ makes $C$ into a collider for the first of those paths, so it is closed and can be ignored. The second path goes through $B$, which is a collider and closes the path. (NB. in this path $C$ is a pipe rather than a collider.) So we are almost back to the original solution, which is considering $XUACY$. Now though we cannot condition on $C$ alone because it will open up the path $XUACVY$. So we condition on $A$ instead. 

Check the answer with {dagitty}.

```{r}
dagitty("dag {X <- U <- A -> C -> Y; U -> B <- C; X -> Y; C <- V -> Y}") %>% 
  adjustmentSets(exposure = "X", outcome = "Y")
```

Seems we could also condition on $C$ and $V$: this makes sense, as conditioning on $C$ would open up that path but then conditioning on $V$ would close it. (Although $V$ is unobserved, so not an option.)

6M2. 

```{r}
set.seed(1001)
d_6m2 <- tibble(X = rnorm(200)) %>% 
  mutate(Z = rnorm(200, mean = X, sd = 0.2)) %>% 
  mutate(Y = rnorm(200, mean = Z + rnorm(200, sd = 0.2)))

d_6m2 %>% 
  corrr::correlate()
```

First run the model as suggested, with both predictors. 

```{r}
quap(
  alist(
    Y ~ dnorm(mu, sigma), 
    mu <- a + (bX * X) + (bZ * Z), 
    a ~ dnorm(0, 0.2), 
    c(bX, bZ) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6m2
) %>% 
  precis()
```

The sort of multicollinearity problems we had earlier aren't present: the intervals on `bX` and `bZ` are fairly wide, but not as severe as with the legs problem. Why is that? It's because our DAG is different: instead of having a direct effect from $X$ to $Y$ its effect is mediated through $Z$. So while the collinearity creates some uncertainty about the true parameter estimates, it's not nearly so crazy as we saw before. 

We can fit the model with each predictor at once also. First with $X$. 

```{r}
quap(
  alist(
    Y ~ dnorm(mu, sigma), 
    mu <- a + (bX * X), 
    a ~ dnorm(0, 0.2), 
    bX ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6m2
) %>% 
  precis()
```
Unsurprisingly $X$ is predictive of $Y$. 

```{r}
quap(
  alist(
    Y ~ dnorm(mu, sigma), 
    mu <- a + (bZ * Z), 
    a ~ dnorm(0, 0.2), 
    bZ ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6m2
) %>% 
  precis()
```

Similar estimates from modelling with $Z$ only. 

6M3. 

In each we want the total causal influence of $X$ on $Y$. In every case I will exclude the direct path $XY$. 

First DAG. 

```{r}
dagify(
  X ~ Z, 
  Y ~ X + Z + A, 
  Z ~ A, 
  coords = tibble(
    name = c("X", "Y", "Z", "A"), 
    x = c(1, 3, 2, 3), 
    y = c(1, 1, 2, 2)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

$XZY$ is open as $Z$ is a fork. $XZAY$ is closed because $A$ is a collider. Both are backdoor paths because of the arrow $Z \to X$. So if we condition on $Z$ we close $XZY$. 

```{r}
dagitty("dag {Z -> X -> Y <- A; A -> Z -> Y}") %>% 
  adjustmentSets(exposure = "X", outcome = "Y")
```

Second DAG. 

```{r}
dagify(
  Z ~ X + A, 
  Y ~ X + Z + A, 
  coords = tibble(
    name = c("X", "Y", "Z", "A"), 
    x = c(1, 3, 2, 3), 
    y = c(1, 1, 2, 2)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

There are no arrows into $X$, which means no backdoor paths to close. So we just model $X \to Y$ directly. 

```{r}
dagitty("dag {X -> Z <- A -> Y; X -> Y <- Z}") %>% 
  adjustmentSets(exposure = "X", outcome = "Y")
```

Third DAG. 

```{r}
dagify(
  Z ~ X + A + Y, 
  X ~ A, 
  Y ~ X, 
  coords = tibble(
    name = c("X", "Y", "Z", "A"), 
    x = c(1, 3, 2, 1), 
    y = c(1, 1, 2, 2)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

$XZY$ is closed because $Z$ is a collider. 

$XAZY$ is closed because $Z$ is a collider. 

No open backdoor paths to close, can model $X \to Y$ directly. 

```{r}
dagitty("dag {A -> X -> Y -> Z; A -> Z <- X}") %>% 
  adjustmentSets(exposure = "X", outcome = "Y")
```
Fourth DAG. 

```{r}
dagify(
  Z ~ X + A, 
  X ~ A, 
  Y ~ X + Z, 
  coords = tibble(
    name = c("X", "Y", "Z", "A"), 
    x = c(1, 3, 2, 1), 
    y = c(1, 1, 2, 2)
  )
) %>% 
  ggdag() + 
  theme_dag()
```

$XZY$ is open but is not a backdoor path as the arrow is out of $X$. 

$XAZY$ is an open backdoor path because $A$ is a fork. So we condition on $A$ to model $X \to Y$. 

```{r}
dagitty("dag {A -> X -> Y <- Z; A -> Z <- X}") %>% 
  adjustmentSets(exposure = "X", outcome = "Y")
```

### Hard

6H1. 

We can have a reasonable prior that Waffle Houses will have no effect on divorce rate. So we want a DAG where $D \perp\!\!\!\perp W | \text{(some predictors)}$. Start by looking at the data. 

```{r}
WaffleDivorce %>% 
  as_tibble()
```

We've already been using median age marriage ($A$) and marriage rate ($M$) as predictors, but the usefulness of $M$ was close to zero once we know $A$. We also have the indicator of a state being in the South, $S$. 

The remaining variables in the data are those relating to the state as it was in 1860: how many slaves they had, the population, and the ratio of the two. By what mechanism could we imagine these variables affecting divorce rate? It's possible that the history of slavery, with its frequent separations of family members, could affect the cultural norms around divorce. However let's keep things simpler for the moment and limit ourselves to just $W$, $A$, $M$, $S$, and $D$. 

Let's put our five variables into a DAG and see if it makes sense. 

```{r}
dag_6h1 <- dagify(
  A ~ S, 
  M ~ A + S, 
  W ~ S, 
  D ~ W + A, 
  coords = tibble(
    name = c("A", "S", "M", "D", "W"), 
    x = c(1, 1, 2, 3, 3), 
    y = c(1, 3, 2, 1, 3)
  )
)

dag_6h1 %>% 
  ggdag() + 
  theme_dag()
```

```{r}
dag_6h1 %>% 
  adjustmentSets(exposure = "W", outcome = "D")
```

The DAG suggests we can control either for $S$ or $A$. 

```{r}
d_6h1 <- WaffleDivorce %>% 
  as_tibble() %>% 
  transmute(
    A = standardize(MedianAgeMarriage), 
    S = South, 
    # add an index variable for the South
    sid = South + 1L, 
    D = standardize(Divorce), 
    W = standardize(WaffleHouses / Population), 
    M = standardize(Marriage)
  )
d_6h1
```

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bW + W), 
    a[sid] ~ dnorm(0, 0.5), 
    bW ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis(depth = 2)
```

We get something fairly close to zero for the mean effect of Waffle Houses, but with some uncertainty. 

Trying the other model:

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bW + W) + (bA * A), 
    a ~ dnorm(0, 0.5), 
    c(bW, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis()
```

The estimate for `bW` is centred exactly at 0 but with a fairly wide interval. 

The estimate for `bA` is reliably negative, which is consistent with our findings from earlier. 

6H2. 

```{r}
dag_6h1 %>% 
  impliedConditionalIndependencies()
```

Testing $A \perp\!\!\!\perp W | S$: 

```{r}
quap(
  alist(
    A ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bW * W), 
    a[sid] ~ dnorm(0, 0.5), 
    bW ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis()
```

Close enough. 

Testing $D \perp\!\!\!\perp M | A, S$.

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bM * M) + (bA * A), 
    a[sid] ~ dnorm(0, 0.5), 
    c(bM, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis()
```

Good enough. 

Testing $D \perp\!\!\!\perp M | A, W$. 

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a + (bM * M) + (bA * A) + (bW * W), 
    a ~ dnorm(0, 0.5), 
    c(bM, bA, bW) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis()
```

Good enough. 

Testing $D \perp\!\!\!\perp S | A, W$. 

```{r}
quap(
  alist(
    D ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bW * W) + (bA * A), 
    a[sid] ~ dnorm(0, 0.5), 
    c(bW, bA) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis(depth = 2)
```

Good enough. 

Testing $M \perp\!\!\!\perp W | S$. 

```{r}
quap(
  alist(
    M ~ dnorm(mu, sigma), 
    mu <- a[sid] + (bW * W), 
    a[sid] ~ dnorm(0, 0.5), 
    bW ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = d_6h1
) %>% 
  precis()
```

Good enough. 

6H3. 

Set up the data and DAG for the fox questions. 

```{r}
data("foxes")
dfox <- foxes %>% 
  as_tibble() %>% 
  mutate(across(c(avgfood, area, weight, groupsize), standardize))
```


```{r}
dag_fox <- dagify(
  avgfood ~ area, 
  groupsize ~ avgfood, 
  weight ~ avgfood + groupsize, 
  coords = tibble(
    name = c("avgfood", "area", "weight", "groupsize"), 
    x = c(1, 2, 2, 3), 
    y = c(2, 3, 1, 2)
  )
)

dag_plot_fox <- dag_fox %>% 
  tidy_dagitty() %>%
  as_tibble() %>% 
  inner_join(
    tibble(
      name = c("avgfood", "area", "weight", "groupsize"), 
      label = c("F", "A", "W", "S")
    ), 
    by = "name"
  ) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node() +
  geom_dag_text(aes(label = label)) +
  geom_dag_edges() +
  theme_dag()

dag_plot_fox
```

What are the implied conditional independencies of the DAG?

```{r}
dag_fox %>% 
  impliedConditionalIndependencies()
```

```{r}
quap(
  alist(
    area ~ dnorm(mu, sigma), 
    mu <- a + (bS * groupsize) + (bF * avgfood), 
    c(a, bS, bF) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  dfox
) %>% 
  precis()
```

```{r}
quap(
  alist(
    area ~ dnorm(mu, sigma), 
    mu <- a + (bW * weight) + (bF * avgfood), 
    c(a, bW, bF) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  dfox
) %>% 
  precis()
```

The first of these implied conditional independencies looks a little suspect, but the second is more reasonable. Noted for later on. 

We now want to infer the effect of area on weight. 

```{r}
dag_fox %>% 
  adjustmentSets(exposure = "area", outcome = "weight")
```

No need to condition we can model $A \to W$ directly. 

Now we want to set priors. First we can specify the linear model. 

$$
\begin{align}
W &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i \\
\end{align}
$$

So we will need priors on $\alpha$, $\beta_A$, and $\sigma$. Let's use the same semi-vague priors we have been using and check the results. 

```{r}
set.seed(1830)
N <- 1e3
tibble(a = rnorm(N, sd = 0.2), bA = rnorm(N, sd = 0.5)) %>% 
  rowid_to_column(".id") %>% 
  expand(nesting(.id, a, bA), x = c(-2, 2)) %>% 
  mutate(y = a + (bA * x)) %>% 
  ggplot(aes(x, y, group = .id)) + 
  geom_line(colour = "steelblue", alpha = 0.4) + 
  geom_hline(yintercept = c(-2, 2), linetype = 2, colour = "grey20")
```

We have some lines corresponding to really dramatic relationships, but the vast majority are within the $\pm 2\sigma$ range. This is good enough. 

Now we can specify the full model. 

$$
\begin{align}
W &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_A &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \text{Expo}(1)
\end{align}
$$

```{r}
m6h3 <- quap(
  alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bA * area), 
    a ~ dnorm(0, 0.2), 
    bA ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)
m6h3 %>% 
  precis()
```

Very little association between them. Doesn't seem that changing `area` will affect `weight`. 

6H4. 

We now want to infer the effect of avgfood on weight. 

```{r}
dag_fox %>% 
  adjustmentSets(exposure = "avgfood", outcome = "weight")
```
No conditioning required, can again model directly. 

$$
\begin{align}
W &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_F F_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_F &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \text{Expo}(1)
\end{align}
$$

```{r}
m6h4 <- quap(
  alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bF * avgfood), 
    a ~ dnorm(0, 0.2), 
    bF ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)
m6h4 %>% 
  precis()
```

Again the inference is that `avgfood` has little impact on its own with `weight`. 

6H5. 

```{r}
dag_fox %>% 
  adjustmentSets(exposure = "groupsize", outcome = "weight")
```

We now need to condition on `avgfood` to get the causal impact of `groupsize` on `weight`. 

$$
\begin{align}
W &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha +\beta_S S_i + \beta_F F_i \\
\alpha &\sim \mathcal{N}(0, 0.2) \\
\beta_F &\sim \mathcal{N}(0, 0.5) \\
\beta_S &\sim \mathcal{N}(0, 0.5) \\
\sigma &\sim \text{Expo}(1)
\end{align}
$$

```{r}
m6h5 <- quap(
  alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bS * groupsize) + (bF * avgfood), 
    a ~ dnorm(0, 0.2), 
    c(bS, bF) ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
)
m6h5 %>% 
  precis()
```

Now `groupsize` has a strong negative association with `weight`, and `avgfood` has a strong positive association. What changed? Let's look at the DAG again. 

```{r}
dag_plot_fox
```

At least some of the effect of avgfood (`F` in the DAG) is mediated through groupsize (`S`). We can examine the correlations between those variables and weight (`W`). 

```{r}
dfox %>% 
  select(avgfood, groupsize, weight) %>% 
  correlate()
```
The correlation between avgfood and groupsize is very high, but because the arrow goes $F \to S$ then the path $FSW$ isn't a backdoor. So when we model $F \to W$ directly we get the correct answer to our question, which is that _on its own_ $F$ is a poor predictor of $W$. 

When we include $S$ in our model we then have to close the backdoor path $SFW$, so we condition on the fork ($F$) to get the full picture of the three. This reveals that _conditional on knowing the group size, there is a strong positive association between food and weight_. We can try modelling $S \to W$ directly, without closing the backdoor path, to see what happens. 

```{r}
quap(
  alist(
    weight ~ dnorm(mu, sigma), 
    mu <- a + (bS * groupsize), 
    a ~ dnorm(0, 0.2), 
    bS ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), 
  data = dfox
) %>% 
  precis()
```

We still see a negative association between $S$ and $W$, but it is much weaker than when we close the backdoor path $SFW$. 
