---
title: "The Golem of Prague"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The Golem of Prague

## Statistical golems

This opening chapter sets up the philosophy of the book. The metaphor of statistical models as golems is a good one: they are powerful but have no wisdom or judgement of their own. That must come from the user. McElreath (hereafter RM) includes an example of a decision trees for picking statistical tests: a means of delegating the judgement entirely. As well as oversimplifying statistics to following a set of rules, then using the designated golem, it's also confusing. You have to remember the specific rules and conditions for each (when it's valid etc.), and the tools are inflexible.

RM proposes a more generalised approach to statistical modelling. The cost is that you can't just run `PROC TTEST`, instead need to invest time in understanding things more deeply. The payoff is being able to build up models that can deal with a much wider range of problems, including greater complexity. (He gives *Fisher's exact test* as an example, stating that "aside from Fisher's original use of it, I have never seen it used appropriately.")

## Statistical rethinking

The next section includes a critique of "folk Popperism": falsely equating Popper's arguments about falsification in science with falsifying null hypotheses. Being able to falsify an explanatory model would indeed be a convincing outcome, but this is not what [NHST](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Null_hypothesis_statistical_significance_testing) is about. The two reasons are:

1. Hypotheses are not models, and since there is a many to many relationship between the two it's more or less impossible to do strict falsification.
2. Measurement errors will mean that even apparent falsification rests upon an assumption about the accuracy of the data that cannot be exact.

There's an excellent diagram on page 6 showing the relations between hypotheses, process models, and statistical models.

RM also points out that interesting hypotheses are not of the form "all swans are black", but rather "black swans are rare" or "80% of swans are white". In that case falsification will not resolve the matter, instead a process of good measurement and sound probabilistic comparisons.

## Tools for golem engineering

The four tools for doing this work are:

1. Bayesian data analysis;
2. Model comparison;
3. Multilevel models;
4. Graphical causal models.

### Bayesian data analysis

Neat summary of what this means:

> Count all the ways data can happen, according to assumptions.
>
> Assumptions with more ways that are consistent with data are more plausible.

Plenty of good reasons given for preferring this approach, including a quick comparison with frequentist stats. Bayesian data analysis does not rest upon an imaginary repeated sampling: **the randomness is in the information, not the measurement**. The example of Galileo observing fuzzy blobs around Saturn that turn out to be the rings is useful: repeated measurement would give the same result, but it's the shape of the planet and rings that generating the uncertainty, not the measurement.

Bayesian results are more consistent with the intuitive understanding of statistical results: the way that many people interpret p-values is really the definition of a Bayesian posterior probability.

### Model comparison

Using tools such as cross-validation and information criteria helps to build models that make better predictions. As RM says: "**fitting is easy, prediction is hard**". This leads to discussion of overfitting and how it can undermine the predictive power of our models.

Quote from the lecture:

> Most ways of training a model on data lead the model to really love your sample, but not to love the world.

### Multilevel models

Also known as hierarchical, random effects, varying effects, or mixed effects models, this is a way of building a structure of parameters that are mutually dependent, and estimating them all together. This helps with mitigating overfitting: cross-validation and information criteria help to measure overfitting, and multilevel models help to address it.

The key element is partial pooling, or sharing information across observations depending on their shared characteristics.

RM wants this to be the default form of regression, and the book works to make that case.

### Graphical causal models

The example given here is of tree branches moving in the wind: it's easy to understand that the wind causes the movement, but a statistical model would only detect an association and might easily lead one to conclude that the branches cause the wind. (This excludes the possibility of other variables.)

This gets to another paradox in prediction (overfitting being the first), which is:

> Models that are causally incorrect can make better predictions than those that are causally correct.

If we want to stay in the realm of pure prediction this may not matter so much, but otherwise we need to work on understanding these causes. RM will use [Directed Acyclic Graphs](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAGs) as a tool for this.
