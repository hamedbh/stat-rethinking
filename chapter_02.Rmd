---
title: "Chapter 2: Small Worlds and Large Worlds"
output: 
  html_document: 
    highlight: pygments
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r}
library(rethinking)
library(tidyverse)
```

- **Small World**: the self-contained world of the model.
- **Large World**: the 'real' world where you deploy the model.

Assumptions made in the small world are tested in the large world when you deploy the model.

# Garden of Forking Data

Bayesian inference considers all the possible ways that the data _could_ have been generated, and weighs the relative probability of each.

Start with simple counts, then add prior information, finally start working with probabilities instead of counts.

For example move from counting the blue marbles to working with $p$ as the proportion of the marbles that are blue (i.e. the probability of any randomly chosen marble being blue).

$p$ after getting $D_{new} \propto$ number of ways $p$ can produce $D_{new} \times$ prior for $p$.

Which leads to Bayes rule:

$$ P(p | D_{new}) = \frac{P(D_{new} | p) \times P(p)}{P(D_{new})}$$

Method for computing the plausibilities is simply to rescale the counts.

```{r}
# these are the numbers of ways D_new could have been generated
ways <- c( 0, 3, 8, 9, 0 )
# normalise them to sum to 1
ways/sum(ways)
```

The names for the components of Bayes rule are:

- $p$ is the **parameter**, conjecture for proportion of blue marbles;
- relative number of ways that $p$ can produce the data is the **likelihood**;
- prior plausibility of any specific $p$ is the **prior probability**;
- updated plausibility of any specific $p$ is the **posterior probability**.

So:

$$
Posterior = \frac{Likelihood \times Prior}{Average \; Likelihood}
$$

# Building a Model

The next example shown is a binomial, using repeated tests of a toy globe to estimate the proportion of water on its surface. The data generated are:

_W L W W W L W L W_

## Likelihood

Start by assuming that the proportion of water on the globe is also the probability $p$ of getting a _W_ on any toss, and that each toss is an independent Bernoulli trial. So each toss is $\text{Bernoulli}(p)$, so the sequence is $\text{Binomial}(n, p)$.

So in this case, the likelihood of any single value of $W$ is:

$$
P(w | n, p) = {n \choose w}p^{w}(1-p)^{n - w}
$$

Where the binomial coefficient at the front is given by:

$$
{n \choose w} =  \frac{n!}{w!(n-w)!}
$$

Can easily generate this in R.

```{r}
dbinom(x = 6, # number of 'successful' trials 
       size = 9, # total number of trials
       prob = 0.5) # probability of success on each trial
```

Can try different values for $p$.

```{r}
tibble(p = seq(from = 0.1, to = 0.9, by = 0.01)) %>% 
    mutate(likelihood = dbinom(x = 6, size = 9, prob = p)) %>% 
    ggplot(aes(x = p, y = likelihood)) + 
    geom_point() + 
    scale_x_continuous(breaks = seq(0, 1, by = 0.1), 
                       minor_breaks = NULL)
```

## Parameters

In this case we observe $n$ and $w$, and $p$ is the parameter to be estimated. 

## Prior 

Any parameter needs a prior: this could be a value or a distribution.

## Posterior

Recreating the figures from the book showing the combination:

$$
Likelihood \times Prior \propto Posterior
$$

```{r}
d <- tibble(
    probability = seq(0, 1, length.out = 1e3)
) %>% 
    expand(probability, row = c("flat", "stepped", "Laplace")) %>% 
    arrange(row, probability) %>% 
    mutate(prior = ifelse(row == "flat", 1,
                          ifelse(row == "stepped", rep(0:1, each = 1e3/2),
                                 exp(-abs(probability - .5) / .25) / ( 2 * .25))),
           likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
    group_by(row) %>% 
    mutate(posterior = (prior * likelihood)/(sum(prior * likelihood))) %>% 
    gather(key, value, -probability, -row) %>% 
    ungroup() %>% 
    mutate(key = factor(key, levels = c("prior", "likelihood", "posterior")),
           row = factor(row, levels = c("flat", "stepped", "Laplace")))

plots <- map(levels(d$key), 
             ~ return(d %>%
                          filter(key == .x) %>% 
                          ggplot(aes(x = probability, y = value)) +
                          geom_line() +
                          scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
                          scale_y_continuous(NULL, breaks = NULL) +
                          labs(subtitle = .x) +
                          theme(panel.grid       = element_blank(),
                                strip.background = element_blank(),
                                strip.text       = element_blank()) +
                          facet_wrap(row ~ ., scales = "free_y", ncol = 1)))

gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], ncol = 3)
```

In a simple model like this it's easy to find an analytical solution to this problem, but in general we need numerical methods. Options include:

1. Grid approximation
2. Quadratic approximation
3. Markov chain Monte Carlo

Start with grid approximation: essentially this changes a continuous problem to a discrete one by picking grid points at which to estimate the parameters.

```{r}
globe_grid <- function(grid_size) {
    d <- tibble(
        p_grid = seq(from = 0,
                     to = 1,
                     length.out = grid_size), 
        prior = 1) %>% 
        mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% 
        mutate(unstd_posterior = prior * likelihood) %>% 
        mutate(posterior = unstd_posterior/sum(unstd_posterior))
    g <- d %>% 
        ggplot(aes(x = p_grid, y = posterior)) + 
        geom_point() + 
        geom_line() + 
        geom_vline(xintercept = d$p_grid[which.max(d$posterior)], 
                   colour = "blue") +
        ggtitle(paste(grid_size, "Points")) + 
        labs(x = "probability of water", y = "posterior") + 
        scale_x_continuous(breaks = seq(0, 1, by = 0.1), 
                           minor_breaks = NULL)
    plot(g)
}
globe_grid(20)
```

Try again with different grid sizes.

```{r}
walk(c(5, 10, 20, 50), globe_grid)
```

Now quadratic approximation. This works because peak of the posterior is often close to Gaussian, so its log will be quadratic. Can then use a quadratic to approximate it.

`rethinking::quap()` (Maximum A Posteriori) is a function that will estimate the mode of the posterior.

```{r}
globe_qa <- quap(alist(w ~ dbinom(9, p),  # binomial likelihood
                       p ~ dunif(0, 1)),   # uniform prior
                 data = list(w = 6))

# display summary of quadratic approximation
precis(globe_qa)
```

This output can be read: Assuming the posterior is Gaussian, it is maximized at 0.67, and its standard deviation is 0.16.

Because the structure of this model is simple can compare directly to the analytical solution.

```{r}
# analytical calculation
w <- 6
n <- 9
tibble(x = seq(0, 1, length.out = 100)) %>% 
    ggplot(aes(x, ..y..)) + 
    stat_function(fun = dbeta, 
                  n = 100, 
                  args = list(shape1 = w + 1, 
                              shape2 = n - w + 1)) + 
    stat_function(fun = dnorm, 
                  n = 100, 
                  colour = "blue", 
                  args = list(mean = 0.67, 
                              sd = 0.16)) + 
    geom_vline(xintercept = 0.01 * which.max(dbeta(seq(0, 1, length.out = 100), 
                                                   shape1 = w + 1, 
                                                   shape2 = n - w + 1)), 
               colour = "red") + 
    labs(x = "probability of water", y = "posterior")
```

Try again with $n = 18$ and $n = 36$.

```{r}
globe_qa_18 <-
  quap(
    alist(
      w ~ dbinom(9*2, p),
      p ~ dunif(0, 1)
    ), data = list(w = 6*2))

globe_qa_36 <-
  quap(
    alist(
      w ~ dbinom(9*4, p),
      p ~ dunif(0, 1)
    ), data = list(w = 6*4))

precis(globe_qa_18)
precis(globe_qa_36)
```

# Practice

## Easy

2E1. Which of the expressions below correspond to the statement: the probability of rain on Monday?

1. $Pr(rain)$
2. $Pr(rain|Monday)$
3. $Pr(Monday|rain)$
4. $Pr(rain,Monday)/Pr(Monday)$

Answer: 2

2E2. Which of the following statements corresponds to the expression: $Pr(Monday|rain)$?

1. The probability of rain on Monday.
2. The probability of rain, given that it is Monday.
3. The probability that it is Monday, given that it is raining. 
4. The probability that it is Monday and that it is raining.

Answer: 3

2E3. Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?

1. $Pr(Monday|rain)$
2. $Pr(rain|Monday)$
3. $Pr(rain|Monday)Pr(Monday)$
4. $\frac{Pr(rain|Monday)Pr(Monday)}{/Pr(rain)}$ 
5. $\frac{Pr(Monday|rain)Pr(rain)}{/Pr(Monday)}$

Answer: 1

## Medium

2M1. 

```{r}
# write a function to make this easier
plot_grid_unif_prior <- function(n, # number of trials
                                 w, # number of 'successes', i.e. water
                                 grid_size = 50L) { # how many grid points to plot
    # define grid
    p_grid <- seq(from = 0,
                  to = 1,
                  length.out = grid_size)
    # define prior
    prior <- rep(1, grid_size)
    # compute likelihood at each value in grid
    likelihood <- dbinom(x = w, 
                         size = n, 
                         prob = p_grid)
    # compute product of likelihood and prior
    unstd.posterior <- likelihood * prior
    # standardize the posterior, so it sums to 1
    posterior <- unstd.posterior / sum(unstd.posterior)
    
    tibble(p = p_grid, posterior = posterior) %>% 
        ggplot(aes(x = p, y = posterior)) + 
        geom_point(size = 0.8) + 
        geom_vline(xintercept = p_grid[which.max(posterior)]) +
        ggtitle(paste(grid_size, "Points"))
}
```

(1) W,W,W
```{r}
plot_grid_unif_prior(3, 3)
```

(2) W,W,W,L
```{r}
plot_grid_unif_prior(4, 3)
```

(3) L,W,W,L,W,W,W
```{r}
plot_grid_unif_prior(7, 5)
```

2M2.

```{r}
# write a function to make this easier
plot_grid_high_prior <- function(n, # number of trials
                                 w, # number of 'successes', i.e. water
                                 grid_size = 50L) { # how many grid points to plot
    # define grid
    p_grid <- seq(from = 0,
                  to = 1,
                  length.out = grid_size)
    # define prior
    prior <- if_else(p_grid < 0.5, 
                     0, 
                     1)
    # compute likelihood at each value in grid
    likelihood <- dbinom(x = w, 
                         size = n, 
                         prob = p_grid)
    # compute product of likelihood and prior
    unstd.posterior <- likelihood * prior
    # standardize the posterior, so it sums to 1
    posterior <- unstd.posterior / sum(unstd.posterior)
    
    tibble(p = p_grid, posterior = posterior) %>% 
        ggplot(aes(x = p, y = posterior)) + 
        geom_point(size = 0.8) + 
        geom_vline(xintercept = p_grid[which.max(posterior)]) +
        ggtitle(paste(grid_size, "Points"))
}
```

```{r}
plot_grid_high_prior(3, 3)
```

(2) W,W,W,L
```{r}
plot_grid_high_prior(4, 3)
```

(3) L,W,W,L,W,W,W
```{r}
plot_grid_high_prior(7, 5)
```

2M3. 

```{r}
pr_land_earth <- 0.3 # likelihood
pr_earth <- 0.5 # prior
pr_land <- (pr_earth * pr_land_earth) + ((1 - pr_earth) * 1.0) # avg likelihood
pr_earth_land <- (pr_land_earth * pr_earth)/pr_land # posterior
round(pr_earth_land, 2)
```

2M4. 

Call the cards by how many black sides they have: 0B, 1B, 2B. We want $P(2B)$.

| Conjecture | Ways to show black side up |
| :--------: | :------------------------: |
|     0B     |              0             |
|     1B     |              1             |
|     2B     |              2             |

Can discount the 0B row and just consider the others: there are three ways that I can have a black side facing up, of which only 2 have a black side down, which implies the probability of $2/3$.

2M5. 

Use Bayes rule:

$$
P(2B \; | \; 1B) = \frac {P(1B \; | \; 2B) \; P(2B)} {P(1B)}
$$


```{r}
bayes_rule <- function(likelihood, 
                       prior, 
                       avg_likelihood) {
    (likelihood * prior) / avg_likelihood
}
bayes_rule(likelihood = 1, 
           prior = 1/2, 
           avg_likelihood = 5/8)
```

2M6. 

Use same as before, but multiply each by the new prior.

| Conjecture | Ways to show black side up | Ways to draw the card |
| :--------: | :------------------------: | :-------------------: |
|     0B     |              0             |           3           |
|     1B     |              1             |           2           |
|     2B     |              2             |           1           |

This reweights the 1B row, so that the 2 ways to have a black side down in the 2B row are now 2/4 of the ways to have a black side up.

2M7.

We want to get $P(2B | 1B, 1W)$. Use the counting method as suggested. Can ignore the 0B row as it cannot give the result.

| Conjecture | Ways to show black, first card | Ways to show white, second card |
| :--------: | :----------------------------: | :-----------------------------: |
|     1B     |                  1             |                  2              |
|     2B     |                  2             |                  3              |

Multiply the ways to show black on the first card by the ways to show white on the second card to give 2 for 1B, and 6 for 2B, so $P(2B) = 0.75$.

## Hard

2H1.

Call event of birth one being twins $T_1$ and on second birth $T_2$. We need $P(T_2 \; | \; T_1)$.

```{r}
likelihood <- c(A = 0.1, B = 0.2)
prior <- c(A = 1, B = 1)
posterior <- (likelihood * prior)/sum(likelihood * prior)
posterior
sum(posterior * likelihood)
```

2H2.

We need $P(A \; | \; Twins)$.

```{r}
bayes_rule(likelihood = 0.1, 
           prior = 0.5, 
           avg_likelihood = (0.1 * 0.5) + (0.2 * 0.5))
```

2H3.

We need $P(A \; | \; Twins, Singleton)$
```{r}
likelihood <- c(A = (1 - 0.1), B = (1 - 0.2))
prior <- c(A = 1, B = 2)
posterior <- (likelihood * prior)/sum(likelihood * prior)
posterior
```

2H4.

Need to calculate:

$$
P(A | Test_A) = \frac{P(Test_A | A) P(A)}{P(Test_A)}
$$
The only difference between the two calculations will be $P(A)$. First without the birth info.

```{r}
bayes_rule(likelihood = 0.8, 
           prior = 0.5, 
           avg_likelihood = ((0.5 * 0.8) + (0.5 * 0.35)))
```

```{r}
bayes_rule(likelihood = 0.8, 
           prior = 0.36, 
           avg_likelihood = ((0.36 * 0.8) + (0.64 * 0.35)))
```

